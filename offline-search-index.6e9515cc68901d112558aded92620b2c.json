

















































































































































































































































































































































































[{"body":"xDS is a set of discovery services, with the full name of “X Discovery Service”, in which “X” refers to different type of discovery services, including LDS (Listener), RDS (RouteConfiguration), CDS (Cluster), and EDS (Endpoint/ClusterLoadAssignment), etc. xDS API enables the date-plane to communicate with the control plane (i.e. Istio) and perform discovery of dynamic service configuration resource.\nKitex supports xDS API via the extension of kitex-contrib/xds, which enables Kitex to perform in Proxyless mode. For more details of the design, please refer to the proposal.\nFeature  Service Discovery Traffic Route: only support exact match for header and method  HTTP route configuration: configure via VirtualService. ThriftProxy: configure via patching EnvoyFilter.   Timeout:  Configuration inside HTTP route configuration: configure via VirtualService.    Usage There are two steps for enabling xDS for Kitex applications: 1. xDS module initialization and 2. Kitex Client/Server Option configuration.\nxDS module To enable xDS mode in Kitex, we should invoke xds.Init() to initialize the xds module, including the xdsResourceManager and xdsClient.\nBootstrap The xdsClient is responsible for the interaction with the xDS Server (i.e. Istio). It needs some environment variables for initialization, which need to be set inside the spec.containers.env of the Kubernetes Manifest file in YAML format.\n POD_NAMESPACE: the namespace of the current service. POD_NAME: the name of this pod. INSTANCE_IP: the ip of this pod.  Add the following part to the definition of your container that uses xDS-enabled Kitex client.\n- name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: INSTANCE_IP valueFrom: fieldRef: fieldPath: status.podIP Client-side For now, we only provide the support on the client-side. To use a xds-enabled Kitex client, you should specify destService using the URL of your target service and add one option WithXDSSuite.\n Construct a xds.ClientSuite that includes RouteMiddleware and Resolver, and then pass it into the WithXDSSuite option.  // import \"github.com/cloudwego/kitex/pkg/xds\" client.WithXDSSuite(xds.ClientSuite{ RouterMiddleware: xdssuite.NewXDSRouterMiddleware(), Resolver: xdssuite.NewXDSResolver(), }),  The URL of the target service should be in the format, which follows the format in Kubernetes:  \u003cservice-name\u003e.\u003cnamespace\u003e.svc.cluster.local:\u003cservice-port\u003e Traffic route based on Tag Match We can define traffic route configuration via VirtualService in Istio.\nThe following example indicates that when the tag contains {\"stage\":\"canary\"} in the header, the request will be routed to the v1 subcluster of kitex-server.\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: kitex-server spec: hosts: - kitex-server http: - name: \"route-based-on-tags\" match: - headers: stage: exact: \"canary\" route: - destination: host: kitex-server subset: v1 weight: 100 timeout: 0.5s To match the rule defined in VirtualService, we can use client.WithTag(key, val string) or callopt.WithTag(key, val string)to specify the tags, which will be used to match the rules.\n Set key and value to be “stage” and “canary” to match the above rule defined in VirtualService.  client.WithTag(\"stage\", \"canary\") callopt.WithTag(\"stage\", \"canary\") Traffic route based on Method Match Same as above, using VirtualService in Istio to define traffic routing configuration.\nThe example below shows that requests with method equal to SayHello are routed to the v1 subcluster of kitex-server. It should be noted that when defining rules, you need to include package name and service name, corresponding to namespace and service in thrift idl.\n uri: /${PackageName}.${ServiceName}/${MethodName}  apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: kitex-server spec: hosts: - kitex-server http: - name: \"route-based-on-path\" match: - uri: # /${PackageName}.${ServiceName}/${MethodName} exact: /proxyless.GreetService/SayHello route: - destination: host: kitex-server subset: v2 weight: 100 timeout: 0.5s Example The usage is as follows:\nimport ( \"github.com/cloudwego/kitex/client\" xds2 \"github.com/cloudwego/kitex/pkg/xds\" \"github.com/kitex-contrib/xds\" \"github.com/kitex-contrib/xds/xdssuite\" \"github.com/cloudwego/kitex-proxyless-test/service/codec/thrift/kitex_gen/proxyless/greetservice\" ) func main() { // initialize xds module err := xds.Init() if err != nil { return } // initialize the client cli, err := greetservice.NewClient( destService, client.WithXDSSuite(xds2.ClientSuite{ RouterMiddleware: xdssuite.NewXDSRouterMiddleware(), Resolver: xdssuite.NewXDSResolver(), }), ) req := \u0026proxyless.HelloRequest{Message: \"Hello!\"} resp, err := c.cli.SayHello1( ctx, req, ) } Detailed examples can be found here kitex-proxyless-example.\nLimitation mTLS mTLS is not supported for now. Please disable mTLS via configuring PeerAuthentication.\napiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"default\" namespace: {your_namespace} spec: mtls: mode: DISABLE Limited support for Service Governance Current version only support Service Discovery, Traffic route and Timeout Configuration via xDS on the client-side.\nOther features supported via xDS, including Load Balancing, Rate Limit and Retry etc., will be added in the future.\nDependencies Kitex \u003e= v0.4.0\n","categories":"","description":"","excerpt":"xDS is a set of discovery services, with the full name of “X Discovery …","ref":"/docs/kitex/tutorials/advanced-feature/xds/","tags":"","title":"xDS Support"},{"body":"xDS 是一组发现服务的总称，全称为 “X Discovery Service”，其中的 “X” 代指多种发现服务，包含 LDS (Listener), RDS (RouteConfiguration), CDS (Cluster), 和 EDS (Endpoint/ClusterLoadAssignment) 等。 数据面可以利用 xDS API 与控制平面（如 Istio）通信，完成配置信息的动态发现。\nKitex 通过外部扩展 kitex-contrib/xds 的形式对 xDS API 进行了支持，可通过代码配置开启 xDS 模块，让 Kitex 服务以 Proxyless 的模式运行，被服务网格统一纳管。具体的设计方案参见 proposal。\n已支持的功能  服务发现 服务路由：当前仅支持 header 与 method 的精确匹配。  HTTP route configuration: 通过 VirtualService 进行配置 ThriftProxy: 通过 EnvoyFilter 进行配置。   超时:  HTTP route configuration 内包含的配置，同样通过 VirtualService 来配置。    开启方式 开启的步骤分为两个部分：1. xDS 模块的初始化和 2. Kitex Client/Server 的 Option 配置。\nxDS 模块 调用 xds.Init() 便可开启对 xDS 模块的初始化，其中包括 xdsResourceManager - 负责 xDS 资源的管理，xdsClient - 负责与控制面进行交互。\nBootstrap xdsClient 负责与控制面（例如 Istio）交互，以获得所需的 xDS 资源。在初始化时，需要读取环境变量用于构建 node 标识。所以，需要在 K8S 的容器配置文件 spec.containers.env 部分加入以下几个环境变量。\n POD_NAMESPACE: 当前 pod 所在的 namespace。 POD_NAME: pod 名。 INSTANCE_IP: pod 的 ip。  在需要使用 xDS 功能的容器配置中加入以下定义即可：\n- name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: INSTANCE_IP valueFrom: fieldRef: fieldPath: status.podIP Kitex 客户端 目前，我们仅在 Kitex 客户端提供 xDS 的支持。 想要使用支持 xds 的 Kitex 客户端，请在构造 Kitex Client 时将 destService 指定为目标服务的 URL，并添加一个选项 WithXDSSuite。\n 构造一个 xds.ClientSuite，需要包含用于服务路由的RouteMiddleware中间件和用于服务发现的 Resolver。将该 ClientSuite 传入WithXDSSuite option 中.  // import \"github.com/cloudwego/kitex/pkg/xds\" client.WithXDSSuite(xds.ClientSuite{ RouterMiddleware: xdssuite.NewXDSRouterMiddleware(), Resolver: xdssuite.NewXDSResolver(), }),  目标服务的 URL 格式应遵循 Kubernetes 中的格式：  \u003cservice-name\u003e.\u003cnamespace\u003e.svc.cluster.local:\u003cservice-port\u003e 基于 tag 匹配的路由匹配 我们可以通过 Istio 中的 VirtualService 来定义流量路由配置。\n下面的例子表示 header 内包含 {\"stage\":\"canary\"} 的 tag 时，则将请求路由到 kitex-server 的 v1 子集群。\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: kitex-server spec: hosts: - kitex-server http: - name: \"route-based-on-tags\" match: - headers: stage: exact: \"canary\" route: - destination: host: kitex-server subset: v1 weight: 100 timeout: 0.5s 为了匹配 VirtualService 中定义的规则，我们可以使用client.WithTag(key, val string)或者callopt.WithTag(key, val string)来指定标签，这些标签将用于匹配规则。\n 比如：将 key 和 value 设置为“stage”和“canary”，以匹配 VirtualService 中定义的上述规则。  client.WithTag(\"stage\", \"canary\") callopt.WithTag(\"stage\", \"canary\") 基于 method 的路由匹配 同上，利用 Istio 中的 VirtualService 来定义流量路由配置。\n下面的例子表示，对于 method 等于 SayHello 的请求，路由到 kitex-server 的 v1 子集群。 需要注意的是，在定义规则时需要包含 package name 和 service name，对应 thrift idl 内的 namespace 和 service。\n uri: /${PackageName}.${ServiceName}/${MethodName}  apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: kitex-server spec: hosts: - kitex-server http: - name: \"route-based-on-path\" match: - uri: # /${PackageName}.${ServiceName}/${MethodName} exact: /proxyless.GreetService/SayHello route: - destination: host: kitex-server subset: v2 weight: 100 timeout: 0.5s 示例 完整的客户端用法如下:\nimport ( \"github.com/cloudwego/kitex/client\" xds2 \"github.com/cloudwego/kitex/pkg/xds\" \"github.com/kitex-contrib/xds\" \"github.com/kitex-contrib/xds/xdssuite\" \"github.com/cloudwego/kitex-proxyless-test/service/codec/thrift/kitex_gen/proxyless/greetservice\" ) func main() { // initialize xds module err := xds.Init() if err != nil { return } // initialize the client cli, err := greetservice.NewClient( destService, client.WithXDSSuite(xds2.ClientSuite{ RouterMiddleware: xdssuite.NewXDSRouterMiddleware(), Resolver: xdssuite.NewXDSResolver(), }), ) req := \u0026proxyless.HelloRequest{Message: \"Hello!\"} resp, err := c.cli.SayHello1( ctx, req, ) } 更详细的例子可以参考该仓库：kitex-proxyless-example.\n当前版本的不足 mTLS 目前不支持 mTLS。 请通过配置 PeerAuthentication 以禁用 mTLS。\napiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"default\" namespace: {your_namespace} spec: mtls: mode: DISABLE 有限的服务治理功能 当前版本仅支持客户端通过 xDS 进行服务发现、流量路由和超时配置。\nxDS 所支持的其他服务治理功能，包括负载均衡、限流和重试等，将在未来补齐。\n依赖 Kitex \u003e= v0.4.0\n","categories":"","description":"","excerpt":"xDS 是一组发现服务的总称，全称为 “X Discovery Service”，其中的 “X” 代指多种发现服务，包含 LDS …","ref":"/zh/docs/kitex/tutorials/advanced-feature/xds/","tags":"","title":"xDS 支持"},{"body":"hz is a tool provided by the Hertz framework for generating code. Currently, hz can generate scaffolding for Hertz projects based on thrift and protobuf’s IDL.\nInstall  Make sure the GOPATH environment variable has been defined correctly (eg export GOPATH=~/go) and add $GOPATH/bin to the PATH environment (eg export PATH=$GOPATH/bin:$PATH); do not set GOPATH to a directory that the current user does not have read/write access to. Install hz:  go install github.com/cloudwego/hertz/cmd/hz@latest Verify that the installation was successful hz -v, if the following version message is displayed, the installation was successful  hz version v0.1.0 Note，Since hz creates soft links to its own binary, make sure that the installation path of hz has writable permissions.\nOperating mode To generate code using thrift or protobuf IDL, The corresponding compiler needs to be installed: thriftgo or protoc.\nThe code generated by hz, part of it is generated by the underlying compiler (usually about the struct defined in IDL), and the other part is the user-defined routing, method and other information in IDL. The user can run the code directly.\nIn terms of execution flow, when hz uses thrift IDL to generate code, hz calls thriftgo to generate the go struct code and executes itself as a plugin to thriftgo (named thrift-gen-hertz) to generate the rest of the code. This is also true when used with the protobuf IDL.\n$\u003e hz ... --idl=IDL | | thrift-IDL |---------\u003e thriftgo --gen go:... -plugin=hertz:... IDL | | protobuf-IDL ---------\u003e protoc --hertz_out=... --hertz_opt=... IDL How to install thriftgo/protoc:\nthriftgo:\n$ GO111MODULE=on go install github.com/cloudwego/thriftgo@latest protoc:\n// brew installation $ brew install protobuf // Official image installation, using macos as an example $ wget https://github.com/protocolbuffers/protobuf/releases/download/v3.19.4/protoc-3.19.4-osx-x86_64.zip $ unzip protoc-3.19.4-osx-x86_64.zip $ cp bin/protoc /usr/local/bin/protoc // Make sure include/google goes under /usr/local/include $ cp -r include/google /usr/local/include/google Usage Basic Usage new: Create a new Hertz project  Create a new project  // Execute under GOPATH, go mod name defaults to the current path relative to GOPATH, or you can specify your own hz new // Execute under non-GOPATH, you need to specify the go mod name hz new -mod hertz/demo // Tidy \u0026 get dependencies go mod tidy After executed, it generates a scaffold for the Hertz project in the current directory.\nCompiling Projects  go build Run the project and test it  Run the project:\n./{{your binary}} Test:\ncurl 127.0.0.1:8888/ping If it returns {\"message\":\"pong\"}, it works.\nCreate a project based on thrift IDL new: Create a new project  Create the thrift IDL file in the current directory  // idl/hello.thrift namespacegohello.examplestructHelloReq{1:stringName (api.query=\"name\");// Add api annotations for easier parameter binding }structHelloResp{1:stringRespBody;}serviceHelloService{HelloRespHelloMethod(1:HelloReqrequest)(api.get=\"/hello\");}Create a new project  // Execute under GOPATH hz new -idl idl/hello.thrift // Tidy \u0026 get dependencies go mod tidy Modify the handler and add your own logic  // handler path: biz/handler/hello/example/hello_service.go // where \"hello/example\" is the namespace of thrift IDL // \"hello_service.go\" is the name of the service in the thrift IDL, all methods defined by the service will be generated in this file  // HelloMethod . // @router /hello [GET] func HelloMethod(ctx context.Context, c *app.RequestContext) { var err error var req example.HelloReq err = c.BindAndValidate(\u0026req) if err != nil { c.String(400, err.Error()) return } resp := new(example.HelloResp) // You can modify the logic of the entire function, not just the current template  resp.RespBody = \"hello,\" + req.Name // added logic  c.JSON(200, resp) } Compile the project  go build Run the project and test it  Run the project:\n./{{your binary}} Test:\ncurl --location --request GET 'http://127.0.0.1:8888/hello?name=hertz' If it returns {\"RespBody\":\"hello,hertz\"}, it works.\nupdate: Update an existing project  If your thrift IDL is updated, for example:  // idl/hello.thrift namespacegohello.examplestructHelloReq{1:stringName (api.query=\"name\");}structHelloResp{1:stringRespBody;}structOtherReq{1:stringOther (api.body=\"other\");}structOtherResp{1:stringResp;}serviceHelloService{HelloRespHelloMethod(1:HelloReqrequest)(api.get=\"/hello\");OtherRespOtherMethod(1:OtherReqrequest)(api.post=\"/other\");}serviceNewService{HelloRespNewMethod(1:HelloReqrequest)(api.get=\"/new\");}Switch to the directory where the new command was executed and update the modified thrift IDL  hz update -idl idl/hello.thrift  As you can see\nAdd new method under “biz/handler/hello/example/hello_service.go”\nThe file “new_service.go” and the corresponding “NewMethod” method have been added under “biz/handler/hello/example”\n  Now let’s develop the “OtherMethod” interface\n// HelloMethod . // @router /hello [GET] func HelloMethod(ctx context.Context, c *app.RequestContext) { var err error var req example.HelloReq err = c.BindAndValidate(\u0026req) if err != nil { c.String(400, err.Error()) return } resp := new(example.HelloResp) // You can modify the logic of the entire function, not just the current template  resp.RespBody = \"hello,\" + req.Name // added logic  c.JSON(200, resp) } // OtherMethod . // @router /other [POST] func OtherMethod(ctx context.Context, c *app.RequestContext) { var err error // The model file corresponding to example.OtherReq will also be regenerated  var req example.OtherReq err = c.BindAndValidate(\u0026req) if err != nil { c.String(400, err.Error()) return } resp := new(example.OtherResp) // added logic  resp.Resp = \"Other method: \" + req.Other c.JSON(200, resp) } Compile the project  go build Run the project and test it  Run the project:\n./{{your binary}} Test：\ncurl --location --request POST 'http://127.0.0.1:8888/other' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"Other\": \"other method\" }' If it returns {\"Resp\":\"Other method: other method\"}, it works.\nCreate a project based on protobuf IDL new: Create a new project   Create the protobuf IDL file in the current directory\nNote: In order to support api annotations in protobuf, please import the following file in the proto file where the annotation is used\n  // idl/api.proto; Annotation extension syntax = \"proto2\";package api;import \"google/protobuf/descriptor.proto\";option go_package = \"/api\";extend google.protobuf.FieldOptions { optional string raw_body = 50101; optional string query = 50102; optional string header = 50103; optional string cookie = 50104; optional string body = 50105; optional string path = 50106; optional string vd = 50107; optional string form = 50108; optional string go_tag = 51001; optional string js_conv = 50109;}extend google.protobuf.MethodOptions { optional string get = 50201; optional string post = 50202; optional string put = 50203; optional string delete = 50204; optional string patch = 50205; optional string options = 50206; optional string head = 50207; optional string any = 50208; optional string gen_path = 50301; optional string api_version = 50302; optional string tag = 50303; optional string name = 50304; optional string api_level = 50305; optional string serializer = 50306; optional string param = 50307; optional string baseurl = 50308;}extend google.protobuf.EnumValueOptions { optional int32 http_code = 50401;}Main IDL definition\n// idl/hello/hello.proto syntax = \"proto3\";package hello;option go_package = \"hertz/hello\";import \"api.proto\";message HelloReq { string Name = 1[(api.query)=\"name\"];}message HelloResp { string RespBody = 1;}service HelloService { rpc Method1(HelloReq) returns(HelloResp) { option (api.get) = \"/hello\"; }}Create a new project  // Execute under GOPATH, if the dependencies of the main IDL and the main IDL are not in the same path, you need to add the -I option, its meaning is IDL search path, equivalent to the option \"-I\" for protoc hz new -I idl -idl idl/hello/hello.proto // Tidy \u0026 get dependencies go mod tidy Modify the handler and add your own logic  // handler path: biz/handler/hello/hello_service.go // where \"/hello\" is the last level of go_package in protobuf IDL // \"hello_service.go\" is the name of the service in protobuf IDL, all methods defined by the service will be generated in this file  // Method1 . // @router /hello [GET] func Method1(ctx context.Context, c *app.RequestContext) { var err error var req hello.HelloReq err = c.BindAndValidate(\u0026req) if err != nil { c.String(400, err.Error()) return } resp := new(hello.HelloResp) // You can modify the logic of the entire function, not just the current template  resp.RespBody = \"hello,\" + req.Name // added logic  c.JSON(200, resp) } Compile the project  go build Run the project and test it  Run the project：\n./{{your binary}} Test：\ncurl --location --request GET 'http://127.0.0.1:8888/hello?name=hertz' If it returns {\"RespBody\":\"hello,hertz\"}, it works.\nupdate: Update an existing Hertz project  If your protobuf IDL is updated, for example:  // idl/hello/hello.proto syntax = \"proto3\";package hello;option go_package = \"hertz/hello\";import \"api.proto\";message HelloReq { string Name = 1[(api.query)=\"name\"];}message HelloResp { string RespBody = 1;}message OtherReq { string Other = 1[(api.body)=\"other\"];}message OtherResp { string Resp = 1;}service HelloService { rpc Method1(HelloReq) returns(HelloResp) { option (api.get) = \"/hello\"; } rpc Method2(OtherReq) returns(OtherResp) { option (api.post) = \"/other\"; }}service NewService { rpc Method3(OtherReq) returns(OtherResp) { option (api.get) = \"/new\"; }}Switch to the directory where the new command was executed and update the modified IDL  hz update -I idl -idl idl/hello/hello.proto  As you can see\nAdd new method under “biz/handler/hello/hello_service.go”\nThe file “new_service.go” and the corresponding “Method3” method have been added under “biz/handler/hello”\n  Now let’s develop the “Method2” interface\n// Method1 . // @router /hello [GET] func Method1(ctx context.Context, c *app.RequestContext) { var err error var req hello.HelloReq err = c.BindAndValidate(\u0026req) if err != nil { c.String(400, err.Error()) return } resp := new(hello.HelloResp) // You can modify the logic of the entire function, not just the current template  resp.RespBody = \"hello,\" + req.Name // added logic  c.JSON(200, resp) } // Method2 . // @router /other [POST] func Method2(ctx context.Context, c *app.RequestContext) { var err error var req hello.OtherReq err = c.BindAndValidate(\u0026req) if err != nil { c.String(400, err.Error()) return } resp := new(hello.OtherResp) // added logic  resp.Resp = \"Other method: \" + req.Other c.JSON(200, resp) } Compile the project  go build Run the project and test it  Run the project:\n./{{your binary}} Test：\ncurl --location --request POST 'http://127.0.0.1:8888/other' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"Other\": \"other method\" }' If it returns {\"Resp\":\"Other method: other method\"}, it works.\nThe structure of the generated code The structure of the code generated by hz is similar. The following is an example of the structure of the code generated by the section “Create a project based on thrift IDL” to illustrate the meaning of the code generated by hz.\n. ├── biz // business layer, which stores business logic related processes │ ├── handler // store handler file │ │ ├── hello // hello/example corresponds to the namespace defined in thrift IDL; for protobuf IDL, it corresponds to the last level of go_package │ │ │ └── example │ │ │ ├── hello_service.go // the handler file, the user will implement the method defined by the IDL service in this file, it will search for the existing handler in the current file when \"update\" command, and append a new handler to the end │ │ │ └── new_service.go // same as above, each service defined in IDL corresponds to a file │ │ └── ping.go // ping handler carried by default, used to generate code for quick debugging, no other special meaning │ ├── model // IDL content-related generation code │ │ └── hello // hello/example corresponds to the namespace defined in thrift IDL; for protobuf IDL, it corresponds to the last level of go_package │ │ └── example │ │ └── hello.go // the product of thriftgo, It contains go code generated from the contents of hello.thrift definition. And it will be regenerated when use \"update\" command. │ └── router // generated code related to the definition of routes in IDL │ ├── hello // hello/example corresponds to the namespace defined in thrift IDL; for protobuf IDL, it corresponds to the last level of go_package │ │ └── example │ │ ├── hello.go // the route registration code generated for the routes defined in hello.thrift by hz; this file will be regenerated each time the relevant IDL is updated │ │ └── middleware.go // default middleware function, hz adds a middleware for each generated route group by default; when updating, it will look for the existing middleware in the current file and append new middleware at the end │ └── register.go // call and register the routing definition in each IDL file; when a new IDL is added, the call of its routing registration will be automatically inserted during the update; do not edit ├── go.mod // go.mod file, if not specified on the command line, defaults to a relative path to GOPATH as the module name ├── idl // user defined IDL, location can be arbitrary │ └── hello.thrift ├── main.go // program entry ├── router.go // user defined routing methods other than IDL └── router_gen.go // the route registration code generated by hz, for calling user-defined routes and routes generated by hz Supported api annotations  Field annotation can be used forparameter binding and validation\nMethod annotation can be used to generate code that related to route registration\n Supported api annotations    Field annotation      annotation description   api.raw_body generate “raw_body” tag   api.query generate “query” tag   api.header generate “header” tag   api.cookie generate “cookie” tag   api.body generate “json” tag   api.path generate “path” tag   api.form generate “form” tag   api.go_tag (protobuf)\ngo.tag (thrift) passing go_tag through will generate the content defined in go_tag   api.vd generate “vd” tag       Method annotation      annotation description   api.get define GET methods and routes   api.post define POST methods and routes   api.put define PUT methods and routes   api.delete define DELETE methods and routes   api.patch define PATCH methods and routes   api.options define OPTIONS methods and routes   api.head define HEAD methods and routes   api.any define ANY methods and routes    Usage: Field annotation: Thrift:\nstructDemo{1:stringDemo (api.query=\"demo\",api.path=\"demo\");2:stringGoTag (go.tag=\"goTag:\"tag\"\");3:stringVd (api.vd=\"$!='your string'\");}Protobuf:\nmessage Demo { string Demo = 1[(api.query)=\"demo\",(api.path)=\"demo\"]; string GoTag = 2[(api.go_tag)=\"goTag:\"tag\"\"]; string Vd = 3[(api.vd)=\"$!='your string'\"];}Method annotation: Thrift:\nserviceDemo{RespMethod(1:Reqrequest)(api.get=\"/route\");}Protobuf:\nservice Demo { rpc Method(Req) returns(Resp) { option (api.get) = \"/route\"; }}Command line parameter description Global: $ hz --help NAME: hz - A idl parser and code generator for Hertz projects USAGE: hz [global options] command [command options] [arguments...] VERSION: 0.0.1 COMMANDS: new Generate a new Hertz project update Update an existing Hertz project help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --help, -h show help (default: false) --verbose turn on verbose mode (default: false) --version, -v print the version (default: false)  new: Create a new Hertz project    update: Updating an existing Hertz project  New: $ hz help new NAME: hz new - Generate a new Hertz project USAGE: hz new [command options] [arguments...] OPTIONS: --client_dir value Specify the client path. If not specified, no client code is generated. --customize_layout value Specify the layout template. ({{Template Profile}}:{{Rendering Data}}) --customize_package value Specify the package template. ({{Template Profile}}:) --exclude_file value, -E value Specify the files that do not need to be updated. (accepts multiple inputs) --handler_dir value Specify the handler path. --idl value Specify the IDL file path. (.thrift or .proto) (accepts multiple inputs) --json_enumstr Use string instead of num for json enums when idl is thrift. (default: false) --model_dir value Specify the model path. --module value, --mod value Specify the Go module name to generate go.mod. --no_recurse Generate master model only. (default: false) --option_package value, -P value Specify the package path. ({include_path}={import_path}) (accepts multiple inputs) --out_dir value Specify the project path. --proto_path value, -I value Add an IDL search path for includes. (Valid only if idl is protobuf) (accepts multiple inputs) --protoc value, -p value Specify arguments for the protoc. ({flag}={value}) (accepts multiple inputs) --service value Specify the service name. --snake_tag Use snake_case style naming for tags. (Only works for 'form', 'query', 'json') (default: false) --thriftgo value, -t value Specify arguments for the thriftgo. ({flag}={value}) (accepts mul  client_dir: Specify the path to generate client-side code, if not specified, it will not be generated; currently generates a global client for each service, and will provide rich client code capabilities later    customize_layout: Customize the layout template of the project. For details, see: hz custom template use    customize_package: Customize the project package related templates, mainly for the handler template. For details, see: hz custom template use    exclude_file: Files that do not need to be updated(relative to the project path, multiple supported)    handler_dir: Specify the handler generation path, the default is “biz/handler”    idl: IDL file path (.thrift or .proto)    json_enumstr: When IDL is thrift, json enums uses string instead of num (option passed through to thriftgo)    model_dir: Specify the model generation path, the default is “biz/model”    module/mod: Specify the name of go mod, which must be specified under non-GOPATH, and default to a path relative to GOPATH under GOPATH.    no_recurse: Generate only the model code for main IDL    option_package/P: Specify the path to the package, ({include_path}={import_path})    out_dir: Specify the project build path    proto_path/I: When IDL is protobuf, specify the search path for IDL, equivalent to the option “-I” for protoc    protoc/p: Option passed through to protoc ({flag}={value})    service: Service name, reserved for later service discovery and other functions    snake_tag: The tag is named in snake_case style(only works for form、query、json )    thriftgo/t: Option passwd through to thrift ({flag}={value})    unset_omitempty: When IDL is protobuf, the model field is generated and the omitempty tag is removed; when IDL is thrift, whether to add omitempty is determined by whether the field is “optional” or “required”  Update: $ hz help update NAME: hz update - Update an existing Hertz project USAGE: hz update [command options] [arguments...] OPTIONS: --client_dir value Specify the client path. If not specified, no client code is generated. --customize_package value Specify the package template. ({{Template Profile}}:) --exclude_file value, -E value Specify the files that do not need to be updated. (accepts multiple inputs) --handler_dir value Specify the handler path. --idl value Specify the IDL file path. (.thrift or .proto) (accepts multiple inputs) --json_enumstr Use string instead of num for json enums when idl is thrift. (default: false) --model_dir value Specify the model path. --no_recurse Generate master model only. (default: false) --option_package value, -P value Specify the package path. ({include_path}={import_path}) (accepts multiple inputs) --out_dir value Specify the project path. --proto_path value, -I value Add an IDL search path for includes. (Valid only if idl is protobuf) (accepts multiple inputs) --protoc value, -p value Specify arguments for the protoc. ({flag}={value}) (accepts multiple inputs) --snake_tag Use snake_case style naming for tags. (Only works for 'form', 'query', 'json') (default: false) --thriftgo value, -t value Specify arguments for the thriftgo. ({flag}={value}) (accepts multiple inputs) --unset_omitempty Remove 'omitempty' tag for generated struct. (default: false)  client_dir: Specify the path to generate client-side code, if not specified, it will not be generated; currently generates a global client for each service, and will provide rich client code capabilities later. Note: If you update the same set of IDL, the value of client_dir needs to be the same as when using new, otherwise it will generate redundant code that needs to be removed by the user.    customize_package: Customize the project package related templates, mainly for the handler template. For details, see:hz custom template use. Note: For an existing handler file, a handler function will be added according to the default template, and for handler files that do not exist yet, a handler will be generated according to a custom template.    exclude_file: Files that do not need to be updated(relative to the project path, multiple supported)    handler_dir: Specify the handler generation path, the default is “biz/handler”; Note: If you update the same set of IDL, the value of handler_dir needs to be the same as when using new, otherwise it will generate redundant code that needs to be removed by the user.    idl: IDL file path (.thrift or .proto)    json_enumstr: When IDL is thrift, json enums uses string instead of num (option passed through to thriftgo)    model_dir: Specify the model generation path, the default is “biz/model”; Note: If you update the same set of IDL, the value of model_dir needs to be the same as when using new, otherwise it will generate redundant code that needs to be removed by the user.    no_recurse: Generate only the model code for main IDL    option_package/P: Specify the path to the package, ({include_path}={import_path})    out_dir: Specify the project build path    proto_path/I: When IDL is protobuf, specify the search path for IDL, same as protoc’s -I command    protoc/p: Option passed through to protoc ({flag}={value})    snake_tag: The tag is named in snake_case style(only works for form、query、json )    thriftgo/t: Option passwd through to thrift ({flag}={value})    unset_omitempty: When IDL is protobuf, the model field is generated and the omitempty tag is removed; when IDL is thrift, whether to add omitempty is determined by whether the field is “optional” or “required”  Notes Notes on using protobuf IDL hz currently supports the syntax of proto2 / proto3\nWe hope that users specify go_package when defining the protobuf IDL, so that one is consistent with the semantics of protobuf and the location of the generated model can be determined by go_package. If the user does not specify go_package, hz will default the package of the proto file to go_package, which may have some unintended naming conflicts.\nFor example, go_package can be defined like this\noption go_package = \"hello.world\"; // or hello/world The generated path of model will be:\n${project path}/${model_dir}/hello/world\nThe handler file will take the last level of go_package as the generation path, and its generation path will be:\n${project path}/${handler_dir}/world\nThe router registration file will also take the last level of the go_package as the generation path, and the generation path will be:\n${project path}/biz/router/world\nNotes on using thrift IDL hz has no special requirements for the definition of thrift IDL, it only needs to comply with the grammar specification. The code generation path will be related to the thrift namespace\nFor example, a namespace can be defined like this\nnamespacegohello.worldThe generated path of model will be:\n${project path}/${model_dir}/hello/world\nThe handler file will take namespace as the generation path, and its generation path will be:\n${project path}/${handler_dir}/hello/world\nThe router registration file will also take namespace as the generation path, and its generation path will be:\n${project path}/biz/router/hello/world\nDescription of the behavior when using the update command  Notes on using custom path  For the convenience of user, hz provides custom handler paths, model paths, templates, etc. However, hz does not save the current project information when creating a new project, so it can be considered as a stateless update when using the update command. Therefore, for the same set of IDL in new and update, using different custom information may result in duplicate code, for example, as follows:\nCreate a new project:\nhz new -idl demo.thrift // In this case, hz will generate the model under \"biz/model\" Update an existing project:\nhz update -idl demo.thrift --model_dir=my_model // In this case, hz will not update the model code under \"biz/model\", but under \"my_model\"; then the code under \"biz/model\" and \"my_model\" will be duplicated, and the new handler will depend on \"my_model\",while the previous handler will depend on \"biz/model\". In this case, you need to delete \u0026 change some code manually. Therefore, we hope that user use the update command with custom paths “client_dir”, “model_dir”, “handler_dir”, preferably same as new.\nBehavior of update handler  hz will generate handlers based on default/custom template when creating a new project, where each service generates a file that contains all the handler code defined by the service; if IDL defines multiple services, each service will generate a file, and these files are in the same path; for example:\n// demo.thrift namespacegohello.exampleserviceService1{HelloRespMethod1(1:HelloReqrequest)(api.get=\"/hello\");}serviceService2{HelloRespMethod2(1:HelloReqrequest)(api.get=\"/new\");}// Then the handler file generated by the IDL is as follows: ${handler_dir}/${namespace}/service1.go-\u003emethod1${handler_dir}/${namespace}/service2.go-\u003emethod2When a new method is added to the IDL, the handler template will be added at the end of the corresponding service file; note that the handler added here will use the default template, and the new service file will use a custom template if appropriate.\nBehavior of update router  The router code generated by hz in new mainly includes the following three:\n biz/router/${namespace}/${idlName}.go: Each primary IDL generates a corresponding routing registration code file, which registers all the routes defined in the IDL in a routing group, and sets the default middleware.    biz/router/${namespace}/middleware.go: The default middleware function corresponding to each primary IDL, which can be modified by the user to add specific middleware logic to a particular route.    biz/router/register.go: This file is responsible for calling the route registration generated by different IDL; for example, if i define service in both IDL “demo1.thrift” and “demo2.thrift”, then both files will generate the corresponding route registration code. register.go is responsible for calling the route registration functions of these two parts.  Based on the above description, a description of the router’s behavior during update is given:\n biz/${namespace}/${idlName}.go: Regenerate based on IDL every time, users should not change the code of this file, otherwise the code will be lost.    biz/${namespace}/middleware.go: Appends a currently unavailable middleware to the end each time.    biz/router/register.go: If there is a new IDL, the route registration method of the new IDL will be inserted.  ","categories":"","description":"","excerpt":"hz is a tool provided by the Hertz framework for generating code. …","ref":"/docs/hertz/tutorials/toolkit/toolkit/","tags":"","title":"hz toolkit usage"},{"body":"If you want to get more detailed monitoring data, e.g. message packet size, or want to adopt other data source, e.g. InfluxDB, you can implement the Trace interface according to your requirements and inject it by WithTracer Option.\n// Tracer is executed at the start and finish of an HTTP. type Tracer interface { Start(ctx context.Context, c *app.RequestContext) context.Context Finish(ctx context.Context, c *app.RequestContext) } You can get TraceInfo from ctx. What is more, from TraceInfo you can get request time cost, package size, and error information returned from request, etc. Usage example：\ntype ServerTracer struct{ // contain entities which recording metric } // Start record the beginning of an RPC invocation. func (s *ServerTracer) Start(ctx context.Context, _ *app.RequestContext) context.Context { // do nothing \treturn ctx } // Finish record after receiving the response of server. func (s *ServerTracer) Finish(ctx context.Context, c *app.RequestContext) { ti := c.GetTraceInfo() rpcStart := ti.Stats().GetEvent(stats.HTTPStart) rpcFinish := ti.Stats().GetEvent(stats.HTTPFinish) cost := rpcFinish.Time().Sub(rpcStart.Time()) // TODO: record the cost of request } ","categories":"","description":"","excerpt":"If you want to get more detailed monitoring data, e.g. message packet …","ref":"/docs/hertz/tutorials/framework-exten/monitor/","tags":"","title":"Monitoring Extension"},{"body":"The framework doesn’t provide any monitoring, but only provides a Tracer interface. This interface can be implemented by yourself and be injected via WithTracer Option.\n// Tracer is executed at the start and finish of an HTTP. type Tracer interface { Start(ctx context.Context, c *app.RequestContext) context.Context Finish(ctx context.Context, c *app.RequestContext) } hertz-contrib provides a default prometheus monitoring extension，which can be used to do:\n Server throughput monitoring Request latency monitoring  The default tags include: HTTP Method, statusCode. Requests related information are stored in RequestContext, and this variable can be accessed after monitoring metrics scraped. You can implement and extend the monitoring functions according to your own requirements. Usage example:\nServer\nimport ( \"context\" \"github.com/cloudwego/hertz/pkg/app\" \"github.com/cloudwego/hertz/pkg/app/server\" \"github.com/cloudwego/hertz/pkg/common/utils\" \"github.com/hertz-contrib/monitor-prometheus\" ) func main() { ··· h := server.Default(server.WithTracer(prometheus.NewServerTracer(\":9091\", \"/hertz\"))) h.GET(\"/ping\", func(c context.Context, ctx *app.RequestContext) { ctx.JSON(200, utils.H{\"ping\": \"pong\"}) }) h.Spin() ··· } Currently, Client doesn’t expose Tracer interface, but monitoring capabilities can be implemented through middleware.\nRelated Repository https://github.com/hertz-contrib/monitor-prometheus\n","categories":"","description":"","excerpt":"The framework doesn’t provide any monitoring, but only provides a …","ref":"/docs/hertz/tutorials/service-governance/monitoring/","tags":"","title":"Monitoring"},{"body":"Usage Add some options when creating a client：\nclient, err := echo.NewClient(\"targetService\", client.WithXXXX...) Basic Options WithClientBasicInfo func WithClientBasicInfo(ebi *rpcinfo.EndpointBasicInfo) Option Set the basic infos for client, such as ServiceName, Method and Tags.\nWithHostPorts func WithHostPorts(hostports ...string) Option Manually specifie one or more targets overrides the results discovered by the service and directly connects to the access.\nWithTransportProtocol func WithTransportProtocol(tp transport.Protocol) Option Set the transport protocol, configure the transport protocol on the message protocol. Thrift/KitexProtobuf can configure TTHeader, TTHeaderFramed, and Framed. In addition, Framed is not strictly a transmission protocol. In order to distinguish it for PurePayload, it is also configured as a transmission protocol. PurePayload means that there is no transmission protocol; if it is configured as GRPC, it means that the GRPC protocol is used. , the transmission protocol of GRPC is HTTP2, but for the convenience of users' understanding, it is directly used as the configuration of the transmission protocol. Note that configuring GRPC needs to use Protobuf to define Service. If GRPC is not configured, KitexProtobuf protocol is used by default.\nWithShortConnection func WithShortConnection() Option Enable short connections. More\nWithLongConnection func WithLongConnection(cfg connpool.IdleConfig) Option Enable long connections. More\nWithMuxConnection func WithMuxConnection(connNum int) Option Enable mux connections. Server side also need to turn on this option, or it won’t work. More\nWithMiddleware func WithMiddleware(mw endpoint.Middleware) Option Add a middleware that executes after service level circuit breaker and timeout middleware. More\nWithInstanceMW func WithInstanceMW(mw endpoint.Middleware) Option Add a middleware that executes after service discovery and load balance. If instance level circuit breaker exists, then it will execute after that. (If proxy is used, it will not be called, such as mesh mode). More\nWithMiddlewareBuilder func WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option Add middleware depends on the context passed in by the framework that contains runtime configuration information (the context of non-RPC calls), so that the middleware can take advantage of the framework’s information when initializing.\nWithCircuitBreaker func WithCircuitBreaker(s *circuitbreak.CBSuite) Option Set the circuit breaker, which includes service circuit break and instance circuit break by default, using the example:\nvar opts []client.Option cbs := circuitbreak.NewCBSuite(circuitbreak.RPCInfo2Key) opts = append(opts, client.WithCloseCallbacks(func() error { // Circuit Breaker Close method is injected into CloseCallbacks to release circuit breaker related resources when the client is destroyed  return cs.cbs.Close() })) opts = append(opts, client.WithCircuitBreaker(cbs)) // Dynamically updates the circuit breaker configuration cbs.UpdateServiceCBConfig(key, config) cbs.UpdateInstanceCBConfig(key, config) For more details, please visit Circuit Breaker.\nWithFailureRetry func WithFailureRetry(p *retry.FailurePolicy) Option Set timeout retry rules, you can configure the maximum number of retries, the maximum time spent accumulated, the threshold of the retry circuit fault rate, the DDL abort and backoff policy. More\nWithBackupRequest func WithBackupRequest(p *retry.BackupPolicy) Option Set the policy for Backup Request, which can configure the number of requests, circuit breaker abort, and link abort. More\nWithRPCTimeout func WithRPCTimeout(d time.Duration) Option Set RPC timeout. More\nWithConnectTimeout func WithConnectTimeout(d time.Duration) Option Set connect timeout. More\nWithTimeoutProvider func WithTimeoutProvider(p rpcinfo.TimeoutProvider) Option Add a TimeoutProvider to set the RPC timeout, connection timeout, etc. policies as a whole. If You use Both WithRPCTimeout or WithConnectTimeout, the settings here will be overridden.\nWithDestService func WithDestService(svr string) Option Specify the service name of the target side of the call.\nWithTag func WithTag(key, val string) Option Add some meta information to the client, such as idc, cluster, etc., for scenarios such as auxiliary service discovery.\nWithStatsLevel func WithStatsLevel(level stats.Level) Optiong Set the stats level for client. More\ngRPC Options  These options only works for scenarios where the transport protocol uses gRPC, with some parameter adjustments to gRPC transfers.\n WithGRPCConnPoolSize func WithGRPCConnPoolSize(s uint32) Option WithGRPCConnPoolSize sets the value for the client connection pool size. In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation. You should adjust the size according to the actual situation.\nWithGRPCWriteBufferSize func WithGRPCWriteBufferSize(s uint32) Option WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire. The corresponding memory allocation for this buffer will be twice the size to keep syscalls low. The default value for this buffer is 32KB. Zero will disable the write buffer such that each write will be on underlying connection. Note: A Send call may not directly translate to a write. It corresponds to the WriteBufferSize ServerOption of gRPC.\nWithGRPCReadBufferSize func WithGRPCReadBufferSize(s uint32) Option WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most for one read syscall. The default value for this buffer is 32KB. Zero will disable read buffer for a connection so data framer can access the underlying conn directly. It corresponds to the ReadBufferSize ServerOption of gRPC.\nWithGRPCInitialWindowSize func WithGRPCInitialWindowSize(s uint32) Option WithGRPCInitialWindowSize returns a Option that sets window size for stream. The lower bound for window size is 64K and any value smaller than that will be ignored. It corresponds to the InitialWindowSize ServerOption of gRPC.\nWithGRPCInitialConnWindowSize func WithGRPCInitialConnWindowSize(s uint32) Option WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection. The lower bound for window size is 64K and any value smaller than that will be ignored. It corresponds to the InitialConnWindowSize ServerOption of gRPC.\nWithGRPCMaxHeaderListSize func WithGRPCMaxHeaderListSize(s uint32) Option WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size of header list that the server is prepared to accept. It corresponds to the MaxHeaderListSize ServerOption of gRPC.\nWithGRPCKeepaliveParams func WithGRPCKeepaliveParams(kp grpc.ClientKeepalive) Option WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport. It corresponds to the WithKeepaliveParams DialOption of gRPC.\nAdvanced Options WithSuite func WithSuite(suite Suite) Option Set up a specific configuration, customize according to the scene, configure multiple options and middlewares combinations and encapsulations in the Suite. More\nWithProxy func WithProxy(p proxy.ForwardProxy) Option For proxy scenarios (such as Mesh Egress), do some configuration processing, return proxy address, configure proxy. After ForwardProxy, the framework does not perform service discovery, circuit breakers, and InstanceMWs.\nWithRetryContainer func WithRetryContainer(rc *retry.Container) Option Manually set up RetryContainer. Used to retry the strategy in combination with circuit breakers. At present, three rapid implementation schemes are available: NewRetryContainer, NewRetryContainerWithCB and NewRetryContainerWithCBStat.\n NewRetryContainerWithCB (recommended)  If you have already configured the circuit breaker, it is recommended to reuse the circuit breaker with RetryContainer to avoid additional statistics, you can use the NewRetryContainerWithCB, such as the example in the example below, enable the circuit breaker scenario, and at the same time pass the circuit breaker to RetryContainer:\ncbs := circuitbreak.NewCBSuite(circuitbreak.RPCInfo2Key) retryC := retry.NewRetryContainerWithCB(cbs.ServiceControl(), cbs.ServicePanel()) var opts []client.Option opts = append(opts, client.WithRetryContainer(retryC)) // enable service circuit breaker  opts = append(opts, client.WithMiddleware(cbs.ServiceCBMW()))  NewRetryContainer  Specifies the default RetryContainer for the retry policy, which has a built-in circuit breaker.   NewRetryContainerWithCBStat  To customize the built-in circuit breaker ServiceCBKeyFunc settings, you can use the NewRetryContainerWithCBStat method:\ncbs := circuitbreak.NewCBSuite(YourGenServiceCBKeyFunc) retry.NewRetryContainerWithCBStat(cbs.ServiceControl(), cbs.ServicePanel()) WithWarmingUp func WithWarmingUp(wuo *warmup.ClientOption) Option Set warming up option. Kitex supports client warm-up, which allows you to pre-initialize the relevant components of service discovery and connection pooling when creating the client, avoiding large delays on the first request.\nWarm up service discovery:\ncli, err := myservice.NewClient(psm, client.WithWarmingUp(\u0026warmup.ClientOption{ ResolverOption: \u0026warmup.ResolverOption{ Dests: []*rpcinfo.EndpointBasicInfo{ \u0026rpcinfo.EndpointBasicInfo{ ServiceName: psm, Method: method, Tags: map[string]string{ \"cluster\": \"default\", }, }, }, }, })) Warm up connection pool:\ncli, err := myservice.NewClient(psm, client.WithWarmingUp(\u0026warmup.ClientOption{ PoolOption: \u0026warmup.PoolOption{ ConnNum: 2, }, })) WithCloseCallbacks func WithCloseCallbacks(callback func() error) Option Set close callback function.\nWithErrorHandler func WithErrorHandler(f func(error) error) Option Set the error handler function, which is executed after the server handler is executed and before the middleware executes.\nWithGeneric func WithGeneric(g generic.Generic) Option Specifie the generalization call type, which needs to be used in conjunction with the generalization Client/Server. More\nWithACLRules func WithACLRules(rules ...acl.RejectFunc) Option Set ACL permission access control, which is executed before service discovery. More\nWithConnReporterEnabled func WithConnReporterEnabled() Option Enable connection pool reporter. More\nWithHTTPConnection func WithHTTPConnection() Option Specifie client use RPC over http.\nExtended Options WithTracer func WithTracer(c stats.Tracer) Option Add an additional Tracer. More\nWithResolver func WithResolver(r discovery.Resolver) Option Specifie a resolver to do service discovery. More\nWithHTTPResolver func WithHTTPResolver(r http.Resolver) Option Set HTTP resolver. More\nWithLoadBalancer func WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option Set load balancer. More\nWithBoundHandler func WithBoundHandler(h remote.BoundHandler) Option Add a new IO Bound handler. More\nWithCodec func WithCodec(c remote.Codec) Option Specifie a Codec for scenarios that require custom protocol. More\nWithPayloadCodec func WithPayloadCodec(c remote.PayloadCodec) Option Specifie a PayloadCodec. More\nWithMetaHandler func WithMetaHandler(h remote.MetaHandler) Option Add a meta handler for customizing transparent information in conjunction with the transport protocol, such as service name, invocation method, machine room, cluster, env, tracerInfo. More\nWithFirstMetaHandler func WithFirstMetaHandler(h remote.MetaHandler) Option Add a meta handler at the first position.\nWithTransHandlerFactory func WithTransHandlerFactory(f remote.ClientTransHandlerFactory) Option Set transHandlerFactory. More\nWithDiagnosisService func WithDiagnosisService(ds diagnosis.Service) Option Set diagnosis service. More\nWithDialer func WithDialer(d remote.Dialer) Option Set dialer.\nWithConnPool func WithConnPool(pool remote.ConnPool) Option Set connection pool.\n","categories":"","description":"","excerpt":"Usage Add some options when creating a client：\nclient, err := …","ref":"/docs/kitex/tutorials/options/client_options/","tags":"","title":"Client Option"},{"body":"用法 在创建客户端时，带上 Option 参数即可：\nclient, err := echo.NewClient(\"targetService\", client.WithXXXX...) 基础 Option 基本信息 - WithClientBasicInfo func WithClientBasicInfo(ebi *rpcinfo.EndpointBasicInfo) Option 设置 Client 侧的 RPC 调用基本信息，例如 ServiceName，Method 和 Tags。\nIP 端口 - WithHostPorts func WithHostPorts(hostports ...string) Option 手动指定一个或者多个目标端，会覆盖掉服务发现的结果，直接直连访问。\n传输协议 - WithTransportProtocol func WithTransportProtocol(tp transport.Protocol) Option 设置传输协议，在消息协议上配置传输协议。Thrift/KitexProtobuf 可以配置 TTHeader、TTHeaderFramed、Framed，另外，Framed 严格意义上并不算传输协议，为区分用于 PurePayload 将其也作为传输协议配置，PurePayload 表示没有传输协议；如果配置为 GRPC 表示使用 GRPC 协议，GRPC 的传输协议是 HTTP2，但为便于用户理解，直接作为传输协议的配置，注意配置 GRPC 需要用 Protobuf 定义 Service，如果没有配置 GRPC，默认使用的是 KitexProtobuf 协议。\n短连接 - WithShortConnection func WithShortConnection() Option 是否启用短连接，详见连接类型-短连接。\n长连接 - WithLongConnection func WithLongConnection(cfg connpool.IdleConfig) Option 是否启用长连接，详见连接类型-长连接。\n多路复用 - WithMuxConnection func WithMuxConnection(connNum int) Option 是否启用连接多路复用，需要同时在服务端也开启设置，详见连接类型-连接多路复用。\n中间件扩展 - WithMiddleware func WithMiddleware(mw endpoint.Middleware) Option 添加一个中间件，在 Service 熔断和超时中间件之后执行。用法参考 Middleware 扩展。\n中间件扩展 - WithInstanceMW func WithInstanceMW(mw endpoint.Middleware) Option 用于添加一个中间件，在服务发现和负载均衡之后执行，如果有实例熔断器，会在实例熔断器后执行（如果使用了 Proxy 则不会调用到，如 Mesh 模式下）。用法参考 Middleware 扩展。\n中间件扩展 - WithMiddlewareBuilder func WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option 用于创建并添加中间件，可以根据 ctx 判断场景并创建中间件。 ctx 是框架传入的包含运行时配置信息的上下文（非 RPC 调用的上下文），以便中间件初始化时能利用框架的信息。\n熔断器 - WithCircuitBreaker func WithCircuitBreaker(s *circuitbreak.CBSuite) Option 设置熔断，默认包含 service 熔断和 instance 熔断，使用示例：\nvar opts []client.Option cbs := circuitbreak.NewCBSuite(circuitbreak.RPCInfo2Key) opts = append(opts, client.WithCloseCallbacks(func() error { // 熔断器的 Close 方法注入到 CloseCallbacks，用于在client销毁时，释放熔断相关资源  return cs.cbs.Close() })) opts = append(opts, client.WithCircuitBreaker(cbs)) // 动态更新熔断配置 cbs.UpdateServiceCBConfig(key, config) cbs.UpdateInstanceCBConfig(key, config) 关于熔断说明，详见熔断器。\n超时重试 - WithFailureRetry func WithFailureRetry(p *retry.FailurePolicy) Option 设置超时重试规则，可以配置最大重试次数，累计最大耗时，重试熔断错误率阈值，DDL 中止和退避策略等，详见请求重试。\n备份请求 - WithBackupRequest func WithBackupRequest(p *retry.BackupPolicy) Option 设置 Backup Request 的策略，可以配置请求次数、熔断中止、链路中止等，详见请求重试。\n超时设置 - WithRPCTimeout func WithRPCTimeout(d time.Duration) Option 进行 RPC 超时设置，详见超时控制。\n超时设置 - WithConnectTimeout func WithConnectTimeout(d time.Duration) Option 设置连接超时，详见超时控制。\n超时设置 - WithTimeoutProvider func WithTimeoutProvider(p rpcinfo.TimeoutProvider) Option 添加一个 TimeoutProvider 来整体设置 RPC 超时，连接超时等策略。若同时使用了 WithRPCTimeout 或 WithConnectTimeout，那么这里的设置会被覆盖。\n指定服务 - WithDestService func WithDestService(svr string) Option 指定调用目标端的服务名称，在 NewClient 中，第一个 string 参数就封装了这个 Option ，服务发现等场景会用到该字段。\n添加标签 - WithTag func WithTag(key, val string) Option 为 Client 添加一些元信息，例如 idc，cluster 等，用于辅助服务发现等场景。\n埋点粒度 - WithStatsLevel func WithStatsLevel(level stats.Level) Optiong 为 Client 设置埋点粒度，详见埋点粒度。\ngRPC 相关配置  这类设置只对传输协议使用 gRPC 的场景生效，对 gRPC 传输进行一些参数调整。\n WithGRPCConnPoolSize func WithGRPCConnPoolSize(s uint32) Option 设置 gRPC 的连接池大小。只对传输协议使用 gRPC 的场景生效。\nWithGRPCWriteBufferSize func WithGRPCWriteBufferSize(s uint32) Option 设置 gRPC 写缓冲大小，写缓冲决定了每次批量调用底层写发送数据的大小。默认值为32KB，如果设置为0，则相当于禁用缓冲区，每次写操作都直接调用底层连接进行发送。该设置只对传输协议使用 gRPC 的场景生效。\nWithGRPCReadBufferSize func WithGRPCReadBufferSize(s uint32) Option 设置 gRPC 的读缓冲大小，读缓冲决定了每次批量从底层读取多少数据。默认值为32KB，如果设置为0，则相当于禁用缓冲区，每次读操作都直接从底层连接进行读操作。该设置只对传输协议使用 gRPC 的场景生效。\nWithGRPCInitialWindowSize func WithGRPCInitialWindowSize(s uint32) Option 设置 gRPC 每个 Stream 的初始收发窗口大小，最低为64KB，若设置的值小于最低值，则会被忽略。该设置只对传输协议使用 gRPC 的场景生效。\nWithGRPCInitialConnWindowSize func WithGRPCInitialConnWindowSize(s uint32) Option 设置 gRPC 单条连接上的初始窗口大小，最低为64KB，若设置的值小于最低值，则会被忽略。该设置只对传输协议使用 gRPC 的场景生效。\nWithGRPCMaxHeaderListSize func WithGRPCMaxHeaderListSize(s uint32) Option 设置 gRPC MaxHeaderListSize 参数，该参数决定了每次调用允许发送的header的最大条数。该设置只对传输协议使用 gRPC 的场景生效。\nWithGRPCKeepaliveParams func WithGRPCKeepaliveParams(kp grpc.ClientKeepalive) Option 设置 gRPC 客户端 Keepalive 的各项参数。该设置只对传输协议使用 gRPC 的场景生效。\n高级 Option 配套扩展 - WithSuite func WithSuite(suite Suite) Option 设置一套特定配置，可根据场景进行定制，在 Suite 中配置多个 Option 和 Middleware 的组合和封装，详见 Suite 扩展。\n代理 - WithProxy func WithProxy(p proxy.ForwardProxy) Option 用于代理场景（如 Mesh Egress）做一些配置处理、返回代理地址，配置 proxy.ForwardProxy 后，框架不会执行服务发现、熔断、InstanceMWs。\n重试 - WithRetryContainer func WithRetryContainer(rc *retry.Container) Option 手动设置 RetryContainer。用于结合熔断器进行重试策略。目前提供了 NewRetryContainer，NewRetryContainerWithCB 与 NewRetryContainerWithCBStat 三个快速实现方案。\n NewRetryContainerWithCB（建议）  ​ 若在已经配置熔断器的情况下，建议与 RetryContainer 复用熔断器，避免额外的统计，可以使用 NewRetryContainerWithCB ，例如下面的示例中，启用熔断器的场景，同时将熔断器透传给 RetryContainer：\ncbs := circuitbreak.NewCBSuite(circuitbreak.RPCInfo2Key) retryC := retry.NewRetryContainerWithCB(cbs.ServiceControl(), cbs.ServicePanel()) var opts []client.Option opts = append(opts, client.WithRetryContainer(retryC)) // enable service circuit breaker  opts = append(opts, client.WithMiddleware(cbs.ServiceCBMW()))   NewRetryContainer\n 指定重试策略的默认 RetryContainer，其内置了一个熔断器    NewRetryContainerWithCBStat\n  ​ 若想对内置的熔断器进行自定义 ServiceCBKeyFunc 设置，则可以使用 NewRetryContainerWithCBStat 方法：\ncbs := circuitbreak.NewCBSuite(YourGenServiceCBKeyFunc) retry.NewRetryContainerWithCBStat(cbs.ServiceControl(), cbs.ServicePanel()) 预热 - WithWarmingUp func WithWarmingUp(wuo *warmup.ClientOption) Option 设置预热。Kitex 支持了客户端预热，可以在创建客户端的时候预先初始化服务发现和连接池的相关组件，避免在首次请求时产生较大的延迟。\n预热服务发现：\ncli, err := myservice.NewClient(psm, client.WithWarmingUp(\u0026warmup.ClientOption{ ResolverOption: \u0026warmup.ResolverOption{ Dests: []*rpcinfo.EndpointBasicInfo{ \u0026rpcinfo.EndpointBasicInfo{ ServiceName: psm, Method: method, Tags: map[string]string{ \"cluster\": \"default\", }, }, }, }, })) 预热连接池：\ncli, err := myservice.NewClient(psm, client.WithWarmingUp(\u0026warmup.ClientOption{ PoolOption: \u0026warmup.PoolOption{ ConnNum: 2, }, })) 设置关闭时回调 - WithCloseCallbacks func WithCloseCallbacks(callback func() error) Option 设置客户端 Close 时的回调函数。\n异常处理器 - WithErrorHandler func WithErrorHandler(f func(error) error) Option 设置异常处理函数，该函数会在远程调用结束，中间件执行前被执行。\n泛化调用 - WithGeneric func WithGeneric(g generic.Generic) Option 指定泛化调用类型，泛化需要结合泛化 Client/Server 使用。详见 Kitex 泛化调用使用指南。\n权限控制 - WithACLRules func WithACLRules(rules ...acl.RejectFunc) Option 设置 ACL 权限访问控制，该模块会在服务发现之前执行，具体用法详见自定义访问控制。\n连接池监控 - WithConnReporterEnabled func WithConnReporterEnabled() Option 设置连接池状态监控，详见连接类型-状态监控。\n启用 HTTP 连接 - WithHTTPConnection func WithHTTPConnection() Option 指定客户端使用 netpoll 提供的 http 连接进行 RPC 交互。\n扩展 Option 链路监控 - WithTracer func WithTracer(c stats.Tracer) Option 额外添加一个 Tracer 进行链路监控，详见链路跟踪-自定义 tracer。\n服务发现 - WithResolver func WithResolver(r discovery.Resolver) Option 指定一个 Resolver 进行服务发现，用法详见服务发现。\nHTTP 解析器 - WithHTTPResolver func WithHTTPResolver(r http.Resolver) Option 指定HTTP Resolver，详见直连访问-自定义 DNS resolver。\n负载均衡 - WithLoadBalancer func WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option 设置负载均衡器，详见负载均衡。\nIO Bound 处理器 - WithBoundHandler func WithBoundHandler(h remote.BoundHandler) Option 自定义 IO Bound，详见 Transport Pipeline-Bound 扩展。\n编解码 - WithCodec func WithCodec(c remote.Codec) Option 指定 Codec，详见编解码协议扩展\nPayload 编解码 - WithPayloadCodec func WithPayloadCodec(c remote.PayloadCodec) Option 指定 PayloadCodec，详见编解码协议扩展\n元信息处理 - WithMetaHandler func WithMetaHandler(h remote.MetaHandler) Option 添加一个元信息处理器，用法详见元信息传递扩展。\n元信息处理 - WithFirstMetaHandler func WithFirstMetaHandler(h remote.MetaHandler) Option 在 MetaHandler 链的最前面添加一个元信息处理器，功能同 WithMetaHandler 类似。\n传输设置 - WithTransHandlerFactory func WithTransHandlerFactory(f remote.ClientTransHandlerFactory) Option 自定义传输模块，详见传输模块扩展。\n诊断扩展 - WithDiagnosisService func WithDiagnosisService(ds diagnosis.Service) Option 添加一个自定义的 Diagnosis Service，用来获取更多的诊断信息，详见诊断模块扩展。\nDialer 扩展 - WithDialer func WithDialer(d remote.Dialer) Option 手动指定 Dialer。通常情况下 Dialer 在其他配置中已经进行了配套实现，一般情况下不建议使用。\n连接池扩展 - WithConnPool func WithConnPool(pool remote.ConnPool) Option 手动设置连接池。通常情况下 ConnPool 在其他配置中已经进行了配套实现，一般情况下不建议使用。\n","categories":"","description":"","excerpt":"用法 在创建客户端时，带上 Option 参数即可：\nclient, err := …","ref":"/zh/docs/kitex/tutorials/options/client_options/","tags":"","title":"Client Option"},{"body":"Hertz integrated Netpoll and Golang network lib by default. Users can choose the appropriate one according to the actual scenarios to meet the best performance.\nUsage While creating a server, Hertz uses netpoll by default, but this behavior can be modified by configuration:\nserver.New(server.WithTransport(standard.NewTransporter)) server.New(server.WithTransport(netpoll.NewTransporter)) While creating a Client, it can also be modified by configuration:\nclient.NewClient(client.WithDialer(standard.NewDialer())) client.NewClient(client.WithDialer(netpoll.NewDialer())) Choosing appropriate network library  If you need to start a TLS server, Please use go net lib instead. netpoll is now working on it but not ready yet. Due to the different I/O trigger model between the two network libs, go net for ET model and netpoll for LT model, which makes the application scenarios of the two libs somewhat different. Under the ET mode, Read / Write events will be handled by the framework. Under the LT mode, Read / Write events will be handled by the network lib itself instead. So with the small size requests, better schedule strategy provided by netpoll will makes LT model perform better; But under the situation with large size requests, since the Read / Write is not controlled by the framework layer, it may cause memory pressure because large amount of data will be loaded into the memory but can not be handled in time.   Under the situation with large request size ( generally larger than 1M ), go net lib with streaming is recommended. In other situation, netpoll lib is recommended for extreme performance.  ","categories":"","description":"","excerpt":"Hertz integrated Netpoll and Golang network lib by default. Users can …","ref":"/docs/hertz/tutorials/basic-feature/network-lib/","tags":"","title":"Network Lib"},{"body":"hz 是 Hertz 框架提供的一个用于生成代码的命令行工具。目前，hz 可以基于 thrift 和 protobuf 的 IDL 生成 Hertz 项目的脚手架。\n安装  确保 GOPATH 环境变量已经被正确地定义（例如 export GOPATH=~/go）并且将$GOPATH/bin添加到 PATH 环境变量之中（例如 export PATH=$GOPATH/bin:$PATH）；请勿将 GOPATH 设置为当前用户没有读写权限的目录 安装 hz：  go install github.com/cloudwego/hertz/cmd/hz@latest 验证是否安装成功 hz -v, 如果显示如下版本的信息，则说明安装成功  hz version v0.1.0 注意，由于 hz 会为自身的二进制文件创建软链接，因此请确保 hz 的安装路径具有可写权限。\n运行模式 要使用 thrift 或 protobuf 的 IDL 生成代码，需要安装相应的编译器：thriftgo 或 protoc 。\nhz 生成的代码里，一部分是底层的编译器生成的（通常是关于 IDL 里定义的结构体），另一部分是 IDL 中用户定义的路由、method 等信息。用户可直接运行该代码。\n从执行流上来说，当 hz 使用 thrift IDL 生成代码时，hz 会调用 thriftgo 来生成 go 结构体代码，并将自身作为 thriftgo 的一个插件（名为 thrift-gen-hertz）来执行来生成其他代码。当用于 protobuf IDL 时亦是如此。\n$\u003e hz ... --idl=IDL | | thrift-IDL |---------\u003e thriftgo --gen go:... -plugin=hertz:... IDL | | protobuf-IDL ---------\u003e protoc --hertz_out=... --hertz_opt=... IDL 如何安装 thriftgo/protoc:\nthriftgo:\n$ GO111MODULE=on go install github.com/cloudwego/thriftgo@latest protoc:\n// brew 安装 $ brew install protobuf // 官方镜像安装，以 macos 为例 $ wget https://github.com/protocolbuffers/protobuf/releases/download/v3.19.4/protoc-3.19.4-osx-x86_64.zip $ unzip protoc-3.19.4-osx-x86_64.zip $ cp bin/protoc /usr/local/bin/protoc // 确保 include/google 放入 /usr/local/include下 $ cp -r include/google /usr/local/include/google 使用 基本使用 new: 创建一个 Hertz 新项目  创建新项目  // GOPATH 下执行，go mod 名字默认为当前路径相对GOPATH的路径，也可自己指定 hz new // 非GOPATH 下执行，需要指定 go mod 名 hz new -mod hertz/demo // 整理 \u0026 拉取依赖 go mod tidy 执行后会在当前目录下生成 Hertz 项目的脚手架。\n编译项目  go build 运行项目并测试  运行项目：\n./{{your binary}} 测试：\ncurl 127.0.0.1:8888/ping 如果返回{\"message\":\"pong\"}，说明接口调通。\n基于 thrift IDL 创建项目 new: 创建一个新项目  在当前目录下创建 thrift idl 文件  // idl/hello.thrift namespacegohello.examplestructHelloReq{1:stringName (api.query=\"name\");// 添加 api 注解为方便进行参数绑定 }structHelloResp{1:stringRespBody;}serviceHelloService{HelloRespHelloMethod(1:HelloReqrequest)(api.get=\"/hello\");}创建新项目  // GOPATH 下执行 hz new -idl idl/hello.thrift // 整理 \u0026 拉取依赖 go mod tidy 修改 handler，添加自己的逻辑  // handler path: biz/handler/hello/example/hello_service.go // 其中 \"hello/example\" 是 thrift idl 的 namespace // \"hello_service.go\" 是 thrift idl 中 service 的名字，所有 service 定义的方法都会生成在这个文件中  // HelloMethod . // @router /hello [GET] func HelloMethod(ctx context.Context, c *app.RequestContext) { var err error var req example.HelloReq err = c.BindAndValidate(\u0026req) if err != nil { c.String(400, err.Error()) return } resp := new(example.HelloResp) // 你可以修改整个函数的逻辑，而不仅仅局限于当前模板  resp.RespBody = \"hello,\" + req.Name // 添加的逻辑  c.JSON(200, resp) } 编译项目  go build 运行项目并测试  运行项目：\n./{{your binary}} 测试：\ncurl --location --request GET 'http://127.0.0.1:8888/hello?name=hertz' 如果返回{\"RespBody\":\"hello,hertz\"}，说明接口调通。\nupdate: 更新一个已有的项目  如果你的 thrift idl 有更新，例如：  // idl/hello.thrift namespacegohello.examplestructHelloReq{1:stringName (api.query=\"name\");}structHelloResp{1:stringRespBody;}structOtherReq{1:stringOther (api.body=\"other\");}structOtherResp{1:stringResp;}serviceHelloService{HelloRespHelloMethod(1:HelloReqrequest)(api.get=\"/hello\");OtherRespOtherMethod(1:OtherReqrequest)(api.post=\"/other\");}serviceNewService{HelloRespNewMethod(1:HelloReqrequest)(api.get=\"/new\");}切换到执行 new 命令的目录，更新修改后的 thrift idl  hz update -idl idl/hello.thrift  可以看到\n在 “biz/handler/hello/example/hello_service.go” 下新增了新的方法\n在 “biz/handler/hello/example” 下新增了文件 “new_service.go” 以及对应的 “NewMethod” 方法。\n  下面我们来开发 “OtherMethod” 接口\n// HelloMethod . // @router /hello [GET] func HelloMethod(ctx context.Context, c *app.RequestContext) { var err error var req example.HelloReq err = c.BindAndValidate(\u0026req) if err != nil { c.String(400, err.Error()) return } resp := new(example.HelloResp) // 你可以修改整个函数的逻辑，而不仅仅局限于当前模板  resp.RespBody = \"hello,\" + req.Name // 添加的逻辑  c.JSON(200, resp) } // OtherMethod . // @router /other [POST] func OtherMethod(ctx context.Context, c *app.RequestContext) { var err error // example.OtherReq 对应的model文件也会重新生成  var req example.OtherReq err = c.BindAndValidate(\u0026req) if err != nil { c.String(400, err.Error()) return } resp := new(example.OtherResp) // 增加的逻辑  resp.Resp = \"Other method: \" + req.Other c.JSON(200, resp) } 编译项目  go build 运行项目并测试  运行项目：\n./{{your binary}} 测试：\ncurl --location --request POST 'http://127.0.0.1:8888/other' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"Other\": \"other method\" }' 如果返回{\"Resp\":\"Other method: other method\"}，说明接口调通。\n基于 protobuf IDL 创建项目 new: 创建一个新项目   在当前目录下创建 protobuf idl 文件\n注：为在 protobuf 中支持 api 注解，请在使用了注解的 proto 文件中，import 下面的文件\n  // idl/api.proto; 注解拓展 syntax = \"proto2\";package api;import \"google/protobuf/descriptor.proto\";option go_package = \"/api\";extend google.protobuf.FieldOptions { optional string raw_body = 50101; optional string query = 50102; optional string header = 50103; optional string cookie = 50104; optional string body = 50105; optional string path = 50106; optional string vd = 50107; optional string form = 50108; optional string go_tag = 51001; optional string js_conv = 50109;}extend google.protobuf.MethodOptions { optional string get = 50201; optional string post = 50202; optional string put = 50203; optional string delete = 50204; optional string patch = 50205; optional string options = 50206; optional string head = 50207; optional string any = 50208; optional string gen_path = 50301; optional string api_version = 50302; optional string tag = 50303; optional string name = 50304; optional string api_level = 50305; optional string serializer = 50306; optional string param = 50307; optional string baseurl = 50308;}extend google.protobuf.EnumValueOptions { optional int32 http_code = 50401;}主 idl 定义：\n// idl/hello/hello.proto syntax = \"proto3\";package hello;option go_package = \"hertz/hello\";import \"api.proto\";message HelloReq { string Name = 1[(api.query)=\"name\"];}message HelloResp { string RespBody = 1;}service HelloService { rpc Method1(HelloReq) returns(HelloResp) { option (api.get) = \"/hello\"; }}创建新项目  // GOPATH 下执行, 如果主IDL的依赖和主IDL不在同一路径下，需要加入 -I 选项，其含义为IDL搜索路径，等同于 protoc 的 -I 命令 hz new -I idl -idl idl/hello/hello.proto // 整理 \u0026 拉取依赖 go mod tidy 修改 handler，添加自己的逻辑  // handler path: biz/handler/hello/hello_service.go // 其中 \"/hello\" 是 protobuf idl 中 go_package 的最后一级 // \"hello_service.go\" 是 protobuf idl 中 service 的名字，所有 service 定义的方法都会生成在这个文件中  // Method1 . // @router /hello [GET] func Method1(ctx context.Context, c *app.RequestContext) { var err error var req hello.HelloReq err = c.BindAndValidate(\u0026req) if err != nil { c.String(400, err.Error()) return } resp := new(hello.HelloResp) // 你可以修改整个函数的逻辑，而不仅仅局限于当前模板  resp.RespBody = \"hello,\" + req.Name // 添加的逻辑  c.JSON(200, resp) } 编译项目  go build 运行项目并测试  运行项目：\n./{{your binary}} 测试：\ncurl --location --request GET 'http://127.0.0.1:8888/hello?name=hertz' 如果返回{\"RespBody\":\"hello,hertz\"}，说明接口调通。\nupdate: 更新一个已有的项目  如果你的 protobuf idl 有更新，例如：  // idl/hello/hello.proto syntax = \"proto3\";package hello;option go_package = \"hertz/hello\";import \"api.proto\";message HelloReq { string Name = 1[(api.query)=\"name\"];}message HelloResp { string RespBody = 1;}message OtherReq { string Other = 1[(api.body)=\"other\"];}message OtherResp { string Resp = 1;}service HelloService { rpc Method1(HelloReq) returns(HelloResp) { option (api.get) = \"/hello\"; } rpc Method2(OtherReq) returns(OtherResp) { option (api.post) = \"/other\"; }}service NewService { rpc Method3(OtherReq) returns(OtherResp) { option (api.get) = \"/new\"; }}切换到执行 new 命令的目录，更新修改后的 protobuf idl  hz update -I idl -idl idl/hello/hello.proto 可以看到 在\"biz/handler/hello/hello_service.go\" 下新增了新的方法 在\"biz/handler/hello\" 下新增了文件 “new_service.go” 以及对应的 “Method3” 方法。  下面我们来开发 “Method2” 接口\n// Method1 . // @router /hello [GET] func Method1(ctx context.Context, c *app.RequestContext) { var err error var req hello.HelloReq err = c.BindAndValidate(\u0026req) if err != nil { c.String(400, err.Error()) return } resp := new(hello.HelloResp) // 你可以修改整个函数的逻辑，而不仅仅局限于当前模板  resp.RespBody = \"hello,\" + req.Name // 添加的逻辑  c.JSON(200, resp) } // Method2 . // @router /other [POST] func Method2(ctx context.Context, c *app.RequestContext) { var err error var req hello.OtherReq err = c.BindAndValidate(\u0026req) if err != nil { c.String(400, err.Error()) return } resp := new(hello.OtherResp) // 增加的逻辑  resp.Resp = \"Other method: \" + req.Other c.JSON(200, resp) } 编译项目  go build 运行项目并测试  运行项目：\n./{{your binary}} 测试：\ncurl --location --request POST 'http://127.0.0.1:8888/other' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"Other\": \"other method\" }' 如果返回{\"Resp\":\"Other method: other method\"}，说明接口调通。\n生成代码的结构 hz 生成的代码结构都类似，下面以\"基于 thrift IDL 创建项目\"小节生成的代码结构为例，说明 hz 生成的代码的含义。\n. ├── biz // business 层，存放业务逻辑相关流程 │ ├── handler // 存放 handler 文件 │ │ ├── hello // hello/example 对应 thrift idl 中定义的 namespace；而对于 protobuf idl，则是对应 go_package 的最后一级 │ │ │ └── example │ │ │ ├── hello_service.go // handler 文件，用户在该文件里实现 IDL service 定义的方法，update 时会查找 当前文件已有的 handler 在尾部追加新的 handler │ │ │ └── new_service.go // 同上，idl 中定义的每一个 service 对应一个文件 │ │ └── ping.go // 默认携带的 ping handler，用于生成代码快速调试，无其他特殊含义 │ ├── model // IDL 内容相关的生成代码 │ │ └── hello // hello/example 对应 thrift idl 中定义的 namespace；而对于 protobuf idl，则是对应 go_package │ │ └── example │ │ └── hello.go // thriftgo 的产物，包含 hello.thrift 定义的内容的 go 代码，update 时会重新生成 │ └── router // idl 中定义的路由相关生成代码 │ ├── hello // hello/example 对应 thrift idl 中定义的namespace；而对于 protobuf idl，则是对应 go_package 的最后一级 │ │ └── example │ │ ├── hello.go // hz 为 hello.thrift 中定义的路由生成的路由注册代码；每次 update 相关 idl 会重新生成该文件 │ │ └── middleware.go // 默认中间件函数，hz 为每一个生成的路由组都默认加了一个中间件；update 时会查找当前文件已有的 middleware 在尾部追加新的 middleware │ └── register.go // 调用注册每一个 idl 文件中的路由定义；当有新的 idl 加入，在更新的时候会自动插入其路由注册的调用；勿动 ├── go.mod // go.mod 文件，如不在命令行指定，则默认使用相对于GOPATH的相对路径作为 module 名 ├── idl // 用户定义的idl，位置可任意 │ └── hello.thrift ├── main.go // 程序入口 ├── router.go // 用户自定义除 idl 外的路由方法 └── router_gen.go // hz 生成的路由注册代码，用于调用用户自定义的路由以及 hz 生成的路由 支持的 api 注解  Field 注解可用于参数绑定及校验\nMethod 注解可用于生成路由注册相关代码\n 支持的 api 注解：    Field 注解      注解 说明   api.raw_body 生成 “raw_body” tag   api.query 生成 “query” tag   api.header 生成 “header” tag   api.cookie 生成 “cookie” tag   api.body 生成 “json” tag   api.path 生成 “path” tag   api.form 生成 “form” tag   api.go_tag (protobuf)\ngo.tag (thrift) 透传 go_tag，会生成 go_tag 里定义的内容   api.vd 生成 “vd” tag       Method 注解      注解 说明   api.get 定义 GET 方法及路由   api.post 定义 POST 方法及路由   api.put 定义 PUT 方法及路由   api.delete 定义 DELETE 方法及路由   api.patch 定义 PATCH 方法及路由   api.options 定义 OPTIONS 方法及路由   api.head 定义 HEAD 方法及路由   api.any 定义 ANY 方法及路由    使用方法： Field 注解： Thrift：\nstructDemo{1:stringDemo (api.query=\"demo\",api.path=\"demo\");2:stringGoTag (go.tag=\"goTag:\"tag\"\");3:stringVd (api.vd=\"$!='your string'\");}Protobuf:\nmessage Demo { string Demo = 1[(api.query)=\"demo\",(api.path)=\"demo\"]; string GoTag = 2[(api.go_tag)=\"goTag:\"tag\"\"]; string Vd = 3[(api.vd)=\"$!='your string'\"];}Method 注解： Thrift：\nserviceDemo{RespMethod(1:Reqrequest)(api.get=\"/route\");}Protobuf:\nservice Demo { rpc Method(Req) returns(Resp) { option (api.get) = \"/route\"; }}命令行参数说明 Global: $ hz --help NAME: hz - A idl parser and code generator for Hertz projects USAGE: hz [global options] command [command options] [arguments...] VERSION: 0.0.1 COMMANDS: new Generate a new Hertz project update Update an existing Hertz project help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --help, -h show help (default: false) --verbose turn on verbose mode (default: false) --version, -v print the version (default: false)  New: 创建一个新的 Hertz 项目    update: 更新一个已存在的 Hertz 项目  New: $ hz help new NAME: hz new - Generate a new Hertz project USAGE: hz new [command options] [arguments...] OPTIONS: --client_dir value Specify the client path. If not specified, no client code is generated. --customize_layout value Specify the layout template. ({{Template Profile}}:{{Rendering Data}}) --customize_package value Specify the package template. ({{Template Profile}}:) --exclude_file value, -E value Specify the files that do not need to be updated. (accepts multiple inputs) --handler_dir value Specify the handler path. --idl value Specify the IDL file path. (.thrift or .proto) (accepts multiple inputs) --json_enumstr Use string instead of num for json enums when idl is thrift. (default: false) --model_dir value Specify the model path. --module value, --mod value Specify the Go module name to generate go.mod. --no_recurse Generate master model only. (default: false) --option_package value, -P value Specify the package path. ({include_path}={import_path}) (accepts multiple inputs) --out_dir value Specify the project path. --proto_path value, -I value Add an IDL search path for includes. (Valid only if idl is protobuf) (accepts multiple inputs) --protoc value, -p value Specify arguments for the protoc. ({flag}={value}) (accepts multiple inputs) --service value Specify the service name. --snake_tag Use snake_case style naming for tags. (Only works for 'form', 'query', 'json') (default: false) --thriftgo value, -t value Specify arguments for the thriftgo. ({flag}={value}) (accepts mul  client_dir: 指定 client 侧代码的生成路径，如果不指定则不生成；当前为每个 service 生成一个全局的 client，后续会提供更丰富的 client 代码能力    customize_layout: 自定义项目 layout 模板，具体详见：自定义模板使用    customize_package: 自定义项目 package 相关模板，主要可针对 handler 模板进行定制化，具体详见：自定义模板使用    exclude_file: 不需要更新的文件(相对项目路径，支持多个)    handler_dir: 指定 handler 的生成路径，默认为 “biz/handler”    idl: idl 文件路径(.thrift 或者.proto)    json_enumstr: 当 idl 为 thrift 时，json enums 使用 string 代替 num(透传给 thriftgo 的选项)    model_dir: 指定 model 的生成路径，默认为\"biz/model\"    module/mod: 指定 go mod 的名字，非 GOPATH 下必须指定，GOPATH 下默认以相对于 GOPATH 的路径作为名字    no_recurse: 只生成主 idl 的 model 代码    option_package/P: 指定包的路径，({include_path}={import_path})    out_dir: 指定项目生成路径    proto_path/I: 当 idl 为 protobuf 时，指定 idl 的搜索路径，同 protoc 的 -I 指令    protoc/p: 透传给 protoc 的选项({flag}={value})    service: 服务名，为之后做服务发现等功能预留    snake_tag: tag 使用 snake_case 风格命名(仅对 form、query、json 生效)    thriftgo/t: 透传给 thriftgo 的选项({flag}={value})    unset_omitempty: 当 idl 为 protobuf 时，生成 model field，去掉 omitempty tag；当 idl 为 thrift 时，是否添加 omitempty 根据 field 是 “optional\"还是\"required\"决定  Update: $ hz help update NAME: hz update - Update an existing Hertz project USAGE: hz update [command options] [arguments...] OPTIONS: --client_dir value Specify the client path. If not specified, no client code is generated. --customize_package value Specify the package template. ({{Template Profile}}:) --exclude_file value, -E value Specify the files that do not need to be updated. (accepts multiple inputs) --handler_dir value Specify the handler path. --idl value Specify the IDL file path. (.thrift or .proto) (accepts multiple inputs) --json_enumstr Use string instead of num for json enums when idl is thrift. (default: false) --model_dir value Specify the model path. --no_recurse Generate master model only. (default: false) --option_package value, -P value Specify the package path. ({include_path}={import_path}) (accepts multiple inputs) --out_dir value Specify the project path. --proto_path value, -I value Add an IDL search path for includes. (Valid only if idl is protobuf) (accepts multiple inputs) --protoc value, -p value Specify arguments for the protoc. ({flag}={value}) (accepts multiple inputs) --snake_tag Use snake_case style naming for tags. (Only works for 'form', 'query', 'json') (default: false) --thriftgo value, -t value Specify arguments for the thriftgo. ({flag}={value}) (accepts multiple inputs) --unset_omitempty Remove 'omitempty' tag for generated struct. (default: false)  client_dir: 指定 client 侧代码的生成路径，如果不指定则不生成；当前为每个 service 生成一个全局的 client，后续会提供更丰富的 client 代码能力。注意：如果对同一套 idl 进行 update，需要 client_dir 的值与使用 new 的时候相同，否则会生成冗余的代码，需要用户自行删除。    customize_package: 自定义项目 package 相关模板，主要可针对 handler 模板进行定制化，具体详见：自定义模板使用 。注意：对于已经存在的 handler 文件会按照默认模板新增 handler 函数，对于还未存在的 handler 文件，则会按照自定义模板来生成 handler。    exclude_file: 不需要更新的文件(相对项目路径，支持多个)    handler_dir: 指定 handler 的生成路径，默认为\"biz/handler”；注意：如果对同一套 idl 进行 update，需要 handler_dir 的值与使用 new 的时候相同，否则会生成冗余的代码，需要用户自行删除。    idl: idl 文件路径(.thrift 或者.proto)    json_enumstr: 当 idl 为 thrift 时，json enums 使用 string 代替 num(透传给 thriftgo 的选项)    model_dir: 指定 model 的生成路径，默认为\"biz/model\"；注意：如果对同一套 idl 进行 update，需要 model_dir 的值与使用 new 的时候相同，否则会生成重复的 model 代码且导致 handler 引用不一致。    no_recurse: 只生成主 idl 的 model 代码    option_package/P: 指定包的路径，({include_path}={import_path})    out_dir: 指定项目生成路径    proto_path/I: 当 idl 为 protobuf 时，指定 idl 的搜索路径，同 protoc 的 -I 指令    protoc/p: 透传给 protoc 的选项({flag}={value})    snake_tag: tag 使用 snake_case 风格命名(仅对 form、query、json 生效)    thriftgo/t: 透传给 thriftgo 的选项({flag}={value})    unset_omitempty: 当 idl 为 protobuf 时，生成 model field，去掉 mitempty tag；当 idl 为 thrift 时，是否添加 omitempty 根据 field 是 “optional\"还是\"required\"决定  注意事项 使用 protobuf IDL 的注意事项 hz 目前支持 proto2 / proto3 的语法\n我们希望用户在定义 protobuf idl 的时候指定 go_package，这样一来符合 protobuf 的语义，二来生成的 model 位置可以通过 go_package 来决定。如果用户不指定 go_package，hz 会默认将 proto 文件的 package 做为 go_package，可能会有一些预期外的命名冲突。\n例如，可以这样定义 go_package\noption go_package = \"hello.world\"; // or hello/world model 生成的路径会是：\n${项目路径}/${model_dir}/hello/world\nhandler 文件会取 go_package 最后一级作为生成路径，其生成路径会是：\n${项目路径}/${handler_dir}/world\nrouter 注册文件同样会取 go_package 最后一级作为生成路径，其生成路径会是：\n${项目路径}/biz/router/world\n使用 thrift IDL 的注意事项 hz 对于 thrift idl 的定义无特殊要求，符合语法规范即可。代码的生成路径会和 thrift 的 namespace 相关。\n例如，可以这样定义 namespace\nnamespacegohello.worldmodel 生成的路径会是：\n${项目路径}/${model_dir}/hello/world\nhandler 文件会取 namespace 作为生成路径，其生成路径会是：\n${项目路径}/${handler_dir}/hello/world\nrouter 注册文件同样会取 namespace 作为生成路径，其生成路径会是：\n${项目路径}/biz/router/hello/world\n使用 update 命令时的行为说明  使用自定义路径的注意事项  hz 为了用户使用方便，提供了自定义 handler 路径、model 路径、模板等功能。但是 hz 在创建一个新项目的时候并没有保存当前项目的信息，所以在使用 update 命令时可以认为是一种无状态的更新。因此对于同一套 idl 在 new 和 update 的时候，使用了不同的自定义信息，可能会产生重复的代码，举个例子，如下：\n创建新项目：\nhz new -idl demo.thrift // 此时，hz 会把 model 生成在 \"biz/mdoel\"下 更新项目：\nhz update -idl demo.thrift --model_dir=my_model // 此时，hz 不会更新\"biz/model\"下的 model 代码，而是会在\"my_model\"下；这时\"biz/model\"和\"my_model\"下的代码就会重复，且新生成的handler会依赖\"my_model\"，之前的handler会依赖\"biz/model\"，这时就需要用户手动删除\u0026改动一些代码了。 因此，我们希望用户使用 update 命令的时候，自定义的路径 “client_dir”、“model_dir”、“handler_dir”，最好和 new 相同。\nupdate handler 的行为  hz 在 new 项目的时候会根据默认模板/自定义模板来生成 handler，其中每个 service 生成一个文件，该文件包含了该 service 定义的所有 handler 代码；如果 idl 定义了多个 service，则每个 service 都会生成一个文件，这些文件都在同一路径下；举个例子：\n// demo.thrift namespacegohello.exampleserviceService1{HelloRespMethod1(1:HelloReqrequest)(api.get=\"/hello\");}serviceService2{HelloRespMethod2(1:HelloReqrequest)(api.get=\"/new\");}// 那么该 idl 生成的 handler 文件如下： ${handler_dir}/${namespace}/service1.go-\u003emethod1${handler_dir}/${namespace}/service2.go-\u003emethod2当该 idl 增加了新的 method 后，就会在对应 service 的文件的末尾追加 handler 模板；注意这里追加的 handler 会使用默认的模板，新生成 service 文件会根据情况使用自定义模板。\nupdate router 的行为  hz 在 new 的时候生成的 router 代码主要有如下三个：\n biz/router/${namespace}/${idlName}.go: 每个主 idl 都会生成对应的路由注册代码文件，该文件以路由组的方式注册 idl 中定义的所有路由，并设置默认的中间件。    biz/router/${namespace}/middleware.go: 每个主 idl 对应的默认中间件函数，用户可修改中间件函数，以此为特定的路由增加特定的中间件逻辑。    biz/router/register.go：该文件负责调用不同 idl 生成的路由注册；比如我在两个 idl “demo1.thrift”、“demo2.thrift\"中都定义了 service ，那么这两个文件都会生成对应的路由注册代码。register.go 负责调用这两部分的路由注册函数。  基于上述描述，给出 router 在 update 时的行为描述：\n biz/${namespace}/${idlName}.go: 每次都基于 idl 重新生成，用户不要改该文件代码，否则会丢失代码。    biz/${namespace}/middleware.go: 每次都会在尾部追加目前没有的 middleware。    biz/router/register.go: 如果有新增的 idl 会插入新的 idl 的路由注册方式。  ","categories":"","description":"","excerpt":"hz 是 Hertz 框架提供的一个用于生成代码的命令行工具。目前，hz 可以基于 thrift 和 protobuf 的 IDL …","ref":"/zh/docs/hertz/tutorials/toolkit/toolkit/","tags":"","title":"hz 命令行工具使用"},{"body":"The CORS (Cross-Origin Resource Sharing) mechanism allows a server to identify any origin other than its own so that browsers can access and load those resources. This mechanism is also used to check whether the server allows the browser to send a real request by sending a “precheck” request through the browser. In the “precheck” request header, there are HTTP methods and headers that the real request will use.\nHertz provides implementation of CORS middleware. The implementation here refers to GIN’s cors, and the usage method can be referred as well.\npackage main import ( \"time\" \"github.com/cloudwego/hertz/pkg/app/server\" \"github.com/hertz-contrib/cors\" ) func main() { h := server.Default() // CORS for https://foo.com and https://github.com origins, allowing:  // - PUT and PATCH methods  // - Origin header  // - Credentials share  // - Preflight requests cached for 12 hours  h.Use(cors.New(cors.Config{ AllowOrigins: []string{\"https://foo.com\"}, AllowMethods: []string{\"PUT\", \"PATCH\"}, AllowHeaders: []string{\"Origin\"}, ExposeHeaders: []string{\"Content-Length\"}, AllowCredentials: true, AllowOriginFunc: func(origin string) bool { return origin == \"https://github.com\" }, MaxAge: 12 * time.Hour, })) h.Spin() } ","categories":"","description":"","excerpt":"The CORS (Cross-Origin Resource Sharing) mechanism allows a server to …","ref":"/docs/hertz/tutorials/basic-feature/middleware/cors/","tags":"","title":"CORS"},{"body":"   date version author update content     2022-05-22 v1.0 wangjingpei first version of IDL Definition Specification for Mapping between Thrift and HTTP    This specification is the IDL definition standard for mapping between Thrift and HTTP. It contains definition standards of service, endpoint, request and response parameters. Kitex partially implements the specification, and the parts of annotation description indicate if it is supported.\nSpecification （1）The standards use annotations to describe API details such as API method, path, position and name of request and response parameters and so on\n（2）The annotations mentioned above are in the form of api.{key}={value}, the key is usually used to specify the position occurred of the parameter, such as header，cookie，query，body and so on. The value is used to specify the actual name of fields, some functional annotations like api.none, api.js_conv, api.http_code are exceptions\n（3）The annotations mentioned above must be in lower-case, uppercase or mixed case letters are not supported. for example api.get, api.header and so on.\nStandards for Whole File  A service corresponds to only one thrift main file. The methods in the main file are for the corresponding API of the current service. The main IDL file can refer to other thrift file definitions In principle, each method corresponds to one request and one response definition In principle, response can be reused, while request reuse is not recommended  Standards for Request Restrict  We should specify the name and type of associated HTTP API parameters, such as header, cookie and name by annotations. If not specified, the GET method corresponds to query parameters, while the POST method corresponds to body parameters automatically. The field name is used as parameter key If the HTTP request uses GET method, api.body annotation occurred in request definitions is invalid. Only annotations such as api.query, api.path or api.cookie are supported If one HTTP request uses POST method and the serialization strategy is form, the request field type like object and map is not supported. But Kitex doesn’t support form now, only json format.  Annotation Description    annotation description field restrict is KiteX supported     api.query api.querycorresponds url query parameter for HTTP request Only basic types (except for object, map ） and list split by , are supported ✅   api.path api.query corresponds url path parameter for HTTP request Only basic types are supported ✅   api.header api.header corresponds header parameter for HTTP request Only basic type and list split by , are supported ✅   api.cookie api. cookie corresponds cookie parameter for HTTP request Only basic types are supported ✅    api.body api.body corresponds body parameter for HTTP request\nBoth serialization type like json and form in body are supported Json is supported by default . The api.serializer annotation for method can sepify serialization json or form ✅, but only json format   api.raw_body api.raw_body corresponds raw body for HTTP request, we can get raw binary body  ✅   api.vd Parameter valid, we can refer HTTPs://github.com/bytedance/go-tagexpr/tree/master/validator for details  ❌   api.js_conv api.js_conv indicates the field should be string while the definition is in64, since int64 is not supported by typescript The annotation value should be true, if else will be treated as invalid ✅   api.raw_uri api.raw_uri is used for protocol exchange from HTTP to RPC, the RPC service can get the raw uri by the field Only string type is supported ❌    Example structItem{// For nested structures, if you want to set the serialization key, use gotag, such as `json: \"Id\"` 1:optionali64id(go.tag='json:\"id\"')2:optionalstringtext}typedefstringJsonDictstructBizRequest{// Corresponding to v_int64 in HTTP query, and the value range is (0, 200) 1:optionali64v_int64(api.query='v_int64')2:optionalstringtext(api.body='text')// Corresponding serialization key = text 3:optionali32token(api.header='token')// Corresponding token in HTTP header 4:optionalJsonDictjson_header(api.header='json_header')5:optionalItemsome(api.body='some')// Corresponding first level key = some 6:optionallist\u003cReqItem\u003ereq_items(api.query='req_items')// api.query Only list split by are supported,Other complex types are not supported 7:optionali32api_version(api.path='action')// Corresponding path in uri 8:optionali64uid(api.path='biz')// Corresponding path in uri 9:optionallist\u003ci64\u003ecids(api.query='cids')// Corresponding to comma separated numbers in query, for example: cids=1,2,3,4 Only supported list\u003ci64\u003e、list\u003ci32\u003e 10:optionallist\u003cstring\u003evids(api.query='vids')}Standards for Response Restrict  Only basic type like int64, string, bool and list split by , are supported for header value Response is defined directly by the business itself. The default JSON is serialized to the body, the key is the field name, and the annotation can be empty  Annotation Description    annotation description field restrict is KiteX supported     api.header api.header corresponds header parameter for HTTP response Only basic types and list split by , are supported ✅   api.http_code api.http_code corresponds HTTP code for HTTP response，such as 200, 500 and so on The annotation value should be true, if else will be treated as invalid ✅   api.body api.body corresponds body parameter for HTTP response  ✅   api.none api.body indicates the field will be ignored for HTTP response The annotation value should be true, if else will be treated as invalid ✅   api.js_conv api.js_conv indicates the field should be trans to string in response since it is int64 The annotation value should be true, if else will be treated as invalid ✅   api.raw_body api.raw_body indicates the field will be treated as raw body for response  ✅   api.cookie api.cookie indicates the field will be treated as cookie for HTTP response  ✅    Example // Finally, BizResponse json will be serialized as a return package to the client structRspItem{1:optionali64item_id// By default, it is serialized with the field name as key, which is equivalent to using gotag `json:\"item_id\"` 2:optionalstringtext}structBizResponse{// This field will be filled in the header returned to the client 1:optionalstringT (api.header='T')// first level key = 'rsp_items' 2:optionalmap\u003ci64,RspItem\u003ersp_items (api.body='rsp_items')3:optionali32v_enum (api.none='true')// Ignore current parameter 4:optionallist\u003cRspItem\u003ersp_item_list (api.body='rsp_item_list')// The business specifies the HTTP Code itself. If not specified, baseResp.StatuCode=0 -\u003e HTTPCode=200, other HTTPCode=500 5:optionali32http_code (api.http_code='true')6:optionallist\u003ci64\u003eitem_count (api.header='item_count')// Comma separated list when setting header 7:optionalstringtoken (api.cookie='token')// 对应 response Cookie 字段 }Standards for Method Restrict  The serialization specified by the api.serializer is valid for GET request Each URI corresponds one method in IDL by annotation, the annotation must be written  Annotation Description    annotation type description example is KiteX supported     api.get string api.get corresponds GET method, the value is the HTTP path, the uri syntex is in accord with gin( we can refer httprouter for detail) api.get = '/life/client/favorite/collect' ✅   api.post string api.post corresponds POST method, the uri syntex is in accord with gin( we can refer httprouter for detail) api.post='/life/client/favorite/collect' ✅   api.put string api.put corresponds PUT method, the uri syntex is in accord with gin( we can refer httprouter for detail) api.put='/life/client/favorite/collect' ✅   api.delete string api.delete corresponds DELETE method, the uri syntex is in accord with gin( we can refer httprouter for detail) api.delete='/life/client/favorite/collect' ✅   api.patch string api.delete corresponds DELETE method, the uri syntex is in accord with gin( we can refer httprouter for detail) api.patch='/life/client/favorite/collect' ✅   api.serializer string Request serialization type of client request Such as form, json, thrift or pb ❌    Example serviceBizService{// Example 1: get request BizResponseBizMethod1(1:biz.BizRequestreq)(api.get='/life/client/:action/:biz',api.baseurl='ib.snssdk.com',api.param='true',api.category='demo')// Example 2: post request BizResponseBizMethod2(1:biz.BizRequestreq)(api.post='/life/client/:action/:biz',api.baseurl='ib.snssdk.com',api.param='true',api.serializer='form')// Example 3: delete request BizResponseBizMethod3(1:biz.BizRequestreq)(api.post='/life/client/:action/:biz',api.baseurl='ib.snssdk.com',api.param='true',api.serializer='json')}","categories":"","description":"","excerpt":"   date version author update content     2022-05-22 v1.0 wangjingpei …","ref":"/docs/kitex/tutorials/advanced-feature/generic-call/thrift_idl_annotation_standards/","tags":"","title":"IDL Definition Specification for Mapping between Thrift and HTTP"},{"body":"   日期 版本 作者 变更内容     2022-05-22 v1.0 王景佩 第一版 Thrift-HTTP 映射的 IDL 规范    本规范是 Thrift 与 HTTP 接口映射的 IDL 定义标准，包括服务、接口以及请求 request、response 参数定义规范和错误码定义规范。Kitex 部分实现了该规范，注解说明有标注支持情况。\n规范说明 （1）本规范采用注解方式来描述接口信息，包括接口的 method, path 以及接口请求参数，返回参数位置（如 header，cookie )、名称等信息\n（2）本规范所述注解采用 api.{key}={value} 的形式，其中key通常用于指定该字段出现的位置如（header，cookie，query，body 等), value 用于指定该字段在对应位置的实际名称, 一些功能性注解（如api.none, api.js_conv, api.http_code) 除外\n（3）本规范中定义的IDL注解如 api.get, api.header 等，只支持小写，不支持大写或者大小写混用如api.GET, api.Header\n文件整体规范  一个服务 service 对应一个 thrift 主文件，主文件里的 method 都是针对当前服务对应接口，主文件可以引用其它 thrift 文件定义 每个 Method 原则上对应一个 Request 和 Response 定义 原则上不建议 Request 复用，可以复用 Response  Request 规范 约束  接口请求字段需要使用注解关联到 HTTP 的某类参数和参数名称。如果无注解，GET 方法接口关联query 参数, POST 方法关联 body 参数, 字段名对应参数key 如果 HTTP 请求是采用 GET 方式的，那么 request 定义中出现的 api.body 注解无效，只有api.query, api.path, api.cookie 有效。 如果 HTTP 请求是采用 POST 方式且序列化方式是 form 的，那么 request 的字段不能有复杂结构如map，object，否则该字段无效。Kitex 目前暂未支持 form 格式，只支持 json 格式。  注解说明    注解 说明 字段约束 Kitex 支持情况     api.query api.query 对应 HTTP 请求的 url query 参数 只支持基本类型(object, map 以外）和逗号分隔的 list 支持   api.path api.path 对应 HTTP 请求的 url path 参数 支持基本类型 支持   api.header api.header 对应 HTTP 请求的 header 参数 只支持基本类型和逗号分隔的list 支持   api.cookie api.cookie 对应 HTTP 的 cookie 参数 支持基本类型 支持   api.body api.body 对应 HTTP 的 body 参数\n支持 body 为 json 和 form 两种格式 在未指定接口序列化方式下默认json格式，也可以在method注解中使用api.serializer来指定json/form 支持，但目前仅支持 JSON 格式   api.raw_body api.raw_body HTTP 原始 body，少数接口 body 加密了，可以拿到原始的 body（二进制数组)  支持   api.vd 参数校验，使用了HTTPs://github.com/bytedance/go-tagexpr/tree/master/validator库，检验表达式语法参见包内readme文件  暂未支持   api.js_conv api.js_conv 标识该字段传入参数需要进行 string to int64 转换，来解决前端 js 不支持 int64 的场景 value通常写true，其它情况与不写该注解等价 支持   api.raw_uri api.raw_uri 用于 HTTP to RPC 协议转换，RPC 服务获取 HTTP接口对应的原始 uri 只支持 string 类型 暂未支持    举例 structItem{1:optionali64id(go.tag='json:\"id\"')// 对于嵌套结构体，如果要设置序列化key,使用gotag 如 `json:\"id\"` 2:optionalstringtext}typedefstringJsonDictstructBizRequest{1:optionali64v_int64(api.query='v_int64')// 对应HTTP query中的v_int64, 且值范围为(0,200) 2:optionalstringtext(api.body='text')// 对应序列化key = text 3:optionali32token(api.header='token')// 对应HTTP header中的token 4:optionalJsonDictjson_header(api.header='json_header')5:optionalItemsome(api.body='some')// 对应一级key = some 6:optionallist\u003cReqItem\u003ereq_items(api.query='req_items')// api.query仅支持逗号相隔的list,其他复杂类型不支持 7:optionali32api_version(api.path='action')// 对应uri的path参数 8:optionali64uid(api.path='biz')// 对应uri的path参数 9:optionallist\u003ci64\u003ecids(api.query='cids')// 对应query里面逗号隔开的数字, 如 cids=1,2,3,4仅支持list\u003ci64\u003e、list\u003ci32\u003e 10:optionallist\u003cstring\u003evids(api.query='vids')}Response 规范 约束  header 不支持除逗号相隔并且 value 为基本类型的 list 以外的复杂类型 直接按照业务自己定义的 response。默认 json 序列化到 body，key为字段名，注解可为空  注解说明    注解 说明 字段约束 Kitex 支持情况     api.header api.header 设置HTTP 请求回复中的header 只支持基本类型和逗号分隔的list 支持   api.http_code api.http_code 对应HTTP 回复中的status code，200/500等 value通常写true，其它情况与不写该注解等价 支持   api.body api.body 对应HTTP 回复中的body参数  支持   api.none 标识该字段在 response中将会被忽略 value通常写true，其它情况与不写该注解等价 支持   api.js_conv 兼容js int64问题，response时需要将int64表示为string value通常写true，其它情况与不写该注解等价 支持   api.raw_body api.raw_body 设置该字段content作为HTTP response的完整body  支持   api.cookie api.cookie 设置HTTP 回复中的cookie （string类型，后端自行拼接）  支持    举例 // 最终将把BizResponse json序列化之后作为给客户端的返包 structRspItem{1:optionali64item_id// 默认被以字段名作key序列化，等价于使用gotag `json:\"item_id\"` 2:optionalstringtext}structBizResponse{1:optionalstringT (api.header='T')// 该字段将填入给客户端返回的header中 2:optionalmap\u003ci64,RspItem\u003ersp_items (api.body='rsp_items')// 一级key = 'rsp_items' 3:optionali32v_enum (api.none='true')// 该注解value通常写true，其它情况与不写该注解等价 4:optionallist\u003cRspItem\u003ersp_item_list (api.body='rsp_item_list')// 业务自己指定了HTTPCode, 如果没有指定, baseResp.StatuCode=0 -\u003e HTTPCode=200, 其他 HTTPCode=500 5:optionali32http_code (api.http_code='true')//对应 response HTTP Code 6:optionallist\u003ci64\u003eitem_count (api.header='item_count')// 当设置header时以逗号相隔的列表 7:optionalstringtoken (api.cookie='token')// 对应 response Cookie 字段 }Method 规范 约束  如果是GET请求，api.serializer定义的序列化方式是无效的 每个URI对应IDL的一个method，通过注解关联，注解不可为空  注解说明    注解 类型 说明 举例 Kitex 支持情况     api.get string get请求，值为HTTP path，uri的语法与gin一致(参考 httprouter) 例如 api.get = '/life/client/favorite/collect' 支持   api.post string post请求，值为HTTP path，uri的语法与gin一致(参考 httprouter) 例如 api.post='/life/client/favorite/collect' 支持   api.put string put请求，值为HTTP path，uri的语法与gin一致(参考 httprouter) 例如 api.put='/life/client/favorite/collect' 支持   api.delete string delete请求，值为HTTP path，uri的语法与gin一致(参考 httprouter) api.delete='/life/client/favorite/collect' 支持   api.patch string delete请求，值为HTTP path，uri的语法与gin一致(参考 httprouter) api.patch='/life/client/favorite/collect' 支持   api.serializer string 客户端请求体序列化方式 如form, json, thrift, pb等 暂未支持    举例 serviceBizService{// 例子1： get请求 BizResponseBizMethod1(1:biz.BizRequestreq)(api.get='/life/client/:action/:biz',api.baseurl='ib.snssdk.com',api.param='true',api.category='demo')// 例子2: post请求 BizResponseBizMethod2(1:biz.BizRequestreq)(api.post='/life/client/:action/:biz',api.baseurl='ib.snssdk.com',api.param='true',api.serializer='form')// 例子3: delete请求 BizResponseBizMethod3(1:biz.BizRequestreq)(api.post='/life/client/:action/:biz',api.baseurl='ib.snssdk.com',api.param='true',api.serializer='json')}","categories":"","description":"","excerpt":"   日期 版本 作者 变更内容     2022-05-22 v1.0 王景佩 第一版 Thrift-HTTP 映射的 IDL …","ref":"/zh/docs/kitex/tutorials/advanced-feature/generic-call/thrift_idl_annotation_standards/","tags":"","title":"Thrift-HTTP 映射的 IDL 规范"},{"body":"跨源资源共享（CORS）机制允许服务器标识除了它自己的其它 origin，使得浏览器可以访问加载这些资源； 该机制也用来检查服务器是否允许浏览器发送真实的请求，通过浏览器发送\"预检\"请求实现，在预检请求头部中有 HTTP 方法和真实请求会用到的头。\nhertz 提供 cors 跨域中间件的实现 ，这里的实现参考了 gin 的 cors，使用方法可参考 gin 的 cors 。\npackage main import ( \"time\" \"github.com/cloudwego/hertz/pkg/app/server\" \"github.com/hertz-contrib/cors\" ) func main() { h := server.Default() // CORS for https://foo.com and https://github.com origins, allowing:  // - PUT and PATCH methods  // - Origin header  // - Credentials share  // - Preflight requests cached for 12 hours  h.Use(cors.New(cors.Config{ AllowOrigins: []string{\"https://foo.com\"}, AllowMethods: []string{\"PUT\", \"PATCH\"}, AllowHeaders: []string{\"Origin\"}, ExposeHeaders: []string{\"Content-Length\"}, AllowCredentials: true, AllowOriginFunc: func(origin string) bool { return origin == \"https://github.com\" }, MaxAge: 12 * time.Hour, })) h.Spin() } ","categories":"","description":"","excerpt":"跨源资源共享（CORS）机制允许服务器标识除了它自己的其它 origin，使得浏览器可以访问加载这些资源； 该机制也用来检查服务器是否允许浏 …","ref":"/zh/docs/hertz/tutorials/basic-feature/middleware/cors/","tags":"","title":"跨源资源共享"},{"body":"Hertz 默认集成了 Netpoll 和 Golang 原生网络库 两个网络库，用户可以根据自己的场景选择合适的网络库以达到最佳性能。\n使用方式 对于 Server 来说，默认使用 netpoll ，可以通过配置项进行更改：\nserver.New(server.WithTransport(standard.NewTransporter)) server.New(server.WithTransport(netpoll.NewTransporter)) 对于 Client 来说，可以通过配置项进行更改：\nclient.NewClient(client.WithDialer(standard.NewDialer())) client.NewClient(client.WithDialer(netpoll.NewDialer())) 网络库选择  如果有启动 TLS Server 的需求，请使用 go net 网络库。netpoll 正在实现对 TLS 的支持。 由于网络库触发模式的不同：go net 为 ET 模型，netpoll 为 LT 模型，使得两个网络库的适用场景有一些不同。 在 ET 模型下，由框架处理 Read / Write 事件；在 LT 模型下，由网络库处理 Read / Write 事件。 使得在小包场景下，由于更优的调度策略使得 LT 性能更好；在大包场景下，由于读 / 写不受框架层控制，使得大量数据被读入内存而不能及时处理，可能会造成内存压力。   在较大 request size 下（ request size \u003e 1M ），推荐使用 go net 网络库加流式。 在其他场景下，推荐使用 netpoll 网络库，会获得极致的性能。  ","categories":"","description":"","excerpt":"Hertz 默认集成了 Netpoll 和 Golang 原生网络库 两个网络库，用户可以根据自己的场景选择合适的网络库以达到最佳性能。\n使 …","ref":"/zh/docs/hertz/tutorials/basic-feature/network-lib/","tags":"","title":"网络库"},{"body":"会议主题： CloudWeGo 社区会议 2.25\n参会人员： Joway, YangruiEmma, liu-song, AshleeT, li-jin-gou, Hchenn, PureWhiteWu, GuangmingLuo, baiyutang, yccpt, horizonzy.\n会前必读： http://www.cloudwego.io/; https://github.com/cloudwego\n议程 1 ：自我介绍+历史贡献介绍 内容：参会人员轮流开展了自我介绍，包含个人基本情况、历史贡献、个人未来规划、相关建议。\n议程 2 ：社区规划介绍和后续安排 社区介绍（@罗广明 负责介绍）：\n 对CloudWeGo官网进行了简要介绍，并欢迎社区成员对官网内容进行提问，发表看法。 向参会人员介绍 kitex-contrib 库。 提出与其它开源项目合作，吸引更多用户。 介绍 kitex 框架的扩展性，可支持很多能力的扩展，提出未来框架能和更多开源项目做一些对接。  Action Items\n 在官网 Community 部分，后期计划征得大家同意之后，公开各位Contributor 的信息，比如 GitHub 的一些 ID 头像。 在渠道推广方面，后期会考虑与社区其他开源项目合作与对接，包括框架接入、宣传推广等方面。 在 Kitex 框架对接方面，安排 Kitex 框架后期与更多的开源项目（有一定的用户量和知名度的开源项目）进行对接。 在官网优化方面，后期进行网站文档的建设与优化。 在宣传方面，欢迎源码分析方面的文章投稿，同时我们也会加大对 Kitex example 的对外分享与宣传。 在用户案例收集方面，正在逐步沟通与接洽。 确定双周例会时间：暂定双周五晚7:30  议程 3：社区建议  收集外部用户案例：Kitex 除了字节之外也有其他的用户，建议可以收集外部用户案例。@刘嵩 共享代码变更设计文档：关于核心库的变更，内部同学改动的设计文档可以及时放在 Issue 或者群里面，便于感兴趣的同学可以参与进来。@赵延 共享源码分析：源码分析内容可以放到 Issue 中置顶，让大家快速了解整个框架，后期输出的源码分析文档也可以向外部同学同步出来@赵延 ；同时，源码分析文档的框架核心介绍，有助于外部同学对框架做一个深刻的认知，做出更多的 PR @clark(王伟超) 。 丰富宣传方式：建议换一些新颖的方式进行推广和宣传，例如B站和某些有影响力的技术圈之类的。@clark(王伟超) 文章输出：建议@李龙 将之前写的Example 中的 Easy demo ，整理为一篇文章，对项目做一下介绍，之后会对它进行对外发布。  ","categories":"","description":"","excerpt":"会议主题： CloudWeGo 社区会议 2.25\n参会人员： Joway, YangruiEmma, liu-song, AshleeT, …","ref":"/zh/community/meeting_notes/2022-02-25/","tags":"","title":"CloudWeGo 社区会议 2.25"},{"body":"Currently, only one service discovery extension is open-sourced: DNS Resolver.\nDNS Resolver is suitable for the clusters where DNS is used as a service discovery, commonly used for Kubernetes clusters.\nExtended repository: Extended Repository\nUsage import ( ... dns \"github.com/kitex-contrib/resolver-dns\" \"github.com/cloudwego/kitex/client\" ... ) func main() { ... client, err := echo.NewClient(\"echo\", client.WithResolver(dns.NewDNSResolver())) if err != nil { log.Fatal(err) } ... } ","categories":"","description":"","excerpt":"Currently, only one service discovery extension is open-sourced: DNS …","ref":"/docs/kitex/tutorials/service-governance/discovery/","tags":"","title":"Service Discovery"},{"body":"目前在 Kitex 的开源版本中，暂时只提供了一种服务发现的扩展支持 : DNS Resolver, 适合使用 DNS 作为服务发现的场景， 常见的用于 Kubernetes 集群。 扩展库：扩展仓库\n使用方式 import ( ... dns \"github.com/kitex-contrib/resolver-dns\" \"github.com/cloudwego/kitex/client\" ... ) func main() { ... client, err := echo.NewClient(\"echo\", client.WithResolver(dns.NewDNSResolver())) if err != nil { log.Fatal(err) } ... } ","categories":"","description":"","excerpt":"目前在 Kitex 的开源版本中，暂时只提供了一种服务发现的扩展支持 : DNS Resolver, 适合使用 DNS 作为服务发现的场景， …","ref":"/zh/docs/kitex/tutorials/service-governance/discovery/","tags":"","title":"服务发现"},{"body":"Generic call is typically used for mid-platform services that do not need generated code, and only Thrift generic call is supported currently.\nSupported Scenarios  Binary Generic Call: for traffic transit scenario HTTP Mapping Generic Call: for API Gateway scenario Map Mapping Generic Call JSON Mapping Generic Call  Example of Usage 1. Binary Generic Client Usage Application scenario: mid-platform services can forward the received original Thrift protocol packets to the target miscoservice through Binary Forwarding.\n  Client Initialization\nimport ( \"github.com/cloudwego/kitex/client/genericclient\" \"github.com/cloudwego/kitex/pkg/generic\" ) func NewGenericClient(destServiceName string) genericclient.Client { genericCli := genericclient.NewClient(destServiceName, generic.BinaryThriftGeneric()) return genericCli }   Generic Call\nIf you encode by yourself, you have to use Thrift serialization protocol thrift/thrift-binary-protocol.md. Note that you shouldn’t encode original function parameter, but the XXXArgs which wraps function parameters. You can refer to github.com/cloudwego/kitex/generic/generic_test.go.\nKitex provides a thrift codec package github.com/cloudwego/kitex/pkg/utils.NewThriftMessageCodec.\nrc := utils.NewThriftMessageCodec() buf, err := rc.Encode(\"Test\", thrift.CALL, 100, args) // generic call resp, err := genericCli.GenericCall(ctx, \"actualMethod\", buf)   Server Usage It is not necessary to use Client and Server of Binary Generic Call together. Binary Generic Client can access normal Thrift Server if the correct Thrift encoded binary is passed.\nThe server just supports request with a length header like Framed and TTheader, Bufferd Binary is not ok. So the client has to specify the transport protocol with an option, eg: client.WithTransportProtocol(transport.Framed).\npackage main import ( \"github.com/cloudwego/kitex/pkg/generic\" \"github.com/cloudwego/kitex/server/genericserver\" ) func main() { g := generic.BinaryThriftGeneric() svr := genericserver.NewServer(\u0026GenericServiceImpl{}, g) err := svr.Run() if err != nil { panic(err) } } type GenericServiceImpl struct {} // GenericCall ... func (g *GenericServiceImpl) GenericCall(ctx context.Context, method string, request interface{}) (response interface{}, err error) { // request is thrift binary  reqBuf := request.([]byte) // e.g.  fmt.Printf(\"Method: %s\\n\", method)) result := xxx.NewMockTestResult() result.Success = \u0026resp respBuf, err = rc.Encode(mth, thrift.REPLY, seqID, result) return respBuf, nil } 2. HTTP Mapping Generic Call The HTTP Mapping Generic Call is only for the client, and requires Thrift IDL to comply with the interface mapping specification. See the specific specification IDL Definition Specification for Mapping between Thrift and HTTP\nIDL Definition Example namespacegohttpstructReqItem{1:optionali64id(go.tag=\"json:\\\"id\\\"\")2:optionalstringtext}structBizRequest{1:optionali64v_int64(api.query='v_int64',api.vd=\"$\u003e0\u0026\u0026$\u003c200\")2:optionalstringtext(api.body='text')3:optionali32token(api.header='token')4:optionalmap\u003ci64,ReqItem\u003ereq_items_map (api.body='req_items_map')5:optionalReqItemsome(api.body='some')6:optionallist\u003cstring\u003ereq_items(api.query='req_items')7:optionali32api_version(api.path='action')8:optionali64uid(api.path='biz')9:optionallist\u003ci64\u003ecids(api.query='cids')10:optionallist\u003cstring\u003evids(api.query='vids')}structRspItem{1:optionali64item_id2:optionalstringtext}structBizResponse{1:optionalstringT (api.header='T')2:optionalmap\u003ci64,RspItem\u003ersp_items (api.body='rsp_items')3:optionali32v_enum (api.none='')4:optionallist\u003cRspItem\u003ersp_item_list (api.body='rsp_item_list')5:optionali32http_code (api.http_code='')6:optionallist\u003ci64\u003eitem_count (api.header='item_count')}serviceBizService{BizResponseBizMethod1(1:BizRequestreq)(api.get='/life/client/:action/:biz',api.baseurl='ib.snssdk.com',api.param='true')BizResponseBizMethod2(1:BizRequestreq)(api.post='/life/client/:action/:biz',api.baseurl='ib.snssdk.com',api.param='true',api.serializer='form')BizResponseBizMethod3(1:BizRequestreq)(api.post='/life/client/:action/:biz/other',api.baseurl='ib.snssdk.com',api.param='true',api.serializer='json')}Generic Call Example  Request  Type: *generic.HTTPRequest\n Response  Type: *generic.HTTPResponse\npackage main import ( \"github.com/cloudwego/kitex/client/genericclient\" \"github.com/cloudwego/kitex/pkg/generic\" ) func main() { // Parse IDL with Local Files \t// YOUR_IDL_PATH thrift file path, eg: ./idl/example.thrift  // includeDirs: specify include path  p, err := generic.NewThriftFileProvider(\"./YOUR_IDL_PATH\") if err != nil { panic(err) } g, err := generic.HTTPThriftGeneric(p) if err != nil { panic(err) } cli, err := genericclient.NewClient(\"destServiceName\", g, opts...) if err != nil { panic(err) } body := map[string]interface{}{ \"text\": \"text\", \"some\": map[string]interface{}{ \"id\": 1, \"text\": \"text\", }, \"req_items_map\": map[string]interface{}{ \"1\": map[string]interface{}{ \"id\": 1, \"text\": \"text\", }, }, } data, err := json.Marshal(body) if err != nil { panic(err) } url := \"http://example.com/1/1?v_int64=1\u0026req_items=item1,item2,itme3\u0026cids=1,2,3\u0026vids=1,2,3\" req, err := http.NewRequest(http.MethodGet, url, bytes.NewBuffer(data)) if err != nil { panic(err) } req.Header.Set(\"token\", \"1\") customReq, err := generic.FromHTTPRequest(req) // customReq *generic.HttpRequest  resp, err := cli.GenericCall(ctx, \"\", customReq) realResp := resp.(*generic.HttpResponse) realResp.Write(w) } Annotation Extension For example, add a xxx.source = 'not_body_struct' annotation to indicate that a certain field itself does not have a mapping to the HTTP request fields, and you need to traverse its subfields to obtain the corresponding value from the HTTP request. The usage is as follows:\nstructRequest{1:optionali64v_int64(api.query='v_int64')2:optionalCommonParamcommon_param (xxx.source='not_body_struct')}structCommonParam{1:optionali64api_version (api.query='api_version')2:optionali32token(api.header='token')}Extension way：\nfunc init() { descriptor.RegisterAnnotation(new(notBodyStruct)) } // Implement descriptor.Annotation type notBodyStruct struct { } func (a * notBodyStruct) Equal(key, value string) bool { return key == \"xxx.source\" \u0026\u0026 value == \"not_body_struct\" } // Support 4 types Handle: HttpMapping, FieldMapping, ValueMapping, Router func (a * notBodyStruct) Handle() interface{} { return newNotBodyStruct } type notBodyStruct struct{} var newNotBodyStruct descriptor.NewHttpMapping = func(value string) descriptor.HttpMapping { return \u0026notBodyStruct{} } // get value from request func (m *notBodyStruct) Request(req *descriptor.HttpRequest, field *descriptor.FieldDescriptor) (interface{}, bool) { return req, true } // set value to response func (m *notBodyStruct) Response(resp *descriptor.HttpResponse, field *descriptor.FieldDescriptor, val interface{}) { } 3. Map Mapping Generic Call Map Mapping Generic Call means that the user can directly construct Map request or response according to the specification, and Kitex will do Thrift codec accordingly.\nBuild Map Kitex will strictly verify the field name and type constructed according to the given IDL. The field name only supports string type corresponding to the Map Key. The type mapping of the field Value is shown in the Type Mapping Table below.\nReturns the Field ID and type that will verify the Response and generate the corresponding Map Key based on the Field Name of the IDL.\nFor response, the Field ID and Type will be verified, and return Map to user corresponding to the IDL.\nType Mapping Table The Mapping between Golang and Thrift:\n   Golang Type Thrift IDL Type     bool bool   int8 i8   int16 i16   int32 i32   int64 i64   float64 double   string string   []byte binary   []interface{} list/set   map[interface{}]interface{} map   map[string]interface{} struct   int32 enum    Example Take the following IDL as an example:\nenumErrorCode{SUCCESS=0FAILURE=1}structInfo{1:map\u003cstring,string\u003eMap2:i64ID}structEchoRequest{1:stringMsg2:i8I83:i16I164:i32I325:i64I646:binaryBinary7:map\u003cstring,string\u003eMap8:set\u003cstring\u003eSet9:list\u003cstring\u003eList10:ErrorCodeErrorCode11:InfoInfo255:optionalBaseBase}The request construction is as follows:\nreq := map[string]interface{}{ \"Msg\": \"hello\", \"I8\": int8(1), \"I16\": int16(1), \"I32\": int32(1), \"I64\": int64(1), \"Binary\": []byte(\"hello\"), \"Map\": map[interface{}]interface{}{ \"hello\": \"world\", }, \"Set\": []interface{}{\"hello\", \"world\"}, \"List\": []interface{}{\"hello\", \"world\"}, \"ErrorCode\": int32(1), \"Info\": map[string]interface{}{ \"Map\": map[interface{}]interface{}{ \"hello\": \"world\", }, \"ID\": int64(232324), }, } Generic Call Example Example IDL:\nbase.thrift\nnamespacepybasenamespacegobasenamespacejavacom.xxx.thrift.basestructTrafficEnv{1:boolOpen=false,2:stringEnv=\"\",}structBase{1:stringLogID=\"\",2:stringCaller=\"\",3:stringAddr=\"\",4:stringClient=\"\",5:optionalTrafficEnvTrafficEnv,6:optionalmap\u003cstring,string\u003eExtra,}structBaseResp{1:stringStatusMessage=\"\",2:i32StatusCode=0,3:optionalmap\u003cstring,string\u003eExtra,}example_service.thrift\ninclude \"base.thrift\" namespace go kitex.test.server struct ExampleReq { 1: required string Msg, 255: base.Base Base, } struct ExampleResp { 1: required string Msg, 255: base.BaseResp BaseResp, } service ExampleService { ExampleResp ExampleMethod(1: ExampleReq req), } Client Usage  Request  Type: map[string]interface{}\n Response  Type: map[string]interface{}\npackage main import ( \"github.com/cloudwego/kitex/pkg/generic\" \"github.com/cloudwego/kitex/client/genericclient\" ) func main() { // Parse IDL with Local Files  // YOUR_IDL_PATH thrift file path, eg:./idl/example.thrift  p, err := generic.NewThriftFileProvider(\"./YOUR_IDL_PATH\") if err != nil { panic(err) } g, err := generic.MapThriftGeneric(p) if err != nil { panic(err) } cli, err := genericclient.NewClient(\"destServiceName\", g, opts...) if err != nil { panic(err) } // 'ExampleMethod' method name must be passed as param  resp, err := cli.GenericCall(ctx, \"ExampleMethod\", map[string]interface{}{ \"Msg\": \"hello\", }) // resp is a map[string]interface{} } Server Usage  Request  Type: map[string]interface{}\n Response  Type: map[string]interface{}\npackage main import ( \"github.com/cloudwego/kitex/pkg/generic\" \"github.com/cloudwego/kitex/server/genericserver\" ) func main() { // Parse IDL with Local Files  // YOUR_IDL_PATH thrift file path,eg: ./idl/example.thrift  p, err := generic.NewThriftFileProvider(\"./YOUR_IDL_PATH\") if err != nil { panic(err) } g, err := generic.MapThriftGeneric(p) if err != nil { panic(err) } svc := genericserver.NewServer(new(GenericServiceImpl), g, opts...) if err != nil { panic(err) } err := svr.Run() if err != nil { panic(err) } // resp is a map[string]interface{} } type GenericServiceImpl struct { } func (g *GenericServiceImpl) GenericCall(ctx context.Context, method string, request interface{}) (response interface{}, err error) { m := request.(map[string]interface{}) fmt.Printf(\"Recv: %v\\n\", m) return map[string]interface{}{ \"Msg\": \"world\", }, nil } 4. JSON Mapping Generic Call JSON Mapping Generic Call means that the user can directly construct JSON string request or response according to the specification, and Kitex will do Thrift codec accordingly.\nBuild JSON Kitex JSON Mapping Generic Call will convert the request parameters according to the given IDL, will not strictly verify the field name and type constructed.\nThe field name only supports string type corresponding to the JSON Field. The type mapping of the field Value is shown in the Type Mapping Table below.\nReturns the Field ID and type that will verify the Response and generate the corresponding JSON Field based on the Field Name of the IDL.\nFor response, the Field ID and Type will be verified, and return JSON string to user corresponding to the IDL.\nType Mapping Table The Mapping between Golang and Thrift:\n   Golang Type Thrift IDL Type     bool bool   int8 i8   int16 i16   int32 i32   int64 i64   float64 double   string string   []byte binary   []interface{} list/set   map[interface{}]interface{} map   map[string]interface{} struct   int32 enum    Example Take the following IDL as an example：\nenumErrorCode{SUCCESS=0FAILURE=1}structInfo{1:map\u003cstring,string\u003eMap2:i64ID}structEchoRequest{1:stringMsg2:i8I83:i16I164:i32I325:i64I646:map\u003cstring,string\u003eMap7:set\u003cstring\u003eSet8:list\u003cstring\u003eList9:ErrorCodeErrorCode10:InfoInfo255:optionalBaseBase}The request construction is as follows：\nreq := { \"Msg\": \"hello\", \"I8\": 1, \"I16\": 1, \"I32\": 1, \"I64\": 1, \"Map\": \"{\\\"hello\\\":\\\"world\\\"}\", \"Set\": [\"hello\", \"world\"], \"List\": [\"hello\", \"world\"], \"ErrorCode\": 1, \"Info\": \"{\\\"Map\\\":\\\"{\\\"hello\\\":\\\"world\\\"}\\\", \\\"ID\\\":232324}\" } Generic Call Example Example IDL ：\nbase.thrift\nnamespacepybasenamespacegobasenamespacejavacom.xxx.thrift.basestructTrafficEnv{1:boolOpen=false,2:stringEnv=\"\",}structBase{1:stringLogID=\"\",2:stringCaller=\"\",3:stringAddr=\"\",4:stringClient=\"\",5:optionalTrafficEnvTrafficEnv,6:optionalmap\u003cstring,string\u003eExtra,}structBaseResp{1:stringStatusMessage=\"\",2:i32StatusCode=0,3:optionalmap\u003cstring,string\u003eExtra,}example_service.thrift\ninclude \"base.thrift\" namespace go kitex.test.server struct ExampleReq { 1: required string Msg, 255: base.Base Base, } struct ExampleResp { 1: required string Msg, 255: base.BaseResp BaseResp, } service ExampleService { ExampleResp ExampleMethod(1: ExampleReq req), } Client Usage  Request  Type：JSON string\n Response  Type：JSON string\npackage main import ( \"github.com/cloudwego/kitex/pkg/generic\" \"github.com/cloudwego/kitex/client/genericclient\" ) func main() { // Parse IDL with Local Files  // YOUR_IDL_PATH thrift file path, eg:./idl/example.thrift  p, err := generic.NewThriftFileProvider(\"./YOUR_IDL_PATH\") if err != nil { panic(err) } g, err := generic.JSONThriftGeneric(p) if err != nil { panic(err) } cli, err := genericclient.NewClient(\"psm\", g, opts...) if err != nil { panic(err) } // 'ExampleMethod' method name must be passed as param  resp, err := cli.GenericCall(ctx, \"ExampleMethod\", \"{\\\"Msg\\\": \\\"hello\\\"}\") // resp is a JSON string } Server Usage  Request  Type：JSON string\n Response  Type：JSON string\npackage main import ( \"github.com/cloudwego/kitex/pkg/generic\" \"github.com/cloudwego/kitex/server/genericserver\" ) func main() { // Parse IDL with Local Files  // YOUR_IDL_PATH thrift file path,eg: ./idl/example.thrift  p, err := generic.NewThriftFileProvider(\"./YOUR_IDL_PATH\") if err != nil { panic(err) } g, err := generic.JSONThriftGeneric(p) if err != nil { panic(err) } svc := genericserver.NewServer(new(GenericServiceImpl), g, opts...) if err != nil { panic(err) } err := svr.Run() if err != nil { panic(err) } // resp is a JSON string } type GenericServiceImpl struct { } func (g *GenericServiceImpl) GenericCall(ctx context.Context, method string, request interface{}) (response interface{}, err error) { // use jsoniter or other json parse sdk to assert request  m := request.(string) fmt.Printf(\"Recv: %v\\n\", m) return \"{\\\"Msg\\\": \\\"world\\\"}\", nil } IDLProvider Generic Call of HTTP/Map/JSON mapping does not require generated code, but requires IDL which need users to provide.\nAt present, Kitex has two IDLProvider implementations. Users can choose to specify the IDL path or pass in IDL content. Of course, you can also expand the generic.DescriptorProvider according to your needs.\nParse IDL with Local Files p, err := generic.NewThriftFileProvider(\"./YOUR_IDL_PATH\") if err != nil { panic(err) } Parse IDL with Memory All IDLs need to be constructed into a Map, Key is Path, Value is IDL definition, and the usage is as follows:\np, err := generic.NewThriftContentProvider(\"YOUR_MAIN_IDL_CONTENT\", map[string]string{/*YOUR_INCLUDES_IDL_CONTENT*/}) if err != nil { panic(err) } // dynamic update err = p.UpdateIDL(\"YOUR_MAIN_IDL_CONTENT\", map[string]string{/*YOUR_INCLUDES_IDL_CONTENT*/}) if err != nil { // handle err } Simple example (not real IDL, just for minimizing display Path constructs):\npath := \"a/b/main.thrift\" content := ` namespace go kitex.test.server include \"x.thrift\" include \"../y.thrift\" service InboxService {} ` includes := map[string]string{ path: content, \"x.thrift\": \"namespace go kitex.test.server\", \"../y.thrift\": ` namespace go kitex.test.server include \"z.thrift\" `, } p, err := NewThriftContentProvider(path, includes) Absolute Path including path Addressing If you construct an IDL Map for convenience, you can also use an absolute path as a Key through NewThriftContentWithAbsIncludePathProvider .\np, err := generic.NewThriftContentWithAbsIncludePathProvider(\"YOUR_MAIN_IDL_PATH\", \"YOUR_MAIN_IDL_CONTENT\", map[string]string{\"ABS_INCLUDE_PATH\": \"CONTENT\"}) if err != nil { panic(err) } // dynamic update err = p.UpdateIDL(\"YOUR_MAIN_IDL_PATH\", \"YOUR_MAIN_IDL_CONTENT\", map[string]string{/*YOUR_INCLUDES_IDL_CONTENT*/}) if err != nil { // handle err } Simple example (not real IDL, just for minimizing display Path constructs):\npath := \"a/b/main.thrift\" content := ` namespace go kitex.test.server include \"x.thrift\" include \"../y.thrift\" service InboxService {} ` includes := map[string]string{ path: content, \"a/b/x.thrift\": \"namespace go kitex.test.server\", \"a/y.thrift\": ` namespace go kitex.test.server include \"z.thrift\" `, \"a/z.thrift\": \"namespace go kitex.test.server\", } p, err := NewThriftContentWithAbsIncludePathProvider(path, includes) ","categories":"","description":"","excerpt":"Generic call is typically used for mid-platform services that do not …","ref":"/docs/kitex/tutorials/advanced-feature/generic-call/","tags":"","title":"Generic Call"},{"body":"目前仅支持 Thrift 泛化调用，通常用于不需要生成代码的中台服务。\n支持场景  二进制泛化调用：用于流量中转场景 HTTP 映射泛化调用：用于 API 网关场景 Map 映射泛化调用 JSON 映射泛化调用  使用方式示例 1. 二进制泛化调用 调用端使用 应用场景：比如中台服务，可以通过二进制流转发将收到的原始 Thrift 协议包发给目标服务。\n  初始化 Client\nimport ( \"github.com/cloudwego/kitex/client/genericclient\" \"github.com/cloudwego/kitex/pkg/generic\" ) func NewGenericClient(destServiceName string) genericclient.Client { genericCli := genericclient.NewClient(destServiceName, generic.BinaryThriftGeneric()) return genericCli }   泛化调用\n若自行编码，需要使用 Thrift 编码格式 thrift/thrift-binary-protocol.md。注意，二进制编码不是对原始的 Thrift 请求参数编码，是 method 参数封装的 XXXArgs。可以参考 github.com/cloudwego/kitex/generic/generic_test.go。\nKitex 提供了 thrift 编解码包github.com/cloudwego/kitex/pkg/utils.NewThriftMessageCodec。\nrc := utils.NewThriftMessageCodec() buf, err := rc.Encode(\"Test\", thrift.CALL, 100, args) // generic call resp, err := genericCli.GenericCall(ctx, \"actualMethod\", buf)   服务端使用 二进制泛化 Client 和 Server 并不是配套使用的，Client 传入正确的 Thrift 编码二进制，可以访问普通的 Thrift Server。\n二进制泛化 Server 只支持 Framed 或 TTHeader 请求，不支持 Bufferd Binary，需要 Client 通过 Option 指定，如：client.WithTransportProtocol(transport.Framed)。\npackage main import ( \"github.com/cloudwego/kitex/pkg/generic\" \"github.com/cloudwego/kitex/server/genericserver\" ) func main() { g := generic.BinaryThriftGeneric() svr := genericserver.NewServer(\u0026GenericServiceImpl{}, g) err := svr.Run() if err != nil { panic(err) } } type GenericServiceImpl struct {} // GenericCall ... func (g *GenericServiceImpl) GenericCall(ctx context.Context, method string, request interface{}) (response interface{}, err error) { // request is thrift binary  reqBuf := request.([]byte) // e.g.  fmt.Printf(\"Method: %s\\n\", method)) result := xxx.NewMockTestResult() result.Success = \u0026resp respBuf, err = rc.Encode(mth, thrift.REPLY, seqID, result) return respBuf, nil } 2. HTTP 映射泛化调用 HTTP 映射泛化调用只针对客户端，要求 Thrift IDL 遵从接口映射规范，具体规范见 Thrift-HTTP 映射的 IDL 规范。\nIDL 定义示例 namespacegohttpstructReqItem{1:optionali64id(go.tag=\"json:\\\"id\\\"\")2:optionalstringtext}structBizRequest{1:optionali64v_int64(api.query='v_int64',api.vd=\"$\u003e0\u0026\u0026$\u003c200\")2:optionalstringtext(api.body='text')3:optionali32token(api.header='token')4:optionalmap\u003ci64,ReqItem\u003ereq_items_map (api.body='req_items_map')5:optionalReqItemsome(api.body='some')6:optionallist\u003cstring\u003ereq_items(api.query='req_items')7:optionali32api_version(api.path='action')8:optionali64uid(api.path='biz')9:optionallist\u003ci64\u003ecids(api.query='cids')10:optionallist\u003cstring\u003evids(api.query='vids')}structRspItem{1:optionali64item_id2:optionalstringtext}structBizResponse{1:optionalstringT (api.header='T')2:optionalmap\u003ci64,RspItem\u003ersp_items (api.body='rsp_items')3:optionali32v_enum (api.none='')4:optionallist\u003cRspItem\u003ersp_item_list (api.body='rsp_item_list')5:optionali32http_code (api.http_code='')6:optionallist\u003ci64\u003eitem_count (api.header='item_count')}serviceBizService{BizResponseBizMethod1(1:BizRequestreq)(api.get='/life/client/:action/:biz',api.baseurl='ib.snssdk.com',api.param='true')BizResponseBizMethod2(1:BizRequestreq)(api.post='/life/client/:action/:biz',api.baseurl='ib.snssdk.com',api.param='true',api.serializer='form')BizResponseBizMethod3(1:BizRequestreq)(api.post='/life/client/:action/:biz/other',api.baseurl='ib.snssdk.com',api.param='true',api.serializer='json')}泛化调用示例  Request  类型：*generic.HTTPRequest\n Response  类型：*generic.HTTPResponse\npackage main import ( \"github.com/cloudwego/kitex/client/genericclient\" \"github.com/cloudwego/kitex/pkg/generic\" ) func main() { // 本地文件 idl 解析  // YOUR_IDL_PATH thrift 文件路径: 举例 ./idl/example.thrift  // includeDirs: 指定 include 路径，默认用当前文件的相对路径寻找 include  p, err := generic.NewThriftFileProvider(\"./YOUR_IDL_PATH\") if err != nil { panic(err) } // 构造 http 类型的泛化调用  g, err := generic.HTTPThriftGeneric(p) if err != nil { panic(err) } cli, err := genericclient.NewClient(\"destServiceName\", g, opts...) if err != nil { panic(err) } // 构造 request（用于测试），实际应用可以直接使用原始的 HTTP Request  body := map[string]interface{}{ \"text\": \"text\", \"some\": map[string]interface{}{ \"id\": 1, \"text\": \"text\", }, \"req_items_map\": map[string]interface{}{ \"1\": map[string]interface{}{ \"id\": 1, \"text\": \"text\", }, }, } data, err := json.Marshal(body) if err != nil { panic(err) } url := \"http://example.com/life/client/1/1?v_int64=1\u0026req_items=item1,item2,itme3\u0026cids=1,2,3\u0026vids=1,2,3\" req, err := http.NewRequest(http.MethodGet, url, bytes.NewBuffer(data)) if err != nil { panic(err) } req.Header.Set(\"token\", \"1\") customReq, err := generic.FromHTTPRequest(req) // 考虑到业务有可能使用第三方 http request，可以自行构造转换函数  // customReq *generic.HttpRequest  // 由于 http 泛化的 method 是通过 bam 规则从 http request 中获取的，所以填空就行  resp, err := cli.GenericCall(ctx, \"\", customReq) realResp := resp.(*generic.HttpResponse) realResp.Write(w) // 写回 ResponseWriter，用于 http 网关 } 注解扩展 比如增加一个 xxx.source='not_body_struct' 注解，表示某个字段本身没有对 HTTP 请求字段的映射，需要遍历其子字段从 HTTP 请求中获取对应的值。使用方式如下：\nstructRequest{1:optionali64v_int64(api.query='v_int64')2:optionalCommonParamcommon_param (xxx.source='not_body_struct')}structCommonParam{1:optionali64api_version (api.query='api_version')2:optionali32token(api.header='token')}扩展方式如下：\nfunc init() { descriptor.RegisterAnnotation(new(notBodyStruct)) } // 实现 descriptor.Annotation type notBodyStruct struct { } func (a * notBodyStruct) Equal(key, value string) bool { return key == \"xxx.source\" \u0026\u0026 value == \"not_body_struct\" } // Handle 目前支持四种类型：HttpMapping, FieldMapping, ValueMapping, Router func (a * notBodyStruct) Handle() interface{} { return newNotBodyStruct } type notBodyStruct struct{} var newNotBodyStruct descriptor.NewHttpMapping = func(value string) descriptor.HttpMapping { return \u0026notBodyStruct{} } // get value from request func (m *notBodyStruct) Request(req *descriptor.HttpRequest, field *descriptor.FieldDescriptor) (interface{}, bool) { // not_body_struct 注解的作用相当于 step into，所以直接返回 req 本身，让当前 filed 继续从 Request 中查询所需要的值  return req, true } // set value to response func (m *notBodyStruct) Response(resp *descriptor.HttpResponse, field *descriptor.FieldDescriptor, val interface{}) { } 3. Map 映射泛化调用 Map 映射泛化调用是指用户可以直接按照规范构造 Map 请求参数或返回，Kitex 会对应完成 Thrift 编解码。\nMap 构造 Kitex 会根据给出的 IDL 严格校验用户构造的字段名和类型，字段名只支持字符串类型对应 Map Key，字段 Value 的类型映射见类型映射表。\n对于Response会校验 Field ID 和类型，并根据 IDL 的 Field Name 生成相应的 Map Key。\n类型映射 Golang 与 Thrift IDL 类型映射如下：\n   Golang 类型 Thrift IDL 类型     bool bool   int8 i8   int16 i16   int32 i32   int64 i64   float64 double   string string   []byte binary   []interface{} list/set   map[interface{}]interface{} map   map[string]interface{} struct   int32 enum    示例 以下面的 IDL 为例：\nenumErrorCode{SUCCESS=0FAILURE=1}structInfo{1:map\u003cstring,string\u003eMap2:i64ID}structEchoRequest{1:stringMsg2:i8I83:i16I164:i32I325:i64I646:binaryBinary7:map\u003cstring,string\u003eMap8:set\u003cstring\u003eSet9:list\u003cstring\u003eList10:ErrorCodeErrorCode11:InfoInfo255:optionalBaseBase}构造请求如下：\nreq := map[string]interface{}{ \"Msg\": \"hello\", \"I8\": int8(1), \"I16\": int16(1), \"I32\": int32(1), \"I64\": int64(1), \"Binary\": []byte(\"hello\"), \"Map\": map[interface{}]interface{}{ \"hello\": \"world\", }, \"Set\": []interface{}{\"hello\", \"world\"}, \"List\": []interface{}{\"hello\", \"world\"}, \"ErrorCode\": int32(1), \"Info\": map[string]interface{}{ \"Map\": map[interface{}]interface{}{ \"hello\": \"world\", }, \"ID\": int64(232324), }, } 泛化调用示例 示例 IDL ：\nbase.thrift\nnamespacepybasenamespacegobasenamespacejavacom.xxx.thrift.basestructTrafficEnv{1:boolOpen=false,2:stringEnv=\"\",}structBase{1:stringLogID=\"\",2:stringCaller=\"\",3:stringAddr=\"\",4:stringClient=\"\",5:optionalTrafficEnvTrafficEnv,6:optionalmap\u003cstring,string\u003eExtra,}structBaseResp{1:stringStatusMessage=\"\",2:i32StatusCode=0,3:optionalmap\u003cstring,string\u003eExtra,}example_service.thrift\ninclude \"base.thrift\" namespace go kitex.test.server struct ExampleReq { 1: required string Msg, 255: base.Base Base, } struct ExampleResp { 1: required string Msg, 255: base.BaseResp BaseResp, } service ExampleService { ExampleResp ExampleMethod(1: ExampleReq req), } 客户端使用  Request  类型：map[string]interface{}\n Response  类型：map[string]interface{}\npackage main import ( \"github.com/cloudwego/kitex/pkg/generic\" \"github.com/cloudwego/kitex/client/genericclient\" ) func main() { // 本地文件 idl 解析  // YOUR_IDL_PATH thrift 文件路径: 举例 ./idl/example.thrift  // includeDirs: 指定 include 路径，默认用当前文件的相对路径寻找 include  p, err := generic.NewThriftFileProvider(\"./YOUR_IDL_PATH\") if err != nil { panic(err) } // 构造 map 请求和返回类型的泛化调用  g, err := generic.MapThriftGeneric(p) if err != nil { panic(err) } cli, err := genericclient.NewClient(\"destServiceName\", g, opts...) if err != nil { panic(err) } // 'ExampleMethod' 方法名必须包含在 idl 定义中  resp, err := cli.GenericCall(ctx, \"ExampleMethod\", map[string]interface{}{ \"Msg\": \"hello\", }) // resp is a map[string]interface{} } 服务端使用  Request  类型：map[string]interface{}\n Response  类型：map[string]interface{}\npackage main import ( \"github.com/cloudwego/kitex/pkg/generic\" \"github.com/cloudwego/kitex/server/genericserver\" ) func main() { // 本地文件 idl 解析  // YOUR_IDL_PATH thrift 文件路径: e.g. ./idl/example.thrift  p, err := generic.NewThriftFileProvider(\"./YOUR_IDL_PATH\") if err != nil { panic(err) } // 构造 map 请求和返回类型的泛化调用  g, err := generic.MapThriftGeneric(p) if err != nil { panic(err) } svc := genericserver.NewServer(new(GenericServiceImpl), g, opts...) if err != nil { panic(err) } err := svr.Run() if err != nil { panic(err) } // resp is a map[string]interface{} } type GenericServiceImpl struct { } func (g *GenericServiceImpl) GenericCall(ctx context.Context, method string, request interface{}) (response interface{}, err error) { m := request.(map[string]interface{}) fmt.Printf(\"Recv: %v\\n\", m) return map[string]interface{}{ \"Msg\": \"world\", }, nil } 4. JSON 映射泛化调用 JSON 映射泛化调用是指用户可以直接按照规范构造 JSON String 请求参数或返回，Kitex 会对应完成 Thrift 编解码。\nJSON 构造 Kitex 与 MAP 泛化调用严格校验用户构造的字段名和类型不同，JSON 泛化调用会根据给出的 IDL 对用户的请求参数进行转化，无需用户指定明确的类型，如 int32 或 int64。\n对于 Response 会校验 Field ID 和类型，并根据 IDL 的 Field Name 生成相应的 JSON Field。\n类型映射 Golang 与 Thrift IDL 类型映射如下：\n   Golang 类型 Thrift IDL 类型     bool bool   int8 i8   int16 i16   int32 i32   int64 i64   float64 double   string string   []byte binary   []interface{} list/set   map[interface{}]interface{} map   map[string]interface{} struct   int32 enum    示例 以下面的 IDL 为例：\nenumErrorCode{SUCCESS=0FAILURE=1}structInfo{1:map\u003cstring,string\u003eMap2:i64ID}structEchoRequest{1:stringMsg2:i8I83:i16I164:i32I325:i64I646:map\u003cstring,string\u003eMap7:set\u003cstring\u003eSet8:list\u003cstring\u003eList9:ErrorCodeErrorCode10:InfoInfo255:optionalBaseBase}构造请求如下：\nreq := { \"Msg\": \"hello\", \"I8\": 1, \"I16\": 1, \"I32\": 1, \"I64\": 1, \"Map\": \"{\\\"hello\\\":\\\"world\\\"}\", \"Set\": [\"hello\", \"world\"], \"List\": [\"hello\", \"world\"], \"ErrorCode\": 1, \"Info\": \"{\\\"Map\\\":\\\"{\\\"hello\\\":\\\"world\\\"}\\\", \\\"ID\\\":232324}\" } 泛化调用示例 示例 IDL ：\nbase.thrift\nnamespacepybasenamespacegobasenamespacejavacom.xxx.thrift.basestructTrafficEnv{1:boolOpen=false,2:stringEnv=\"\",}structBase{1:stringLogID=\"\",2:stringCaller=\"\",3:stringAddr=\"\",4:stringClient=\"\",5:optionalTrafficEnvTrafficEnv,6:optionalmap\u003cstring,string\u003eExtra,}structBaseResp{1:stringStatusMessage=\"\",2:i32StatusCode=0,3:optionalmap\u003cstring,string\u003eExtra,}example_service.thrift\ninclude \"base.thrift\" namespace go kitex.test.server struct ExampleReq { 1: required string Msg, 255: base.Base Base, } struct ExampleResp { 1: required string Msg, 255: base.BaseResp BaseResp, } service ExampleService { ExampleResp ExampleMethod(1: ExampleReq req), } 客户端使用  Request  类型：JSON string\n Response  类型：JSON string\npackage main import ( \"github.com/cloudwego/kitex/pkg/generic\" \"github.com/cloudwego/kitex/client/genericclient\" ) func main() { // 本地文件 idl 解析  // YOUR_IDL_PATH thrift 文件路径: 举例 ./idl/example.thrift  // includeDirs: 指定 include 路径，默认用当前文件的相对路径寻找 include  p, err := generic.NewThriftFileProvider(\"./YOUR_IDL_PATH\") if err != nil { panic(err) } // 构造 JSON 请求和返回类型的泛化调用  g, err := generic.JSONThriftGeneric(p) if err != nil { panic(err) } cli, err := genericclient.NewClient(\"destServiceName\", g, opts...) if err != nil { panic(err) } // 'ExampleMethod' 方法名必须包含在 idl 定义中  resp, err := cli.GenericCall(ctx, \"ExampleMethod\", \"{\\\"Msg\\\": \\\"hello\\\"}\") // resp is a JSON string } 服务端使用  Request  类型：JSON string\n Response  类型：JSON string\npackage main import ( \"github.com/cloudwego/kitex/pkg/generic\" \"github.com/cloudwego/kitex/server/genericserver\" ) func main() { // 本地文件 idl 解析  // YOUR_IDL_PATH thrift 文件路径: e.g. ./idl/example.thrift  p, err := generic.NewThriftFileProvider(\"./YOUR_IDL_PATH\") if err != nil { panic(err) } // 构造 JSON 请求和返回类型的泛化调用  g, err := generic.JSONThriftGeneric(p) if err != nil { panic(err) } svc := genericserver.NewServer(new(GenericServiceImpl), g, opts...) if err != nil { panic(err) } err := svr.Run() if err != nil { panic(err) } // resp is a JSON string } type GenericServiceImpl struct { } func (g *GenericServiceImpl) GenericCall(ctx context.Context, method string, request interface{}) (response interface{}, err error) { // use jsoniter or other json parse sdk to assert request  m := request.(string) fmt.Printf(\"Recv: %v\\n\", m) return \"{\\\"Msg\\\": \\\"world\\\"}\", nil } IDLProvider HTTP/Map/JSON 映射的泛化调用虽然不需要生成代码，但需要使用者提供 IDL。\n目前 Kitex 有两种 IDLProvider 实现，使用者可以选择指定 IDL 路径，也可以选择传入 IDL 内容。当然也可以根据需求自行扩展 generci.DescriptorProvider。\n基于本地文件解析 IDL p, err := generic.NewThriftFileProvider(\"./YOUR_IDL_PATH\") if err != nil { panic(err) } 基于内存解析 IDL 所有 IDL 需要构造成 Map ，Key 是 Path，Value 是 IDL 定义，使用方式如下：\np, err := generic.NewThriftContentProvider(\"YOUR_MAIN_IDL_CONTENT\", map[string]string{/*YOUR_INCLUDES_IDL_CONTENT*/}) if err != nil { panic(err) } // dynamic update err = p.UpdateIDL(\"YOUR_MAIN_IDL_CONTENT\", map[string]string{/*YOUR_INCLUDES_IDL_CONTENT*/}) if err != nil { // handle err } 简单实例（为最小化展示 Path 构造，并非真实的 IDL）：\npath := \"a/b/main.thrift\" content := ` namespace go kitex.test.server include \"x.thrift\" include \"../y.thrift\" service InboxService {} ` includes := map[string]string{ path: content, \"x.thrift\": \"namespace go kitex.test.server\", \"../y.thrift\": ` namespace go kitex.test.server include \"z.thrift\" `, } p, err := NewThriftContentProvider(path, includes) 支持绝对路径的 include path 寻址 若为方便构造 IDL Map，也可以通过 NewThriftContentWithAbsIncludePathProvider 使用绝对路径作为 Key。\np, err := generic.NewThriftContentWithAbsIncludePathProvider(\"YOUR_MAIN_IDL_PATH\", \"YOUR_MAIN_IDL_CONTENT\", map[string]string{\"ABS_INCLUDE_PATH\": \"CONTENT\"}) if err != nil { panic(err) } // dynamic update err = p.UpdateIDL(\"YOUR_MAIN_IDL_PATH\", \"YOUR_MAIN_IDL_CONTENT\", map[string]string{/*YOUR_INCLUDES_IDL_CONTENT*/}) if err != nil { // handle err } 简单实例（为最小化展示 Path 构造，并非真实的 IDL）：\npath := \"a/b/main.thrift\" content := ` namespace go kitex.test.server include \"x.thrift\" include \"../y.thrift\" service InboxService {} ` includes := map[string]string{ path: content, \"a/b/x.thrift\": \"namespace go kitex.test.server\", \"a/y.thrift\": ` namespace go kitex.test.server include \"z.thrift\" `, \"a/z.thrift\": \"namespace go kitex.test.server\", } p, err := NewThriftContentWithAbsIncludePathProvider(path, includes) ","categories":"","description":"","excerpt":"目前仅支持 Thrift 泛化调用，通常用于不需要生成代码的中台服务。\n支持场景  二进制泛化调用：用于流量中转场景 HTTP 映射泛化调 …","ref":"/zh/docs/kitex/tutorials/advanced-feature/generic-call/","tags":"","title":"Kitex 泛化调用使用指南"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kitex/tutorials/basic-feature/","tags":"","title":"Basic Feature"},{"body":"kitex is a command line tool for code generation provided by the RPC framework Kitex. At present, kitex accepts both thrift and protobuf IDLs, and supports generating a skeleton of a server side project.\nInstallation go install github.com/cloudwego/kitex/tool/cmd/kitex You can use the go command to install kitex or build and install it from source. To check where kitex will be installed, try\ngo list -f {{.Target}} github.com/cloudwego/kitex/tool/cmd/kitex Dependencies and Running Mode The Underlying Compilers To generate code for an IDL, the corresponding compiler must be installed ahead: thriftgo for thrift IDLs and protoc for protobuf IDLs.\nCodes generated by kitex consists of two parts: some generated by the underlying compiler (usually serialization and deserialization codes for structures defined in the IDLs) and some glue codes generated by kitex to connect the former and the Kitex framework.\nIn a execution flow, when kitex is run to generate code for a thrift IDL, it invokes thriftgo to do code generation while itself as a plugin for thriftgo generates some glue codes, too. For protobuf IDLs, the process is similar.\n$\u003e kitex IDL | | thrift-IDL |---------\u003e thriftgo --gen go:... -plugin=... -out ... IDL | | protobuf-IDL ----------\u003e protoc --kitex_out=... --kitex_opt=... IDL Library Dependencies The codes generated by kitex may depends on certain Go libraries:\n For thrift IDLs, it is github.com/apache/thrift v0.13.0 For protobuf IDLs, it is google.golang.org/protobuf v1.26.0  It is important to note that, from v0.14.0, the APIs in github.com/apache/thrift/lib/go/thrift are incompatible with previous versions. If you specify the -u flag when using go get to update dependencies, this library will be updated to the incompatible version and cause compilation failure. You can solve this problem by executing an extra command to choose the appropriate version:\ngo get github.com/apache/thrift@v0.13.0 or force the version with a replace command:\ngo mod edit -replace github.com/apache/thrift=github.com/apache/thrift@v0.13.0 Notes for Using Protobuf IDLs The kitex only supports the proto3 version of the protocol buffers language.\nThe go_package option in the IDL is required. Its value is a package name sequence separated by dots (.) or by slashes (/). It determines the suffix of the result import path. For instance,\noption go_package = \"hello.world\"; // or hello/world will generates an import path ${imoprt path of the current directory}/kitex_gen/hello/world.\nIf you assign a complete import path to go_package, then kitex will generate codes for this IDL only when the import path matches the kitex_gen directory. For example:\n go_package=\"${import path of current module}/kitex_gen/hello/world\";: OK. Kitex will generate codes for this IDL; go_package=\"${import path of current module}/hello/world\";: Kitex will not generate codes for this IDL; go_package=\"any.other.domain/some/module/kitex_gen/hello/world\";: Kitex will not generate codes for this IDL;  You can overwrite the go_package value in a proto file with a command line option --protobuf Msome.proto=your.package.name/kitex_gen/wherever/you/like. For the usage of the option, please refer to the official document of Protocol Buffers.\nUsage Basic Usage Syntax: kitex [options] IDL\nThe following examples uses thrift IDLs. Protobuf IDLs work the same way.\nGenerate Client Codes kitex path_to_your_idl.thrift When the command is done, there will be a folder named kitex_gen in the current directory. It contains data structure definitions from the IDL and some *service packages providing client APIs for the service definitions from the IDL.\nGenerate Serverr Codes kitex -service service_name path_to_your_idl.thrift When the command is done, in the current directory, there will be a folder named kitex_gen and some other files for building and running a server (namely a scaffold). See the description in section The Structure of Generated Codes.\nThe Structure of Generated Codes Suppose we have two thrift IDLs, demo.thrift and base.thrift that demo.thrift includes base.thrift. In demo.thrift there is a service definition named demo.\nThen in an empty directory, kitex -service demo dem.thrift produces a skeleton of project:\n. ├── build.sh // A script to build a server. It creates necessary files and puts them into a folder named 'output'. ├── handler.go // Implementation of methods of the service defined in the IDL. ├── kitex_gen // IDL relevant generated codes. │ ├── base // Code generated for base.thrift. │ │ ├── base.go // Output of thriftgo. │ │ └── k-base.go // Codes generated by kitex besides the output of thriftgo. │ └── demo // Code generated for demo.thrift. │ ├── demo.go // Output of thriftgo. │ ├── k-demo.go // Codes generated by kitex besides the output of thriftgo. │ └── demoservice // Codes generated by kitex for services defined in demo.thrift. │ ├── demoservice.go // Provides some definitions shared by client.go and server.go. │ ├── client.go // Provides NewClient API. │ └── server.go // Provides NewServer API. ├── main.go // Entry of program. └── script // Build scripts. └── bootstrap.sh // The bootstraip script of server. Will be copied to output by build.sh. If the -service flag is not specified, then only kitex_gen will be generated.\nOptions The option description here may be outdated. Run kitex -h or kitex --help to get all usable options of kitex.\n-service service_name When this option is specified, kitex will generate a scaffold to build a server. Parameter service_name gives the name of the server itself when launched. Its value is usually up to the service registry and service disccovery components when using the Kitex framework.\n-module module_name This option is used to specify the Go module the generated codes belongs to. It affects import paths.\n  If the current directory is a child path of $GOPATH/src, this option can be omitted; kitex will use a path relative to $GOPATH/src as the prefix of import path in generated codes. For example, if you run kitex under $GOPATH/src/example.com/hello/world, the import path of kitex_gen/example_package/example_package.go for other package will be example.com/hello/world/kitex_gen/example_package.\n  If the current directory is not a child path of $GOPATH/src, this option must be specified.\n  If -module is specified, then kitex searches for go.mod from the current directory to the root.\n If go.mod is not found, kitex will generated one with go mod init. If go.mod is found, then kitex will check if the module name in go.mod is identical with the argument of -module; if they diffs, kitex will report an error and exit. Finally, the position of go.mod and its module names determines the import path in generated codes.    -I path Add a search path for IDLs.\n-type type Specify the type of IDLs. Currently available values are thrift and protobuf.\nKitex will guess the IDL type by the file extension. .thrift for thrift, .proto for protobuf.\n-v or -verbose Output more logs.\nAdvanced Options -use path When generating server codes (with -service), the -use option stops kitex from generating the kitex_gen and uses the import path given by the argument instead.\n-combine-service For thrift IDLs, when generating codes for a server, kitex only generates methods for the last service definition in the IDL. If there are multiple services in the IDL and you want to export all their abilities, the -combine-service option is for that.\nThis option creates a CombineService that assembles all methods of services in the target IDL and uses it in the main package. Notice that no two methods of services can have the same name.\n-protobuf value Pass an argument to protoc. The argument will be appended to the -go_out for protoc. See the documentation of protoc for available values.\n-thrift value Pass an argument to thriftgo. The argument will be appended to the -g go: for thriftgo. See the documentation of thriftgo for available values.\nKitex by default passes naming_style=golint,ignore_initialisms,gen_setter,gen_deep_equal to thriftgo and it can be overridden if you specify the same parameter in addition.\n","categories":"","description":"","excerpt":"kitex is a command line tool for code generation provided by the RPC …","ref":"/docs/kitex/tutorials/code-gen/code_generation/","tags":"","title":"Code Generation Tool"},{"body":"Introduction Middleware is the major method to extend the Kitex framework. Most of the Kitex-based extensions and secondary development features are based on middleware.\nBefore extending, it is important to remember two principles:\n Middleware and Suit are only allowed to be set before initializing Server and Client, do not allow modified dynamically. Middlewares are executed in the order in which they were added.  Middleware is defined in pkg/endpoint/endpoint.go, the two major types are:\n Endpoint is a function that accepts ctx, req, resp and returns err, see the example below. Middleware (aka MW) is also a function that receives and returns an Endpoint. 3.  In fact, a middleware is a function whose input and output are both Endpoint, which ensures the transparency to the application, and the application itself does not need to know whether it is decorated by the middleware. Due to this feature, middleware can be nested.\nMiddleware should be used in series, by calling the next, you can get the response (if any) and err returned by the latter middleware, and then process accordingly and return the err to the former middleware (be sure to check the err of next function returned, do not swallow the err) or set the response.\nClient-side Middleware There are two ways to add client-side middleware:\n client.WithMiddleware adds a middleware to the current client, executes after service circuit breaker middleware and timeout middleware. client.WithInstanceMW adds a middleware to the current client and executes after service discovery and load balancing. If there has instance circuit breaker, this middleware will execute after instance circuit breaker. (if Proxy is used, it will not be called).  Note that the above functions should all be passed as Options when creating the client.\nThe order of client middleware calls:\n the middleware set by client.WithMiddleware ACLMiddleware (ResolveMW + client.WithInstanceMW + PoolMW / DialerMW) / ProxyMW IOErrorHandleMW  The order in which the calls are returned is reversed.\nThe order of all middleware calls on the client side can be seen in client/client.go.\nServer-side Middleware The server-side middleware is different from the client-side.\nYou can add server-side middleware via server.WithMiddleware, and passing Option when creating the server.\nThe order of server-side middleware calls can be found in server/server.go.\nExample You can see how to use the middleware in the following example.\nIf you have a requirement to print out the request and the response, we can write the following MW:\nfunc PrintRequestResponseMW(next endpoint.Endpoint) endpoint.Endpoint { return func(ctx context.Context, request, response interface{}) error { fmt.Printf(\"request: %v\\n\", request) err := next(ctx, request, response) fmt.Printf(\"response: %v\", response) return err } } Assuming we are at Server side, we can use server.WithMiddleware(PrintRequestResponseMW) to use this MW.\n**The above scenario is only for example, not for production, there will be performance issues. **\nAttention If RPCInfo is used in a custom middleware, please pay attention to that RPCInfo will be recycled after the rpc is finished. If you start a goroutine in the middleware to modify RPCInfo, there will have some problems.\n","categories":"","description":"","excerpt":"Introduction Middleware is the major method to extend the Kitex …","ref":"/docs/kitex/tutorials/framework-exten/middleware/","tags":"","title":"Middleware Extensions"},{"body":"Protocols The table below is message types, codecs and transports supported by Kitex.\n   Message Types Codec Transport     PingPong Thrift / Protobuf TTHeader / HTTP2(gRPC)   Oneway Thrift TTHeader   Streaming Protobuf HTTP2(gRPC)     PingPong: the client always waits for a response after sending a request Oneway: the client does not expect any response after sending a request Streaming: the client can send one or more requests while receiving one or more responses.  Thrift When the codec is thrift, Kitex supports PingPong and Oneway. The streaming on thrift is under development.\nExample Given an IDL:\nnamespacegoechostructRequest{1:stringMsg}structResponse{1:stringMsg}serviceEchoService{ResponseEcho(1:Requestreq);// pingpong method onewayvoidVisitOneway(1:Requestreq);// oneway method }The layout of generated code might be:\n. └── kitex_gen └── echo ├── echo.go ├── echoservice │ ├── client.go │ ├── echoservice.go │ ├── invoker.go │ └── server.go └── k-echo.go The handler code in server side might be:\npackage main import ( \"xx/echo\" \"xx/echo/echoservice\" ) type handler struct {} func (handler) Echo(ctx context.Context, req *echo.Request) (r *echo.Response, err error) { //...  return \u0026echo.Response{ Msg: \"world\" } } func (handler) VisitOneway(ctx context.Context, req *echo.Request) (err error) { //...  return nil } func main() { svr, err := echoservice.NewServer(handler{}) if err != nil { panic(err) } svr.Run() } PingPong The code in client side might be:\npackage main import ( \"xx/echo\" \"xx/echo/echoservice\" ) func main() { cli, err := echoservice.NewClient(\"destServiceName\") if err != nil { panic(err) } req := echo.NewRequest() req.Msg = \"hello\" resp, err := cli.Echo(req) if err != nil { panic(err) } // resp.Msg == \"world\" } Oneway The code in client side might be:\npackage main import ( \"xx/echo\" \"xx/echo/echoservice\" ) func main() { cli, err := echoservice.NewClient(\"destServiceName\") if err != nil { panic(err) } req := echo.NewRequest() req.Msg = \"hello\" err = cli.VisitOneway(req) if err != nil { panic(err) } // no response return } Protobuf Kitex supports two kind of protocols that carries Protobuf payload:\n Kitex Protobuf  Only supports the PingPong type of messages. If any streaming method is defined in the IDL, the protocol will switch to gRPC.   The gRPC Protocol  The protocol that shipped with gRPC.    Example The following is an example showing how to use the streaming types.\nGiven an IDL:\nsyntax = \"proto3\";option go_package = \"echo\";package echo;message Request { string msg = 1;}message Response { string msg = 1;}service EchoService { rpc ClientSideStreaming(stream Request) returns (Response) {} // client streaming  rpc ServerSideStreaming(Request) returns (stream Response) {} // server streaming  rpc BidiSideStreaming(stream Request) returns (stream Response) {} // bidirectional streaming }The generated code might be:\n. └── kitex_gen └── echo ├── echo.pb.go └── echoservice ├── client.go ├── echoservice.go ├── invoker.go └── server.go The handler code in server side:\npackage main import ( \"sync\" \"xx/echo\" \"xx/echo/echoservice\" } type handler struct{} func (handler) ClientSideStreaming(stream echo.EchoService_ClientSideStreamingServer) (err error) { for { req, err := stream.Recv() if err != nil { return err } } } func (handler) ServerSideStreaming(req *echo.Request, stream echo.EchoService_ServerSideStreamingServer) (err error) { _ = req for { resp := \u0026echo.Response{Msg: \"world\"} if err := stream.Send(resp); err != nil { return err } } } func (handler) BidiSideStreaming(stream echo.EchoService_BidiSideStreamingServer) (err error) { var once sync.Once go func() { for { req, err2 := stream.Recv() log.Println(\"received:\", req.GetMsg()) if err2 != nil { once.Do(func() { err = err2 }) break } } }() for { resp := \u0026echo.Response{Msg: \"world\"} if err2 := stream.Send(resp); err2 != nil { once.Do(func() { err = err2 }) return } } return } func main() { svr, err := echoservice.NewServer(handler{}) if err != nil { panic(err) } svr.Run() } Streaming ClientSideStreaming:\npackage main import ( \"xx/echo\" \"xx/echo/echoservice\" } func main() { cli, err := echoseervice.NewClient(\"destServiceName\") if err != nil { panic(err) } cliStream, err := cli.ClientSideStreaming(context.Background()) if err != nil { panic(err) } for { req := \u0026echo.Request{Msg: \"hello\"} if err := cliStream.Send(req); err != nil { panic(err) } } } ServerSideStreaming:\npackage main import ( \"xx/echo\" \"xx/echo/echoservice\" } func main() { cli, err := echoseervice.NewClient(\"destServiceName\") if err != nil { panic(err) } req := \u0026echo.Request{Msg: \"hello\"} svrStream, err := cli.ServerSideStreaming(context.Background(), req) if err != nil { panic(err) } for { resp, err := svrStream.Recv() if err != nil { panic(err) } // resp.Msg == \"world\"  } } BidiSideStreaming:\npackage main import ( \"xx/echo\" \"xx/echo/echoservice\" } func main() { cli, err := echoseervice.NewClient(\"destServiceName\") if err != nil { panic(err) } req := \u0026echo.Request{Msg: \"hello\"} bidiStream, err := cli.BidiSideStreaming(context.Background()) if err != nil { panic(err) } go func() { for { req := \u0026echo.Request{Msg: \"hello\"} err := bidiStream.Send(req) if err != nil { panic(err) } } }() for { resp, err := bidiStream.Recv() if err != nil { panic(err) } // resp.Msg == \"world\"  } } ","categories":"","description":"Support PingPong、Oneway、Streaming.\n","excerpt":"Support PingPong、Oneway、Streaming.\n","ref":"/docs/kitex/tutorials/basic-feature/message_type/","tags":"","title":"Message Types"},{"body":"介绍 Middleware 是扩展 Kitex 框架的一个主要的方法，大部分基于 Kitex 的扩展和二次开发的功能都是基于 middleware 来实现的。\n在扩展过程中，要记得两点原则：\n 中间件和套件都只允许在初始化 Server、Client 的时候设置，不允许动态修改。 Middleware 是按照添加的先后顺序执行的。  Kitex 的中间件定义在 pkg/endpoint/endpoint.go 中，其中最主要的是两个类型：\n Endpoint 是一个函数，接受 ctx、req、resp，返回 err，可参考下方示例； Middleware（下称 MW）也是一个函数，接收同时返回一个 Endpoint。  实际上一个中间件就是一个输入是 Endpoint，输出也是 Endpoint 的函数，这样保证了对应用的透明性，应用本身并不会知道是否被中间件装饰的。由于这个特性，中间件可以嵌套使用。\n中间件是串连使用的，通过调用传入的 next，可以得到后一个中间件返回的 response（如果有）和 err，据此作出相应处理后，向前一个中间件返回 err（务必判断 next err 返回，勿吞了 err）或者设置 response。\n客户端中间件 有两种方法可以添加客户端中间件：\n client.WithMiddleware 对当前 client 增加一个中间件，在 Service 熔断和超时中间件之后执行； client.WithInstanceMW 对当前 client 增加一个中间件，在服务发现、负载均衡之后执行，如果有实例熔断器，会在实例熔断器后执行（如果使用了 Proxy 则不会调用到，如 Mesh 模式下）。  注意，上述函数都应该在创建 client 时作为传入的 Option。\n客户端中间件调用顺序 :\n client.WithMiddleware 设置的中间件 ACLMiddleware (ResolveMW + client.WithInstanceMW + PoolMW / DialerMW) / ProxyMW IOErrorHandleMW  调用返回的顺序则相反。\n客户端所有中间件的调用顺序可以看 client/client.go。\n服务端中间件 服务端的中间件和客户端有一定的区别。\n可以通过 server.WithMiddleware 来增加 server 端的中间件，使用方式和 client 一致，在创建 server 时通过 Option 传入。\n总的服务端中间件的调用顺序可以看 server/server.go。\n示例 我们可以通过以下这个例子来看一下如何使用中间件。\n假如我们现在有需求，需要在请求前打印出 request 内容，再请求后打印出 response 内容，可以编写如下的 MW：\nfunc PrintRequestResponseMW(next endpoint.Endpoint) endpoint.Endpoint { return func(ctx context.Context, request, response interface{}) error { fmt.Printf(\"request: %v\\n\", request) err := next(ctx, request, response) fmt.Printf(\"response: %v\", response) return err } } 假设我们是 Server 端，就可以使用 server.WithMiddleware(PrintRequestResponseMW) 来使用这个 MW 了。\n以上方案仅为示例，不可用于生产，会有性能问题。\n注意事项 如果自定义 middleware 中用到了 RPCInfo，要注意 RPCInfo 在 rpc 结束之后会被回收，所以如果在 middleware 中起了 goroutine 操作 RPCInfo 会出问题，不能这么做。\n","categories":"","description":"","excerpt":"介绍 Middleware 是扩展 Kitex 框架的一个主要的方法，大部分基于 Kitex …","ref":"/zh/docs/kitex/tutorials/framework-exten/middleware/","tags":"","title":"Middleware 扩展"},{"body":"kitex 是 Kitex 框架提供的用于生成代码的一个命令行工具。目前，kitex 支持 thrift 和 protobuf 的 IDL，并支持生成一个服务端项目的骨架。\n安装 go install github.com/cloudwego/kitex/tool/cmd/kitex 用 go 命令来安装是最简单的，你也可以选择自己从源码构建和安装。要查看 kitex 的安装位置，可以用：\ngo list -f {{.Target}} github.com/cloudwego/kitex/tool/cmd/kitex 依赖与运行模式 底层编译器 要使用 thrift 或 protobuf 的 IDL 生成代码，需要安装相应的编译器：thriftgo 或 protoc。\nkitex 生成的代码里，一部分是底层的编译器生成的（通常是关于 IDL 里定义的结构体的编解码逻辑），另一部分是用于连接 Kitex 框架与生成代码并提供给终端用户良好界面的胶水代码。\n从执行流上来说，当 kitex 被要求来给一个 thrift IDL 生成代码时，kitex 会调用 thriftgo 来生成结构体编解码代码，并将自身作为 thriftgo 的一个插件执行来生成胶水代码。当用于 protobuf IDL 时亦是如此。\n$\u003e kitex IDL | | thrift-IDL |---------\u003e thriftgo --gen go:... -plugin=... -out ... IDL | | protobuf-IDL ----------\u003e protoc --kitex_out=... --kitex_opt=... IDL 库依赖 kitex 生成的代码会依赖相应的 Go 语言代码库：\n 对于 thrift IDL，是 github.com/apache/thrift v0.13.0 对于 protobuf IDL，是  google.golang.org/protobuf v1.26.0  要注意的一个地方是，github.com/apache/thrift/lib/go/thrift 的 v0.14.0 版本开始提供的 API 和之前的版本是不兼容的，如果在更新依赖的时候给 go get 命令增加了 -u 选项，会导致该库更新到不兼容的版本造成编译失败。你可以通过额外执行一个命令来指定正确的版本：\ngo get github.com/apache/thrift@v0.13.0 或用 replace 指令强制固定该版本：\ngo mod edit -replace github.com/apache/thrift=github.com/apache/thrift@v0.13.0 使用 protobuf IDL 的注意事项 kitex 仅支持 protocol buffers 的 proto3 版本的语法。\nIDL 里的 go_package 是必需的，其值可以是一个用点（.）或斜线（/）分隔的包名序列，决定了生成的 import path 的后缀。例如\noption go_package = \"hello.world\"; // or hello/world 生成的 import path 会是 ${当前目录的 import path}/kitex_gen/hello/world。\n如果你给 go_package 赋值一个完整的导入路径（import path），那么该路径必须匹配到当前模块的 kitex_gen 才会生成代码。即：\n go_package=\"${当前模块的 import path}/kitex_gen/hello/world\";：OK，kitex 会为该 IDL 生成代码； go_package=\"${当前模块的 import path}/hello/world\";：kitex 不会为该 IDL 生成代码； go_package=\"any.other.domain/some/module/kitex_gen/hello/world\";：kitex 不会为该 IDL 生成代码；  你可以通过命令行参数 --protobuf Msome.proto=your.package.name/kitex_gen/wherever/you/like 来覆盖某个 proto 文件的 go_package 值。具体用法说明可以参考 Protocol Buffers 的官方文档。\n使用 基础使用 语法：kitex [options] IDL\n下面以 thrift IDL 作为例子。 Protobuf IDL 的使用也是类似的。\n生成客户端代码 kitex path_to_your_idl.thrift 执行后在当前目录下会生成一个名为 kitex_gen 目录，其中包含了 IDL 定义的数据结构，以及与 IDL 里定义的 service 相对应的 *service 包，提供了创建这些 service 的 client API。\n生成服务端代码 kitex -service service_name path_to_your_idl.thrift 执行后在当前目录下会生成一个名为 kitex_gen 目录，同时包含一些用于创建和运行一个服务的脚手架代码。具体见生成代码的结构一节的描述。\n生成代码的结构 假设我们有两个 thrift IDL 文件，demo.thrift 和 base.thrift，其中 demo.thrift 依赖了 base.thrift，并且 demo.thrift 里定义了一个名为 demo 的 service 而 base 里没有 service 定义。\n那么在一个空目录下，kitex -service demo demo.thrift 生成的结果如下：\n. ├── build.sh // 服务的构建脚本，会创建一个名为 output 的目录并生成启动服务所需的文件到里面 ├── handler.go // 用户在该文件里实现 IDL service 定义的方法 ├── kitex_gen // IDL 内容相关的生成代码 │ ├── base // base.thrift 的生成代码 │ │ ├── base.go // thriftgo 的产物，包含 base.thrift 定义的内容的 go 代码 │ │ └── k-base.go // kitex 在 thriftgo 的产物之外生成的代码 │ └── demo // demo.thrift 的生成代码 │ ├── demo.go // thriftgo 的产物，包含 demo.thrift 定义的内容的 go 代码 │ ├── k-demo.go // kitex 在 thriftgo 的产物之外生成的代码 │ └── demoservice // kitex 为 demo.thrift 里定义的 demo service 生成的代码 │ ├── demoservice.go // 提供了 client.go 和 server.go 共用的一些定义 │ ├── client.go // 提供了 NewClient API │ └── server.go // 提供了 NewServer API ├── main.go // 程序入口 └── script // 构建脚本 └── bootstrap.sh // 服务的启动脚本，会被 build.sh 拷贝至 output 下 如果不指定 -service 参数，那么生成的只有 kitex_gen 目录。\n选项 本文描述的选项可能会过时，可以用 kitex -h 或 kitex --help 来查看 kitex 的所有可用的选项。\n-service service_name 使用该选项时，kitex 会生成构建一个服务的脚手架代码，参数 service_name 给出启动时服务自身的名字，通常其值取决于使用 Kitex 框架时搭配的服务注册和服务发现功能。\n-module module_name 该参数用于指定生成代码所属的 Go 模块，会影响生成代码里的 import path。\n  如果当前目录是在 $GOPATH/src 下的一个目录，那么可以不指定该参数；kitex 会使用 $GOPATH/src 开始的相对路径作为 import path 前缀。例如，在 $GOPATH/src/example.com/hello/world 下执行 kitex，那么 kitex_gen/example_package/example_package.go 在其他代码代码里的 import path 会是 example.com/hello/world/kitex_gen/example_package。\n  如果当前目录不在 $GOPATH/src 下，那么必须指定该参数。\n  如果指定了 -module 参数，那么 kitex 会从当前目录开始往上层搜索 go.mod 文件\n 如果不存在 go.mod 文件，那么 kitex 会调用 go mod init 生成 go.mod； 如果存在 go.mod 文件，那么 kitex 会检查 -module 的参数和 go.mod 里的模块名字是否一致，如果不一致则会报错退出； 最后，go.mod 的位置及其模块名字会决定生成代码里的 import path。    -I path 添加一个 IDL 的搜索路径。\n-type type 指明当前使用的 IDL 类型，当前可选的值有 thrift 和 protobuf。\nKitex 会根据文件的扩展名来猜测相应的 IDL 类型，.thrift 是 thrift，.proto 是 protobuf。\n-v 或 -verbose 输出更多日志。\n高级选项 -use path 在生成服务端代码（使用了 -service）时，可以用 -use 选项来让 kitex 不生成 kitex_gen 目录，而使用该选项给出的 import path。\n-combine-service 对于 thrift IDL，kitex 在生成服务端代码脚手架时，只会针对最后一个 service 生成相关的定义。如果你的 IDL 里定义了多个 service 定义并且希望在一个服务里同时提供这些 service 定义的能力时，可以使用 -combine-service 选项。\n该选项会生成一个合并了目标 IDL 文件中所有 service 方法的 CombineService，并将其用作 main 包里使用的 service 定义。注意这个模式下，被合并的 service 之间不能有冲突的方法名。\n-protobuf value 传递给 protoc 的参数。会拼接在 -go_out 的参数后面。可用的值参考 protoc 的帮助文档。\n-thrift value 传递给 thriftgo 的参数。会拼接在 -g go: 的参数后面。可用的值参考 thriftgo 的帮助文档。kitex 默认传递了 naming_style=golint,ignore_initialisms,gen_setter,gen_deep_equal，可以被覆盖。\n","categories":"","description":"","excerpt":"kitex 是 Kitex 框架提供的用于生成代码的一个命令行工具。目前，kitex 支持 thrift 和 protobuf 的 IDL， …","ref":"/zh/docs/kitex/tutorials/code-gen/code_generation/","tags":"","title":"代码生成工具"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kitex/tutorials/basic-feature/","tags":"","title":"基本特性"},{"body":"协议支持 目前 Kitex 支持的消息类型、编解码协议和传输协议\n   消息类型 编码协议 传输协议     PingPong Thrift / Protobuf TTHeader / HTTP2(gRPC)   Oneway Thrift TTHeader   Streaming Protobuf HTTP2(gRPC)     PingPong：客户端发起一个请求后会等待一个响应才可以进行下一次请求 Oneway：客户端发起一个请求后不等待一个响应 Streaming：客户端发起一个或多个请求 , 等待一个或多个响应  Thrift 目前 Thrift 支持 PingPong 和 Oneway。Kitex 计划支持 Thrift Streaming。\nExample IDL 定义 :\nnamespacegoechostructRequest{1:stringMsg}structResponse{1:stringMsg}serviceEchoService{ResponseEcho(1:Requestreq);// pingpong method onewayvoidVisitOneway(1:Requestreq);// oneway method }生成的代码组织结构 :\n. └── kitex_gen └── echo ├── echo.go ├── echoservice │ ├── client.go │ ├── echoservice.go │ ├── invoker.go │ └── server.go └── k-echo.go Server 的处理代码形如 :\npackage main import ( \"xx/echo\" \"xx/echo/echoservice\" ) type handler struct {} func (handler) Echo(ctx context.Context, req *echo.Request) (r *echo.Response, err error) { //...  return \u0026echo.Response{ Msg: \"world\" } } func (handler) VisitOneway(ctx context.Context, req *echo.Request) (err error) { //...  return nil } func main() { svr, err := echoservice.NewServer(handler{}) if err != nil { panic(err) } svr.Run() } PingPong Client 侧代码 :\npackage main import ( \"xx/echo\" \"xx/echo/echoservice\" ) func main() { cli, err := echoservice.NewClient(\"destServiceName\") if err != nil { panic(err) } req := echo.NewRequest() req.Msg = \"hello\" resp, err := cli.Echo(req) if err != nil { panic(err) } // resp.Msg == \"world\" } Oneway Client 侧代码 :\npackage main import ( \"xx/echo\" \"xx/echo/echoservice\" ) func main() { cli, err := echoservice.NewClient(\"destServiceName\") if err != nil { panic(err) } req := echo.NewRequest() req.Msg = \"hello\" err = cli.VisitOneway(req) if err != nil { panic(err) } // no response return } Protobuf Kitex 支持两种承载 Protobuf 负载的协议：\n Kitex Protobuf  只支持 PingPong，若 IDL 定义了 stream 方法，将默认使用 gRPC 协议   gRPC 协议  可以与 gRPC 互通，与 gRPC service 定义相同，支持 Unary(PingPong)、 Streaming 调用    Example 以下给出 Streaming 的使用示例。\nIDL 定义 :\nsyntax = \"proto3\";option go_package = \"echo\";package echo;message Request { string msg = 1;}message Response { string msg = 1;}service EchoService { rpc ClientSideStreaming(stream Request) returns (Response) {} // 客户端侧 streaming  rpc ServerSideStreaming(Request) returns (stream Response) {} // 服务端侧 streaming  rpc BidiSideStreaming(stream Request) returns (stream Response) {} // 双向流 }生成的代码组织结构 :\n. └── kitex_gen └── echo ├── echo.pb.go └── echoservice ├── client.go ├── echoservice.go ├── invoker.go └── server.go Server 侧代码 :\npackage main import ( \"sync\" \"xx/echo\" \"xx/echo/echoservice\" } type handler struct{} func (handler) ClientSideStreaming(stream echo.EchoService_ClientSideStreamingServer) (err error) { for { req, err := stream.Recv() if err != nil { return err } } } func (handler) ServerSideStreaming(req *echo.Request, stream echo.EchoService_ServerSideStreamingServer) (err error) { _ = req for { resp := \u0026echo.Response{Msg: \"world\"} if err := stream.Send(resp); err != nil { return err } } } func (handler) BidiSideStreaming(stream echo.EchoService_BidiSideStreamingServer) (err error) { var once sync.Once go func() { for { req, err2 := stream.Recv() log.Println(\"received:\", req.GetMsg()) if err2 != nil { once.Do(func() { err = err2 }) break } } }() for { resp := \u0026echo.Response{Msg: \"world\"} if err2 := stream.Send(resp); err2 != nil { once.Do(func() { err = err2 }) return } } return } func main() { svr, err := echoservice.NewServer(handler{}) if err != nil { panic(err) } svr.Run() } Streaming ClientSideStreaming:\npackage main import ( \"xx/echo\" \"xx/echo/echoservice\" } func main() { cli, err := echoservice.NewClient(\"destServiceName\") if err != nil { panic(err) } cliStream, err := cli.ClientSideStreaming(context.Background()) if err != nil { panic(err) } for { req := \u0026echo.Request{Msg: \"hello\"} if err := cliStream.Send(req); err != nil { panic(err) } } } ServerSideStreaming:\npackage main import ( \"xx/echo\" \"xx/echo/echoservice\" } func main() { cli, err := echoservice.NewClient(\"destServiceName\") if err != nil { panic(err) } req := \u0026echo.Request{Msg: \"hello\"} svrStream, err := cli.ServerSideStreaming(context.Background(), req) if err != nil { panic(err) } for { resp, err := svrStream.Recv() if err != nil { panic(err) } // resp.Msg == \"world\"  } } BidiSideStreaming:\npackage main import ( \"xx/echo\" \"xx/echo/echoservice\" } func main() { cli, err := echoservice.NewClient(\"destServiceName\") if err != nil { panic(err) } req := \u0026echo.Request{Msg: \"hello\"} bidiStream, err := cli.BidiSideStreaming(context.Background()) if err != nil { panic(err) } go func() { for { req := \u0026echo.Request{Msg: \"hello\"} err := bidiStream.Send(req) if err != nil { panic(err) } } }() for { resp, err := bidiStream.Recv() if err != nil { panic(err) } // resp.Msg == \"world\"  } } ","categories":"","description":"支持 PingPong、Oneway、Streaming。\n","excerpt":"支持 PingPong、Oneway、Streaming。\n","ref":"/zh/docs/kitex/tutorials/basic-feature/message_type/","tags":"","title":"消息类型"},{"body":"第5期周报\n","categories":"","description":"","excerpt":"第5期周报\n","ref":"/zh/community/weekly_report/5th/","tags":"","title":"CloudWeGo 第05期周报"},{"body":"第6期周报\n","categories":"","description":"","excerpt":"第6期周报\n","ref":"/zh/community/weekly_report/6th/","tags":"","title":"CloudWeGo 第06期周报"},{"body":"第7期周报\n","categories":"","description":"","excerpt":"第7期周报\n","ref":"/zh/community/weekly_report/7th/","tags":"","title":"CloudWeGo 第07期周报"},{"body":"第8期周报\n","categories":"","description":"","excerpt":"第8期周报\n","ref":"/zh/community/weekly_report/8th/","tags":"","title":"CloudWeGo 第08期周报"},{"body":"第9期周报\n","categories":"","description":"","excerpt":"第9期周报\n","ref":"/zh/community/weekly_report/9th/","tags":"","title":"CloudWeGo 第09期周报"},{"body":"第10期周报\n","categories":"","description":"","excerpt":"第10期周报\n","ref":"/zh/community/weekly_report/10th/","tags":"","title":"CloudWeGo 第10期周报"},{"body":"第11期周报\n","categories":"","description":"","excerpt":"第11期周报\n","ref":"/zh/community/weekly_report/11th/","tags":"","title":"CloudWeGo 第11期周报"},{"body":"第 12 期周报\n","categories":"","description":"","excerpt":"第 12 期周报\n","ref":"/zh/community/weekly_report/12th/","tags":"","title":"CloudWeGo 第12期周报"},{"body":"第 13 期周报\n","categories":"","description":"","excerpt":"第 13 期周报\n","ref":"/zh/community/weekly_report/13th/","tags":"","title":"CloudWeGo 第13期周报"},{"body":"第 14 期周报\n","categories":"","description":"","excerpt":"第 14 期周报\n","ref":"/zh/community/weekly_report/14th/","tags":"","title":"CloudWeGo 第14期周报"},{"body":"第 15 期周报\n","categories":"","description":"","excerpt":"第 15 期周报\n","ref":"/zh/community/weekly_report/15th/","tags":"","title":"CloudWeGo 第15期周报"},{"body":"第 16 期周报\n","categories":"","description":"","excerpt":"第 16 期周报\n","ref":"/zh/community/weekly_report/16th/","tags":"","title":"CloudWeGo 第16期周报"},{"body":"第 17 期周报\n","categories":"","description":"","excerpt":"第 17 期周报\n","ref":"/zh/community/weekly_report/17th/","tags":"","title":"CloudWeGo 第17期周报"},{"body":"Server The configuration items on the Server side all use server.xxx when initializing the Server, such as:\npackage main import \"github.com/cloudwego/hertz/pkg/app/server\" func main() { h := server.New(server.WithXXXX()) ... }    Configuration Name Type Description     WithTransport network.NewTransporter Replace the transport. Default：netpoll.NewTransporter   WithHostPorts string Specify the listening address and port   WithKeepAliveTimeout time.Duration Set the keep-alive time of tcp persistent connection, generally no need to modify it, you should more pay attention to idleTimeout rather than modifying it. Default: 1min.   WithReadTimeout time.Duration The timeout of data reading. Default：3min.   WithIdleTimeout time.Duration The free timeout of the request link for persistent connection. Default: 3min.   WithMaxRequestBodySize int Max body size of a request. Default: 4M (the corresponding value of 4M is 4*1024*1024).   WithRedirectTrailingSlash bool Whether to redirect with the / which is at the end of the router automatically. For example： If there is only /foo/ in the router, /foo will be redirected to /foo/. And if there is only /foo in the router, /foo/ will be redirected to /foo. Default: true.   WithRemoveExtraSlash bool RemoveExtraSlash makes the parameter still valid when it contains an extra /. For example, if WithRemoveExtraSlash is true user//xiaoming can match the user/:name router. Default: false.   WithUnescapePathValues bool If true, the request path will be escaped automatically (eg. ‘%2F’ -\u003e ‘/'). If UseRawPath is false (the default), UnescapePathValues is true, because URI().Path() will be used and it is already escaped. To set WithUnescapePathValues to false, you need to set WithUseRawPath to true. Default (true).   WithUseRawPath bool If true, the original path will be used to match the route. Default: false.   WithHandleMethodNotAllowed bool If true when the current path cannot match any method, the server will check whether other methods are registered with the route of the current path, and if exist other methods, it will respond “Method Not Allowed” and return the status code 405; if not, it will use the handler of NotFound to handle it. Default: false.   WithDisablePreParseMultipartForm bool If true, the multipart form will not be preprocessed. The body can be obtained via ctx.Request.Body() and then can be processed by user. Default: false.   WithStreamBody bool If true, the body will be handled by stream processing. Default: false.   WithNetwork string Set the network protocol, optional: tcp，udp，unix(unix domain socket). Default: tcp.   ContinueHandler func(header *RequestHeader) bool Call the ContinueHandler after receiving the Expect 100 Continue header. With ContinueHandler, the server can decide whether to read the potentially large request body based on the header.   PanicHandler HandlerFunc Handle panic used to generate error pages and return error code 500.   NotFound HandlerFunc The handler to be called when the route does not match.   WithExitWaitTime time.Duration Set the graceful exit time. the Server will stop connection establishment for new requests and set the Connection: Close header for each request after closing. When the set time is reached, Server will to be closed. the Server can be closed early when all connections have been closed. Default: 5s.   WithTLS tls.Config Configuring server tls capabilities.   WithListenConfig net.ListenConfig Set the listener configuration. Can be used to set whether to allow reuse ports, etc.   WithALPN bool Whether to enable ALPN. Default: false.   WithTracer tracer.Tracer Inject tracer implementation, if not inject Tracer. Default: close.   WithTraceLevel stats.Level Set trace level, Default: LevelDetailed.    Client The configuration items on the Client side all use server.xxx when initializing the Server, such as:\npackage main import \"github.com/cloudwego/hertz/pkg/app/client\" func main() { c, err := client.NewClient(client.WithXxx()) ... }    Configuration Name Type Description     WithDialTimeout time.Duration Connection establishment timeout. Default: 1s.   WithMaxConnsPerHost int Set the maximum number of connections for every host. Default: 512.   WithMaxIdleConnDuration time.Duration Set the idle connection timeout, which will close the connection after the timeout Default: 10s.   WithMaxConnDuration time.Duration Set the maximum keep-alive time of the connection, when the timeout expired, the connection will be closed after the current request is completed. Default: infinite.   WithMaxConnWaitTimeout time.Duration Set the maximum time to wait for an idle connection. Default: no wait.   WithKeepAlive bool Whether to use persistent connection. Default: true.   WithMaxIdempotentCallAttempts int Set the times of failed retries. Default: 5.   WithClientReadTimeout time.Duration Set the maximum time to read the response. Default: infinite.   WithTLSConfig *tls.Config Set the client’s TLS config for mutual TLS authentication.   WithDialer network.Dialer Set the network library used by the client. Default: netpoll.   WithResponseBodyStream bool Set whether to use stream processing. Default: false.   WithDialFunc client.DialFunc Set Dial Function.    ","categories":"","description":"","excerpt":"Server The configuration items on the Server side all use server.xxx …","ref":"/docs/hertz/reference/config/","tags":"","title":"Configuration instruction"},{"body":"","categories":"","description":"","excerpt":"","ref":"/security/safety-bulletin/detail/","tags":"","title":"detail"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/","tags":"","title":"Documentation"},{"body":"Server Run hertz  hello ：Example of launching a hertz “hello world” application  Config  config ：Example of configuring hertz server  Protocol  Protocol ：Example of hertz using protocols such as http1, tls, etc  Route  Route ：Examples of registering routes, using route groups, and parameter routes  Middleware  CORS ：Example of using the CORS middleware basic_auth ：Example of using basic auth middleware custom ：Example of custom middleware  Parameter binding and validation  binding ：Example of parameter binding and validation  Get Parameters  parameters ：Example of getting query, form, cookie, etc. parameters  Documents  file ：Examples of file upload, file download, and static file services  Render  render ：Example of render body as json, html, protobuf, etc  Redirect  redirect ：Example of a redirect to an internal/external URI  Streaming read/write  streaming ：Example of streaming read/write using hertz server  Graceful shutdown  graceful_shutdown ：Example of hertz server graceful shutdown  Unit test  unit_test ：Example of writing unit tests using the interface provided by hertz without network transmission  Tracing  tracer ：Example of hertz using Jaeger for link tracing  Monitoring  monitoring ：hertz Example of metrics monitoring with Prometheus  Client Send request  send_request ：Example of sending an http request using the hertz client  Client config  client_config ：Example of configuring the hertz client  TLS  tls ：Example of hertz client sending a tls request  Add parameters  add_parameters ：Example of adding request parameters using the hertz client  Upload file  upload_file ：Example of uploading a file using the hertz client  Middleware  middleware ：Example of using the hertz client middleware  Streaming read  streaming_read ：Example of a streaming read response using the hertz client  Forward proxy  forward_proxy ：Example of configuring a forward proxy using the hertz client  ","categories":"","description":"","excerpt":"Server Run hertz  hello ：Example of launching a hertz “hello world” …","ref":"/docs/hertz/tutorials/example/","tags":"","title":"Example code"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/pilota/guide/","tags":"","title":"Guide"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/pilota/guide/","tags":"","title":"Guide"},{"body":"Why Plugin is needed Add some customized meta information for Pilota according to IDL generated Struct and other types.\nFor example, in order to add #[derive(serde::Serialize, serde::Deserialize]).\nHow to write a Plugin Plugin Implementation #[derive(Clone, Copy)]struct SerdePlugin;implpilota_build::PluginforSerdePlugin{fn on_item(\u0026mutself,cx: \u0026mutpilota_build::Context,def_id: pilota_build::DefId,// item 的 def_id item: std::sync::Arc\u003cpilota_build::rir::Item\u003e,){match\u0026*item{pilota_build::rir::Item::Message(_)|pilota_build::rir::Item::Enum(_)|pilota_build::rir::Item::NewType(_)=\u003ecx.with_adjust(def_id,|adj|{// Adjust's add_attrs method can add attributes to the Node corresponding to def_id, // which will be used to generate code later in the Codegen phase adj.add_attrs(\u0026[parse_quote!(#[derive(::serde::Serialize, ::serde::Deserialize)])])}),_=\u003e{}};pilota_build::plugin::walk_item(self,cx,def_id,item)}}Plugin Usage Pass the plugin method provided by the Builder.\npilota_build::thrift().plugin(SerdePlugin).write()","categories":"","description":"","excerpt":"Why Plugin is needed Add some customized meta information for Pilota …","ref":"/docs/pilota/guide/plugin/","tags":"","title":"How to write a Plugin？"},{"body":"案例介绍   企业用户如何搭建针对 Kitex 的可观测性系统？如何在 K8s 集群下使用 Kitex ?\n本文将从以下 4 个方面介绍华兴证券基于 Kitex 在多机房 K8s 集群下的实践经验，包括：\n 针对 Kitex 的可观测性系统搭建经验； 服务压力测试中遇到的问题以及解决方案； Kitex 的不同连接类型在 K8s 同集群/跨集群调用下的一些问题和解决方案； 实践中遇到的其他问题以及解决方案；  Kitex 的可观性系统搭建 华兴证券 CloudWeGo-Kitex 使用情况 去年 6 月 1 日，华兴证券相关研发团队成立。Kitex 在 7 月 12 日发布了首个版本，10 天后就引入了 Kitex。\n选择 Kitex 的原因是：团队早期成员比较了解 Kitex，为了快速支撑业务迭代和验证，选择最熟悉的框架，不但使用上比较习惯，对性能和功能方面也比较有把握。\n后来也支撑了华兴证券 APP 的快速上线，大约 4 个月之后就上线了 APP 的第一个版本。\n下图是业务的微服务调用关系图，一共有三十多个微服务，调用链路数超过 70。服务分别部署在两个机房。核心业务比如交易、行情等部署在私有机房。 非核心的业务，比如资讯、股票信息等部署在阿里的金融云，这样能够更好地利用金融云已有的基础设施比如 MySQL、Kafka 等，作为初创团队，能够降低整体的运维压力。 考虑到性能以及安全方面的因素，两个机房之间专门拉了专线。服务之间存在一些跨机房的依赖。跨机房调用会产生很多问题，后文会详细说明。\nTracing 选型 服务数多了之后，我们需要一套链路追踪系统来描绘调用链路每个环节的耗时情况。考虑到 Kitex 原生支持 Opentracing，为减少集成成本，我们调研了符合 Opentracing 规范的产品。\n排除掉收费的、客户端不支持 Go 之后，就剩阿里云的链路追踪产品和 Uber 公司出品的 Jaeger，考虑到私有机房也要部署，最终选择了 Jaeger。\nKitex 接入 Tracing 选定方案之后，开始对 Kitex 的这个功能进行测试，结果发现当时去年 9 月初的 Kitex 版本并不支持跨服务的 Tracing，原因是调用的时候，没有把 Trace 信息发送给下游，如图所示， 这样上下游是两个孤立的 Trace（OpenTracing 规范里称为 Span），于是就无法通过一个 TraceID 去串起整条链路。当时任务比较急，于是我们没有等 Kitex 官方的实现，决定自研。\n为了自研，我们结合 Kitex 的源码，梳理出客户端和服务端的流程。可以看出 Kitex 的上下游都内置了 Tracer 的 Hook。这里我们要解决的问题是，如何把 Span 信息进行跨服务传输？\n经调研，实现透传有三种方案。\n第一种是在消息层搞一个 Thrift 协议的拓展，把 Trace 信息塞进去。原因是 Thrift 本身没有 Header 结构，只能进行协议的拓展。好在 Kitex 支持自定义的协议拓展，因此具备可行性，然而开发成本较高，所以没选择这种方案。\n第二种是在 IDL 里增加通用参数，在字段里存 Trace 信息。缺点是业务无关的字段要在 IDL 里，对性能有一定的影响。毕竟需要通过 Kitex 的中间件，通过反射来提取。\n第三种是利用了 Kitex 提供的传输层透传能力，对业务没有侵入性。最后选择了这一种方案。\n透传方案定了之后，整体的流程就清晰了。首先客户端会在 metaHandler.write 里通过 CTX 获取当前 Span，提取并写入 spanContext 到 TransInfo 中。\n然后服务端，在 metaHandler.Read 里读取 spanContext 并创建 ChildOf 关系的 Span，中间件结束时 span.finish()，最后为了防止产生孤立 Trace，New 服务端时不使用 Kitex 提供的 Tracing 的 Option。\n这里是因为同一个服务可能分别作为 Kitex 上下游，Tracer 如果共用，需要分别加特殊逻辑，实现上有点复杂。\nTracing 基础库 为了充分利用 Tracing 的能力，除了 Kitex，我们在基础库中也增加了 Gin、Gorm、Redis、Kafka 等组件的 Tracing。\n下面展示实际的一条链路。功能是通过短信验证码进行登录。先是作为 HTTP 服务的 API 入口，然后调用了一个短信的 RPC 服务，RPC 服务里面通过 Redis 来检查验证码。 通过之后调用用户服务，里面可能进行一些增加用户的 MySQL 操作。最后把用户登录事件发给 Kafka，然后运营平台进行消费，驱动一些营销活动。可以看出最耗时的部分是关于新增用户的一堆 MySQL 操作。\n对错误的监控 Tracing 一般只关注调用耗时，然而一条链路中可能出现各种错误：\n Kitex   Kitex RPC 返回的 err（Conn Timeout、Read Timeout 等）； IDL 里自定义的业务 Code（111: 用户不存在）。  2.HTTP\n 返回的 HTTP 状态码（404、503）； JSON 里的业务 Code（-1: 内部错误）。  如何对这类错误进行监控？主要有以下三种方案：\n  打日志 + 日志监控，然后通过监控组件，这种方案需要解析日志，所以不方便；\n  写个中间件上报到自定义指标收集服务，这种方案优点是足够通用，但是需要新增中间件。同时自定义指标更关注具体的业务指标；\n  利用 Tracing 的 Tag，这种方案通用且集成成本低。\n  具体实现如下：\n Kitex 的 err、以及 HTTP 的状态码，定义为系统码； IDL 里的 Code 以及 HTTP 返回的 JSON 里的 Code，定义成业务码； Tracing 基础库里提取相应的值，设置到 span.tag 里； Jaeger 的 tag-as-field 配置里加上相应的字段（原始的 Tags，为 es 里的 Nested 对象，无法在 Grafana 里使用 Group By）。  监控告警 在增加错误监控的基础上，我们构建了一套监控告警系统体系。\n这里重点看一下刚才的链路追踪相关的内容。首先每个业务容器会把指标发送到 Jaeger 服务里。Jaeger 最终把数据落盘到 es 中。然后我们在 Grafana 上配置了一堆看板以及对应的告警规则。\n触发报警时，最终会发送到我们自研的 alert-webhook 里。\n自研的部分首先进行告警内容的解析，提取服务名等信息，然后根据服务的业务分类，分发到不同的飞书群里，级别高的报警会打加急电话。这里也是用到了飞书的功能。\nGrafana 里我们配置了各类型服务调用耗时、错误码一体化看板，描述了一个服务的方方面面的指标。包括日志监控、错误码监控、QPS 和调用耗时、容器事件监控、容器资源监控等。\n下图展示了飞书告警卡片。包括 RPC 调用超时、系统码错误、业务码错误。\n这里我们做了两个简单的工作，一个是带上了 TraceID，方便查询链路情况。另一个是把业务码对应的含义也展示出来，研发收到报警之后就不用再去查表了。\n本章小结\n 完成了 Tracing 接入 Kitex，实现跨服务传递； 对 Tracing 基础库扩展了其他类型中间件（Gin、Gorm、Redis、Kafka）的支持； 对 Tracing 基础库增加了系统码、错误码实现对错误的监控； 配置了全方位的服务指标看板； 结合 es、Grafana、飞书以及自研告警服务，搭建了针对微服务的监控告警系统。  这样我们就完成了可观测性体系的搭建。\n服务压力测试中遇到的问题以及解决方案 完成了监控告警体系之后，我们希望对服务进行压测，来找出性能瓶颈。第二部分介绍一下服务压测中遇到的问题和解决方案。\nKitex v0.0.8：连接超时问题 首先我们发现，QPS=150 左右，Kitex 出现连接建立超时的错误。当时我们检查了下 CPU、网络、内存等均没有达到限制。先是怀疑连接池大小不太够，于是测了下 10 和 1000，如上图所示，结果在报错数目上没有区别。 另外观察到的一个现象是，压测期间出现接近 5000 的 Time Wait 状态。\n5000 的限制，是因为达到了 tcp_max_tw_buckets 的设置的值。超过这个值之后，新的处于 Time Wait 状态的连接会被销毁，这样最大值就保持在 5000 了。 于是我们尝试进行排查，但没有思路，于是去翻看 Kitex 的 Issue，发现有人遇到相同的问题。\n原来，v0.0.8 版本的 Kitex，在使用域名的方式来新建 Client 的时候，会导致连接池失效。因为把连接放回连接池时，用的 Key 是解析之后的 IP，而 GET 的时候，用的是解析前的域名，这样根本 Get 不到连接，于是不停创建短连接。 这样的两个后果是：建立连接比较耗时，另一方面请求执行完毕之后都会关闭掉连接，于是导致了大量的 Time Wait。\n为了进行验证，我把测试服务改成了 IP 访问，然后比较了 IP 访问和域名访问以及不同连接池大小的情况。可以看出：IP 访问（连接池有效），但是连接池比较小的情况，出现减少的 Timeout。 连接池 100，Timeout 消失。而中间的域名访问的情况下，出现大量 Timeout。\nKitex v0.1.3：连接池问题修复 看代码得知在 Kitex v0.1.3 修复了这个问题。\n于是我们打算升级 Kitex 的版本，因为当时已经上了生产环境，在升级基础组件之前，需要进行验证，看一下不同连接池大小状态下的表现。还是域名模式，QPS 为 150 的情况下，随着连接池大小的增加，Timeout 的情况逐渐变少到消失。\n继续进行压测，我们发现 QPS=2000 的时候又出现了报错。结合监控，发现原因是连接建立的时候超过了默认的 50ms。\n我们讨论了几种解决方案：\n 修改超时配置。然而，交易日的 9:30-9:35 有⼀堆集中交易请求，突发的流量，耗时长了体验不好，可能会影响 APP 收入，我们希望系统性能保持稳定。 进行连接耗时的优化。然而 Kitex 已经使用了 Epoll 来处理创建连接的事件，作为使用方，进一步优化的难度和成本都太大。 MaxidleTimeout 参数改成无限大？比如先创建一个足够大的池，然后随着用户请求，池变得越来越大，最终稳定下来。但是每次服务升级之后，这个池就空了，需要慢慢恢复。 进行连接预热。  其实连接预热就相当于压测结束之后立马趁热再压一次，如图，可以发现 QPS=2000 的情况下，几乎都走了连接池，没有报错。因此，如果服务启动时能够进行连接预热，就可以省下建立连接的时间，使服务的性能保持稳定。\n当时 CloudWeGo 团队针对我们公司建了企业用户交流群，于是我们就向群里的 Kitex 研发提了连接预热的需求。其开发之后提供了连接预热个数的选项。我们也进行了测试。按照 QPS=2000 进行测试，\n WARM_UP_CONN_NUM=0：大约 1s 报错； WARM_UP_CONN_NUM=100：大约 4s 报错；   WARM_UP_CONN_NUM=1000：大约 4s 报错，但可以看出一开始都无需新建连接； WARM_UP_CONN_NUM=2000：无报错。  本章小结如下：\n Kitex v0.0.8：域名模式下存在连接池失效问题，v0.1.3 中修复； Kitex v0.1.3：可进一步通过连接预热功能提高系统性能。  Kitex 的不同连接类型在 K8s 同集群/跨集群调用下的一些问题和解决方案 长连接的问题：跨集群调用 第三部分我们讨论一下 Kitex 的不同连接类型在 K8s 同集群/跨集群调用下的一些问题和解决方案。\n首先是长连接跨集群调用下的问题。服务在跨集群调用时，其源 IP: 端口为宿主机的，数量有限，而目的 IP: 端口为下游集群的 LB，一般是固定的。\n那么，当长连接池数目比较大（比如数千），且上游较多（各种服务、每个都多副本，加起来可能数十个）的情况下，请求高峰时段可能导致上游宿主机的源端口不够用。同集群内跨机器调用走了 vxlan，因此没有这个问题。\n解决方案有两类：\n 硬件方案：机器； 软件方案：对于下游为 Kitex 服务，改用 Mux 模式（这样少量连接就可以处理大量并发的请求）。下游不是 Kitex 框架，因为 Mux 是私有协议，不支持非 Kitex。此时可考虑增加下游服务的 LB 数量，比如每个 LB 上分配多个端口。  比较起来，改造成 Mux 模式成本最低。\n连接多路复用的问题：滚动升级 但是多路复用模式，在 K8s 场景下，存在一个滚动升级相关的问题。我们先介绍下 Service 模式， K8s 的 Service 模式采用了 IPVS 的 Nat 模式（DR 和隧道模式不支持端口映射），链路为：\n上游容器←→ClusterIP（服务的虚拟 IP）←→下游容器\n然后我们看看滚动升级流程：\n 新容器启动。 新容器 Readiness Check 通过，之后做两件事情：  更新 Endpoints 列表：新增新容器，删除旧容器； 发送 sigTerm 到旧容器的 1 号进程。   由于更新了 Endpoints 列表，Endpoints 列表发生更新事件，立即回调触发规则更新逻辑（syncProxyRules）：  添加新容器到 IPVS 的 rs，权重为 1； 如果此时 IPVS 的旧容器的中 ActiveConn + InactiveConn \u003e 0（即已有连接还在），旧容器的权重会改成 0，但不会删除 rs。    经过步骤 3 之后，已有的连接仍然能够正常工作（因为旧容器 rs 未删），但新建的连接会走到新的容器上（因为旧容器权重 =0）。\n在 Service 模式下，上游通过一个固定的 IP: 端口来访问下游，当下游滚动升级的时候，上游看到的地址并未变化，即无法感知到滚动升级。于是，下游即使有优雅退出，但上游并不知道下游开始优雅退出了。之后可能的情况是：\n 下游发现连接繁忙，一直没有主动关闭，导致 K8s 配置的优雅升级时间超时，强制 Kill 进程，连接关闭，上游报错。 下游发现连接空闲，主动关闭，然而客户端在关闭之前恰好拿到了连接（且认为可用），然后发起请求，实际上由于连接关闭，发起请求失败报错。  针对此问题，解决方案如下：\n 同集群调用：改用 Headless Service 模式（结合 DNSResolver）：通过 DNS 列表的增删来感知下游变动； 跨集群调用：借鉴 HTTP2 的 GOAWAY 机制。  具体，可采用如下方式：\n 收到 sigTerm 的下游直接告诉上游（通过之前建立的 Conn1），同时下游继续处理发来的请求。 上游收到关闭信息之后：  新请求通过新建 Conn2 来发； 已有的请求仍然通过 Conn1，且处理完了之后，等下游优雅关闭 Conn1。    这种方式的优点是同集群跨集群均可使用，缺点是需要 Kitex 框架支持。在我们找 Kitex 团队讨论之后，他们也提供了排期支持本需求。\n连接多路复用的滚动升级测试：Headless Service 模式 在 Kitex 团队开发期间，我们测下 Kitex 已有版本对 Headless Service 模式下的滚动升级功能。\n测试方案如下：\n Kitex 版本 v0.1.3； 上下游均为 Mux 模式； 上游的加了个自定义 DNSResolver，刷新时间为 1s，加日志打印解析结果； 下游的退出信号处理，收到 sigTerm 之后特意 Sleep 10s（用来排除这个 Case：服务端发现连接空闲关闭了，但客户端在关闭之前恰好拿到连接，接着认为未关闭，实际上已经关闭，而客户端发起了请求，于是导致报错）； QPS=100 恒定压上游，然后触发下游滚动升级。  实测报错如下图：\n时序分析如下：\n 旧下游收到 sigTerm，开始 Sleep 10s； 上游解析到旧下游的 IP，向旧下游发起请求； DNS 规则更新：旧上游 IP 解析项消失，新下游解析项出现； 上游请求报错； 旧下游sleep完成，开始退出逻辑。  可见报错时旧下游还未执行退出逻辑，排除旧下游主动关闭连接。请求旧下游期间，且此时解析到新容器 IP（移除了旧容器 IP），报错是因为还没到退出逻辑的时候。因此推测，解析条目变化导致了报错。\n根据推测，结合代码（Kitex 客户端部分）分析，可能出现以下并发问题：\n 【协程1】客户端从 Mux池里取出 conn1，即将发起请求（所以没有机会再检查 conn1 状态了）； 【协程2】DNS 更新，移除了 IP，于是 Clean 方法中关闭了 conn1； 【协程1】客户端用 conn1 发起请求，导致报错 conn closed。  于是我们向 CloudWeGo 提了 Issue，他们很快修复了这个问题。\n连接多路复用的滚动升级测试：Service 模式 同样地，在 Service 模式中，测试方案如下：\n Kitex 版本使用 Feature 分支：mux-graceful-shutdown； 上下游均为 Mux 模式、服务发现使用 Service 模式； 恒定 QPS=200 压上游，20s 触发下游滚动升级； 另外写个服务打印期间的 IPVS 的日志； 下游的退出信号处理，收到 SigTerm 之后特意 sleep 10s（保证 ipvs 规则已更新）。  测试结果如下： 报错：INFO[0050] “{\"code\":-1,\"message\":\"remote or network error: conn closed\"}\"。\n时序分析为：\n 旧下游收到 sigTerm，开始 sleep 10s。 IPVS规则变化：  新下游 weight=1，ac=0，inac=0； 旧下游 weight=0，ac=2，inac=0。   旧下游 sleep 完成，进入最长为 15s（WithExitWaitTime）的优雅退出。 上游请求报错。 旧下游打印了最后一条日志。 IPVS 规则变化：  新下游 weight=1，ac=2，inac=0 =\u003e ac=2 说明上游新建连接到新容器； 旧下游 weight=0，ac=0，inac=2 =\u003e inac=2 表示连接关闭。   IPVS 规则变化：旧下游的规则被移除。  因此我们得出结论，报错发生在优雅退出期间。最后一条日志时刻大于报错时刻，因此，排除 K8s 的问题，确认 Conn Closed 是由 Kitex 导致的。 之后我们和 Kitex 研发团队沟通了分析结果，找到了 Root Cause，是因为假设了新的下游会有一个新的地址（但实际中 Service 模式都是一个地址），导致新请求取到了老请求的连接并进行关闭。对此进行了修复：\n连接多路复用的问题：下游扩容 如果⽤ Service 模式（上游看到的下游就是体现为⼀个 IP），创建的 TCP 连接会在最开始固定的几个下游 POD 上，之后如果扩容增加 POD，新创建的 POD 就不会路由到了，导致扩容实际上无效。\n解决方案如下：\n 同集群调用：可用 Headless Service 模式，由于 DNS 解析能够得到所有 POD，路由没问题。 跨集群调用：不在同集群内， Headless Service 模式无效，考虑如下方案：   方案1：修改服务发现机制。 优点：Kitex 无需改动。 缺点：增加依赖项（服务发现组件）。 方案2：下游先升级，之后上游 Redeploy 一下，让连接分布到下游的各种实例上。 优点：Kitex 无需改动。 缺点：上游可能很多，逐个 Redeploy 非常不优雅。 方案3：上游定期把 Mux 给过期掉，然后新建连接。 优点：彻底解决。 缺点：需要 Kitex 支持。  本章小结如下：\n 首先，针对长连接模式分析了跨集群时上游源端口数问题，希望通过多路复用模式解决； 其次，针对多路复用模式 + K8s Headless Service 模式的优雅升级，实测报错，分析定位了原因，Kitex 研发团队及时解决了相应问题； 再次，针对多路复用模式 + K8s Service 模式下的优雅升级提出了方案，Kitex 团队完成了实现，迭代了一轮，测试通过； 最后，针对多路复用模式 + K8s Service 模式下的下游副本扩容时路由不到的问题分析了原因，提出了方案，目前方案待实现。  实践中遇到的其他问题以及解决方案 RPC Timeout Context Canceled 错误 第四部分我们分析下实践中遇到的其他问题以及解决方案。研发同学发现日志出现 contexe canceled 的错误，分析日志发现出现频率低，一天只有几十条，属于偶发报错。\n我们推测是用户手机因为某种原因关闭了进行中的连接所导致，对此进行本地验证。三个部分：首先Gin 客户端设置了 500ms 超时限制，去请求 Gin 服务端接口； 其次，Gin 服务端收到请求之后，转而去调用 Kitex 服务；最后，Kitex 服务端 sleep 1s 模拟耗时超时，保证 Gin 客户端在请求过程中关闭连接。\n实测能够稳定地复现。\n我们梳理了源码逻辑，客户端关闭连接之后，Gin 读取到 EOF，调用 cancelCtx，被 Kitex 客户端的 rpcTimeoutMW 捕获到，于是返回了 err。\n那么问题就变成，请求未完成时，连接为何会被关闭？我们按照设备的 ID 去分析日志，发现两类情况：一类是报错对应的请求是该设备短期内的最后一条，于是考虑 APP 被手动关闭； 二是报错对应的请求非短期内的最后一条，客户端研发反馈，有些接口例如搜索，上一条请求执行中（未返回），且新的请求来时，会 Close 掉上一次请求的连接。 第二种情况比较确定，关于第一种情况，APP 被关闭时，IOS 和 Android 是否会关闭连接？客户端同学没有给出肯定的答复。\n于是我们考虑实际测试一下，两端分别写一个测试的应用，持续发起请求，但是不释放连接，此时关闭 APP，分析 TCP 包。实测我们在两端上均看到了 4 次挥手的 Fin 包。所以这个问题得到了确认。\n那么如何进行修复呢？我们采取在 GIN 的中间件上拦截掉 Done 方法的方式。\n上线之后，再没有出现这种情况。\n还有一个问题，我们在测试环境发现，跨集群调用的时候，经常出现连接被重置的问题。生产环境搜日志，无此现象。\n我们分析了环境差异：\n 生产环境是专线直连； 测试环境，因为专线比较昂贵，机房之前通过公网访问，中间有个 NAT 设备。  我们找网络同事咨询，得知 NAT 表项的过期时间是 60s。连接过期时，NAT 设备并不会通知上下游。因此，上游调用的时候，如果 NAT 设备发现表项不存在，会认为是一个失效的连接，就返回了 rst。 于是我们的解决方案是 Kitex 上游的 MaxIdleTimeout 改成 30s。实测再未出现报错。\n本章小结如下：\n Rpc Timeout：Tontext Tanceled 问题分析和解决； Rpc Error：Connection Reset 问题分析和解决。  展望 未来我们计划把 Gin 更换为更高性能（QPS/时延）的 CloudWeGo-Hertz。因为我们 K 线服务的 Response Size 比较大（~202KiB），更换后 QPS 预计可达原先的 5 倍。 同时，为回馈开源社区，我们打算贡献 Tracing 基础库的代码到 Kitex-contrib/Tracer-opentracing。欢迎持续关注 CloudWeGo 项目，加入社区一起交流。\n","categories":"","description":"","excerpt":"案例介绍   企业用户如何搭建针对 Kitex 的可观测性系统？如何在 K8s 集群下使用 Kitex ?\n本文将从以下 4 个方面介绍华兴 …","ref":"/cooperation/huaxingsec/","tags":"","title":"华兴证券：混合云原生架构下的 Kitex 实践"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/releases/kitex/","tags":"","title":"Kitex Release"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kitex/","tags":"","title":"Kitex"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/blog/releases/kitex/","tags":"","title":"Kitex Release"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kitex/","tags":"","title":"Kitex"},{"body":"Hertz provides the ability to extend the network library. If users need to replace with other network libraries, they can implement the corresponding interfaces according to their needs. Server needs to implement the network.Conn interface, Client needs to implement the network.Dialer interface.\nInterface Definition Interfaces in pkg/network/connection.go\ntype Conn interface { net.Conn Reader Writer SetReadTimeout(t time.Duration) error } // Reader is for buffered Reader type Reader interface { // Peek returns the next n bytes without advancing the reader.  Peek(n int) ([]byte, error) // Skip discards the next n bytes.  Skip(n int) error // Release the memory space occupied by all read slices. This method needs to be executed actively to  // recycle the memory after confirming that the previously read data is no longer in use.  // After invoking Release, the slices obtained by the method such as Peek will  // become an invalid address and cannot be used anymore.  Release() error // Len returns the total length of the readable data in the reader.  Len() int // ReadByte is used to read one byte with advancing the read pointer.  ReadByte() (byte, error) // ReadBinary is used to read next n byte with copy, and the read pointer will be advanced.  ReadBinary(n int) (p []byte, err error) } type Writer interface { // Malloc will provide a n bytes buffer to send data.  Malloc(n int) (buf []byte, err error) // WriteBinary will use the user buffer to flush.  // NOTE: Before flush successfully, the buffer b should be valid.  WriteBinary(b []byte) (n int, err error) // Flush will send data to the peer end.  Flush() error } For Client, you should implement the following interface in order to replace the Client-side network library.\ntype Dialer interface { DialConnection(network, address string, timeout time.Duration, tlsConfig *tls.Config) (conn Conn, err error) DialTimeout(network, address string, timeout time.Duration, tlsConfig *tls.Config) (conn net.Conn, err error) AddTLS(conn Conn, tlsConfig *tls.Config) (Conn, error) } Custom Network Library The Hertz Server and Client provide separate initialization configuration items\nServer\nserver.New(server.WithTransport(YOUR_TRANSPORT)) Client\nclient.NewClient(client.WithDialer(YOUR_DIALER)) ","categories":"","description":"","excerpt":"Hertz provides the ability to extend the network library. If users …","ref":"/docs/hertz/tutorials/framework-exten/advanced-exten/network-lib/","tags":"","title":"Network Library Extensions"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/news/","tags":"","title":"News"},{"body":"CloudWeGo-Hertz Hertz [həːts] is a high-performance, high-usability, extensible HTTP framework for Go. It’s designed to simplify building microservices for developers.\nHertz was inspired by other open-source frameworks like fasthttp, gin, and echo, in combination with unique challenges faced by ByteDance, Hertz has become production ready and powered ByteDance’s internal services over the years.\nNowadays, as Go gain popularity in microservice development, Hertz will be the right choice if you are looking for a customizable, high-performance framework to support a variety of use case.\nArchitecture Features High usability In modern software engineering, it is agreed that the delivery of high-quality code in a short period of time has become more important in a highly competitive environment. With that in mind, in the initial interactions of Hertz, we actively listen to users' feedback and endeavour to polish the framework to improve user experience and help developers to get the job done quickly and correctly.\n High performance Hertz uses Netpoll, a high-performance network library built from sketch, by default. In comparison to go net implementation, the benchmark indicates that in some scenarios, Hertz’s performance is better in terms of both QPS and time delay.  The following diagrams show how Hertz performs in comparison with other popular frameworks for echo requests. Please refer to https://github.com/cloudwego/hertz-benchmark for more details about benchmarking.\nHigh extensibility Hertz adopts a layered architecture to deliver a “batteries included” experience. Due to its layered design, the framework is highly extensible while its core functionality remains robust. Hertz comes with default implementations for many modules but also enables users to extend them to fit their own needs. At present, only stable capabilities have been made available to the open source community. More planning refers to RoadMap.\nMulti-protocol support Hertz framework provides out-of-box support for HTTP 1.1 and ALPN protocol. In addition, due to the layered design, Hertz supports the custom implementation of the protocol layer to adapt to different use cases.\nSwitching Network layer on demand Hertz has the ability to switch between network layer implementation ( Netpoll and Go Net ) on demand. Users can choose the network library that best fits their needs. Hertz also supports network layer extension in the form of plug-ins.\nPerformance Performance testing can only provide a relative reference. In production, there are many factors that can affect actual performance.\nWe provide the hertz-benchmark project to track and compare the performance of Hertz and other frameworks in different situations for reference.\nRelated Projects  Netpoll: A high-performance network library. Hertz uses it by default. Hertz-Contrib: A collection of Hertz extensions. Example: A repository to host examples for Hertz.  Blogs  ByteDance Practice on Go Network Library  ","categories":"","description":"","excerpt":"CloudWeGo-Hertz Hertz [həːts] is a high-performance, high-usability, …","ref":"/docs/hertz/overview/","tags":"","title":"Overview"},{"body":"CloudWeGo-Kitex Kitex [kaɪt’eks] is a high-performance and strong-extensibility Golang RPC framework that helps developers build microservices. If the performance and extensibility are the main concerns when you develop microservices, Kitex can be a good choice.\nArchitecture Basic Features  High Performance  Kitex integrates Netpoll, a high-performance network library, which offers significant performance advantage over go net.\n Extensibility  Kitex provides many interfaces with default implementation for users to customize. You can extend or inject them into Kitex to fulfill your needs (please refer to the framework extension section below).\n Multi-message Protocol  Kitex is designed to be extensible to support multiple RPC messaging protocols. The initial release contains support for Thrift, Kitex Protobuf and gRPC, in which Kitex Protobuf is a Kitex custom Protobuf messaging protocol with a protocol format similar to Thrift. Kitex also supports developers extending their own messaging protocols.\n Multi-transport Protocol  For service governance, Kitex supports TTHeader and HTTP2. TTHeader can be used in conjunction with Thrift and Kitex Protobuf; HTTP2 is currently mainly used with the gRPC protocol, and it will support Thrift in the future.\n Multi-message Type  Kitex supports PingPong, One-way, and Bidirectional Streaming. Among them, One-way currently only supports Thrift protocol, two-way Streaming only supports gRPC, and Kitex will support Thrift’s two-way Streaming in the future.\n Service Governance  Kitex integrates service governance modules such as service registry, service discovery, load balancing, circuit breaker, rate limiting, retry, monitoring, tracing, logging, diagnosis, etc. Most of these have been provided with default extensions, and users can choose to integrate.\n Code Generation  Kitex has built-in code generation tools that support generating Thrift, Protobuf, and scaffold code.\nPerformance We compared the performance of Kitex with some popular RPC frameworks (test code), such as gRPC and RPCX, both using Protobuf protocol. The test results show that Kitex performs better.\nNote: The performance benchmarks obtained from the experiment are for reference only, because there are many factors that can affect the actual performance in application scenarios.\nTest environment  CPU: Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz, 4 cores Memory: 8GB OS: Debian 5.4.56.bsk.1-amd64 x86_64 GNU/Linux Go: 1.15.4  Concurrency performance Change the concurrency with a fixed packet size 1KB.\n   QPS TP99 TP999           Throughput performance Change packet size with a fixed concurrency of 100.\n   QPS TP99 TP999           Related Projects  Netpoll: A high-performance network library. kitex-contrib: A partial extension library of Kitex, which users can integrate into Kitex through options according to their needs. Example: Use examples of Kitex.  Blogs  Performance Optimization Practice of Go RPC framework Kitex Practice of ByteDance on Go Network Library  ","categories":"","description":"","excerpt":"CloudWeGo-Kitex Kitex [kaɪt’eks] is a high-performance and …","ref":"/docs/kitex/overview/","tags":"","title":"Overview"},{"body":"Motore is an async middleware abstraction powered by GAT and TAIT.\nAround Motore, we build modular and reusable components for building robust networking clients and servers.\nMotore is greatly inspired by Tower.\nOverview Motore uses GAT and TAIT to reduce the mental burden of writing asynchronous code, especially to avoid the overhead of Box to make people less anxious.\nThe core abstraciton of Motore is the Service trait:\npubtraitService\u003cCx,Request\u003e{/// Responses given by the service. type Response;/// Errors produced by the service. type Error;/// The future response value. type Future\u003c'cx\u003e: Future\u003cOutput=Result\u003cSelf::Response,Self::Error\u003e\u003e+Send+'cxwhereCx: 'cx,Self: 'cx;/// Process the request and return the response asynchronously. fn call\u003c'cx,'s\u003e(\u0026'smutself,cx: \u0026'cxmutCx,req: Request)-\u003e Self::Future\u003c'cx\u003ewhere's: 'cx;}Getting Started Combing GAT and type_alias_impl_trait together, we can write asynchronous code in a very concise and readable way.\npubstruct Timeout\u003cS\u003e{inner: S,duration: Duration,}impl\u003cCx,Req,S\u003eService\u003cCx,Req\u003eforTimeout\u003cS\u003ewhereReq: 'static+Send,S: Service\u003cCx,Req\u003e+'static+Send,Cx: 'static+Send,S::Error: Send +Sync+Into\u003cBoxError\u003e,{type Response=S::Response;type Error=BoxError;type Future\u003c'cx\u003e=implFuture\u003cOutput=Result\u003cS::Response,Self::Error\u003e\u003e+'cx;fn call\u003c'cx,'s\u003e(\u0026'smutself,cx: \u0026'cxmutCx,req: Req)-\u003e Self::Future\u003c'cx\u003ewhere's: 'cx,{asyncmove{letsleep=tokio::time::sleep(self.duration);tokio::select!{r=self.inner.call(cx,req)=\u003e{r.map_err(Into::into)},_=sleep=\u003eErr(std::io::Error::new(std::io::ErrorKind::TimedOut,\"service time out\").into()),}}}}We also provided the #[motore::service] macro to make writing a Service more async-native:\nusemotore::service;pubstruct S\u003cI\u003e{inner: I,}#[service]impl\u003cCx,Req,I\u003eService\u003cCx,Req\u003eforS\u003cI\u003ewhereReq: Send +'static,I: Service\u003cCx,Req\u003e+Send+'static,Cx: Send +'static,{asyncfn call(\u0026mutself,cx: \u0026mutCx,req: Req)-\u003e Result\u003cI::Response,I::Error\u003e{self.inner.call(cx,req).await}}","categories":"","description":"","excerpt":"Motore is an async middleware abstraction powered by GAT and TAIT. …","ref":"/docs/motore/overview/","tags":"","title":"Overview"},{"body":"Introduction Netpoll is a high-performance non-blocking I/O networking framework, which focused on RPC scenarios, developed by ByteDance.\nRPC is usually heavy on processing logic and therefore cannot handle I/O serially. But Go’s standard library net is designed for blocking I/O APIs, so that the RPC framework can only follow the One Conn One Goroutine design. It will waste a lot of cost for context switching, due to a large number of goroutines under high concurrency. Besides, net.Conn has no API to check Alive, so it is difficult to make an efficient connection pool for RPC framework, because there may be a large number of failed connections in the pool.\nOn the other hand, the open source community currently lacks Go network libraries that focus on RPC scenarios. Similar repositories such as: evio, gnet, etc., are all focus on scenarios like Redis, HAProxy.\nBut now, Netpoll was born and solved the above problems. It draws inspiration from the design of evio and netty, has excellent Performance, and is more suitable for microservice architecture. Also Netpoll provides a number of Features, and it is recommended to replace net in some RPC scenarios.\nWe developed the RPC framework Kitex and HTTP framework Hertz (coming soon) based on Netpoll, both with industry-leading performance.\nExamples show how to build RPC client and server using Netpoll.\nFeatures   Already\n LinkBuffer provides nocopy API for streaming reading and writing gopool provides high-performance goroutine pool mcache provides efficient memory reuse IsActive supports checking whether the connection is alive Dialer supports building clients EventLoop supports building a server TCP, Unix Domain Socket Linux, macOS (operating system)    Future\n multisyscall supports batch system calls io_uring Shared Memory IPC Serial scheduling I/O, suitable for pure computing TLS UDP    Unsupported\n Windows (operating system)    Performance Benchmark should meet the requirements of industrial use. In the RPC scenario, concurrency and timeout are necessary support items.\nWe provide the netpoll-benchmark project to track and compare the performance of Netpoll and other frameworks under different conditions for reference.\nMore benchmarks reference kitex-benchmark and hertz-benchmark (open source soon).\nReference  Official Website Getting Started  ","categories":"","description":"","excerpt":"Introduction Netpoll is a high-performance non-blocking I/O networking …","ref":"/docs/netpoll/overview/","tags":"","title":"Overview"},{"body":"Volo Volo is a high-performance and strong-extensibility Rust RPC framework that helps developers build microservices.\nVolo uses Motore as its middleware abstraction, which is powered by GAT.\nArchitecture Features Powered by GAT Volo uses Motore as its middleware abstraction, which is powered by GAT.\nThrough GAT, we can avoid many unnecessary Box memory allocations, improve ease of use, and provide users with a more friendly programming interface and a more ergonomic programming paradigm.\nHigh Performance Rust is known for its high performance and safety. We always take high performance as our goal in the design and implementation process, reduce the overhead of each place as much as possible, and improve the performance of each implementation.\nFirst of all, it is very unfair to compare the performance with the Go framework, so we will not focus on comparing the performance of Volo and Kitex, and the data we give can only be used as a reference, I hope everyone can view it objectively; at the same time, due to the open source community has not found another mature Rust async version Thrift RPC framework, and performance comparison is always easy to lead to war, so we hope to weaken the comparison of performance data as much as possible, and we’ll only publish our own QPS data.\nUnder the same test conditions as Kitex (limited to 4C), the Volo QPS is 35W; at the same time, we are internally verifying the version based on Monoio (CloudWeGo’s open source Rust async runtime), and the QPS can reach 44W.\nFrom the flame graph of our online business, thanks to Rust’s static distribution and excellent compilation optimization, the overhead of the framework part is basically negligible (excluding syscall overhead).\nEasy to Use Rust is known for being hard to learn and hard to use, and we want to make it as easy as possible for users to use the Volo framework and write microservices in the Rust language, providing the most ergonomic and intuitive coding experience possible. Therefore, we make ease of use one of our most important goals.\nFor example, we provide the volo command line tool for bootstraping projects and managing idl files; at the same time, we split thrift and gRPC into two independent(but share some components) frameworks to provide programming paradigms that best conform to different protocol semantics and interface.\nWe also provide the #[service] macro (which can be understood as the async_trait that does not require Box) to enable users to write service middleware using async rust without psychological burden.\nStrong Extensibility Benefiting from Rust’s powerful expression and abstraction capabilities, through the flexible middleware Service abstraction, developers can process RPC meta-information, requests and responses in a very unified form.\nFor example, service governance functions such as service discovery and load balancing can be implemented in the form of services without the need to implement Trait independently.\nWe have also created an organization Volo-rs, any contributions are welcome.\nRelated Projects  Volo-rs：The volo ecosystem which contains a lot of useful components. Pilota：A thrift and protobuf implementation in pure rust with high performance and extensibility. Motore：Middleware abstraction layer powered by GAT. Metainfo：Transmissing metainfo across components.  ","categories":"","description":"","excerpt":"Volo Volo is a high-performance and strong-extensibility Rust RPC …","ref":"/docs/volo/overview/","tags":"","title":"Overview"},{"body":" search    All  Low  Mid  High  Danger     year All 2021 2022 2023 2024 2025       Bulletin Digest Level Influence Publish      Previous  1 2  3 Next   Total: 3    ","categories":"","description":"","excerpt":" search    All  Low  Mid  High  Danger     year All 2021 2022 2023 …","ref":"/security/safety-bulletin/","tags":"","title":"safety-bulletin"},{"body":"Referring to the Thrift THeader protocol, we designed the TTheader protocol.\nHeader Format 0 1 2 3 4 5 6 7 8 9 a b c d e f 0 1 2 3 4 5 6 7 8 9 a b c d e f +----------------------------------------------------------------+ | 0| LENGTH | +----------------------------------------------------------------+ | 0| HEADER MAGIC | FLAGS | +----------------------------------------------------------------+ | SEQUENCE NUMBER | +----------------------------------------------------------------+ | 0| HEADER SIZE | ... +--------------------------------- Header is of variable size: (and starts at offset 14) +----------------------------------------------------------------+ | PROTOCOL ID |NUM TRANSFORMS . |TRANSFORM 0 ID (uint8)| +----------------------------------------------------------------+ | TRANSFORM 0 DATA ... +----------------------------------------------------------------+ | ... ... | +----------------------------------------------------------------+ | INFO 0 ID (uint8)| INFO 0 DATA ... +----------------------------------------------------------------+ | ... ... | +----------------------------------------------------------------+ | | | PAYLOAD | | | +----------------------------------------------------------------+  LENGTH: 32bits, including the byte size of the remaining part of the data packet, but does not include itself HEADER MAGIC: 16bits, value: 0x1000, it’s used to identify TTHeaderTransport FLAGS: 16bits, it’s a reserved field, not used yet, default value is 0x0000 SEQUENCE NUMBER: 32bits, it’s the seqId of the message packet, can be used for multiplexing, and ensure it is incremented within a single connection HEADER SIZE: 16bits, it’s equal to the number of bytes divided by 4 of the head length. The header length calculation starts from the 14th byte and continues until before PAYLOAD (Notice: The maximum length of the header is 64K) PROTOCOL ID: uint8 encoding, the values are:  ProtocolIDBinary = 0 ProtocolIDCompact = 2   NUM TRANSFORMS: uint8 encoding, the number of TRANSFORM TRANSFORM ID: uint8 encoding, refer the instructions below INFO ID: uint8 encoding, refer the instructions below PAYLOAD: message content  PADDING Header will be padded out to next 4-byte boundary with 0x00.\nTransform IDs Transform IDs represents the compression way, which is a reserved field and is not supported currently. The values are:\n ZLIB_TRANSFORM = 0x01: the corresponding data is empty, indicating that the data is compressed with zlib SNAPPY_TRANSFORM = 0x03: the corresponding data is empty, indicating that the data is compressed with snappy  Info IDs Info IDs is used to transmit some key/value pair information, the values are:\n INFO_KEYVALUE = 0x01: the corresponding data is a key/value pair, key and value are each composed of the length of uint16 plus no-training-null string, which is generally used to transmit some common meta information, such as tracingId INFO_INTKEYVALUE = 0x10: the corresponding data is a key/value pair, the key is uint16, the value is composed of the length of uint16 plus the no-trading-null string, which is generally used to transmit some internally customized meta information, some keys are required as request:  TRANSPORT_TYPE = 1 (value: framed/unframed) LOG_ID = 2 FROM_SERVICE = 3 FROM_CLUSTER = 4 (default) FROM_IDC = 5 TO_SERVICE = 6 TO_METHOD = 9   ACL_TOKEN_KEYVALUE = 0x11: the corresponding data is a key/value pair, key and value are each composed of the length of uint16 plus no-trading-null string, used to transmit ACL Token  ","categories":"","description":"","excerpt":"Referring to the Thrift THeader protocol, we designed the TTheader …","ref":"/docs/kitex/reference/transport_protocol_ttheader/","tags":"","title":"TTHeader"},{"body":"参考 Thrift THeader 协议 ，我们设计了 TTheader 协议。\n协议编码  0 1 2 3 4 5 6 7 8 9 a b c d e f 0 1 2 3 4 5 6 7 8 9 a b c d e f +----------------------------------------------------------------+ | 0| LENGTH | +----------------------------------------------------------------+ | 0| HEADER MAGIC | FLAGS | +----------------------------------------------------------------+ | SEQUENCE NUMBER | +----------------------------------------------------------------+ | 0| HEADER SIZE | ... +--------------------------------- Header is of variable size: (and starts at offset 14) +----------------------------------------------------------------+ | PROTOCOL ID |NUM TRANSFORMS . |TRANSFORM 0 ID (uint8)| +----------------------------------------------------------------+ | TRANSFORM 0 DATA ... +----------------------------------------------------------------+ | ... ... | +----------------------------------------------------------------+ | INFO 0 ID (uint8)| INFO 0 DATA ... +----------------------------------------------------------------+ | ... ... | +----------------------------------------------------------------+ | | | PAYLOAD | | | +----------------------------------------------------------------+ 其中：\n LENGTH 字段 32bits，包括数据包剩余部分的字节大小，不包含 LENGTH 自身长度 HEADER MAGIC 字段 16bits，值为：0x1000，用于标识 TTHeaderTransport FLAGS 字段 16bits，为预留字段，暂未使用，默认值为 0x0000 SEQUENCE NUMBER 字段 32bits，表示数据包的 seqId，可用于多路复用，最好确保单个连接内递增 HEADER SIZE 字段 16bits，等于头部长度字节数 /4，头部长度计算从第 14 个字节开始计算，一直到 PAYLOAD 前（备注：header 的最大长度为 64K） PROTOCOL ID 字段 uint8 编码，取值有：  ProtocolIDBinary = 0 ProtocolIDCompact = 2   NUM TRANSFORMS 字段 uint8 编码，表示 TRANSFORM 个数 TRANSFORM ID 字段 uint8 编码，具体取值参考下文 INFO ID 字段 uint8 编码，具体取值参考下文 PAYLOAD 消息内容  PADDING 填充 Header 部分长度 bytes 数必须是 4 的倍数，不足部分用 0x00 填充\nTransform IDs 表示压缩方式，为预留字段，暂不支持，取值有：\n ZLIB_TRANSFORM = 0x01，对应的 data 为空，表示用 zlib 压缩数据； SNAPPY_TRANSFORM = 0x03，对应的 data 为空，表示用 snappy 压缩数据；  Info IDs 用于传递一些 kv 对信息，取值有：\n INFO_KEYVALUE = 0x01，对应的 data 为 key/value 对，key 和 value 各自都是由 uint16 的长度加上 no-trailing-null 的字符串组成，一般用于传递一些常见的 meta 信息，例如 tracingId； INFO_INTKEYVALUE = 0x10，对应的 data 为 key/value 对，key 为 uint16，value 由 uint16 的长度加上 no-trailing-null 的字符串组成，一般用于传递一些内部定制的 meta 信息，其中作为 request 有些 key 是必填的：  TRANSPORT_TYPE = 1（取值：framed/unframed） LOG_ID = 2 FROM_SERVICE = 3 FROM_CLUSTER = 4（默认 default） FROM_IDC = 5 TO_SERVICE = 6 TO_METHOD = 9   ACL_TOKEN_KEYVALUE = 0x11，对应的 data 为 key/value 对，key 和 value 各自都是由 uint16 的长度加上 no-trailing-null 的字符串组成，用于传递 ACL Token；  ","categories":"","description":"","excerpt":"参考 Thrift THeader 协议 ，我们设计了 TTheader 协议。\n协议编码  0 1 2 3 4 5 6 7 8 9 a b …","ref":"/zh/docs/kitex/reference/transport_protocol_ttheader/","tags":"","title":"TTHeader"},{"body":"Server 启动hertz  hello ：启动一个 hertz “hello world\"应用的示例  配置  config ：配置 hertz server 的示例  协议  Protocol ：hertz 使用 http1、tls 等协议的示例  路由  Route ：注册路由、使用路由组、参数路由的示例  中间件  CORS ：使用 CORS 中间件的示例 basic_auth ：使用 basic auth 中间件的示例 custom ：自定义中间件的示例  参数绑定及验证  binding ：参数绑定及验证的示例  获取参数  parameters ：获取 query、form、cookie 等参数的示例  文件  file ：文件上传、文件下载、静态文件服务的示例  Render  render ：render body 为 json、html、protobuf 等的示例  重定向  redirect ：重定向到内部/外部 URI 的示例  流式读/写  streaming ：使用 hertz server 流式读/写的示例  优雅退出  graceful_shutdown ：hertz server 优雅退出的示例  单元测试  unit_test ：使用 hertz 提供的接口不经过网络传输编写单元测试的示例  链路追踪  tracer ：hertz 使用 Jaeger 进行链路追踪的示例  监控  monitoring ：hertz 使用 Prometheus 进行指标监控的示例  Client 发送请求  send_request ：使用 hertz client 发送 http 请求的示例  配置  client_config ：配置 hertz client 的示例  TLS  tls ：hertz client 发送 tls 请求的示例  添加请求内容  add_parameters ：使用 hertz client 添加请求参数的示例  上传文件  upload_file ：使用 hertz client 上传文件的示例  中间件  middleware ：使用 hertz client middleware 的示例  流式读响应  streaming_read ：使用 hertz client 流式读响应的示例  正向代理  forward_proxy ：使用 hertz client 配置正向代理的示例  ","categories":"","description":"","excerpt":"Server 启动hertz  hello ：启动一个 hertz “hello world\"应用的示例  配置  config ： …","ref":"/zh/docs/hertz/tutorials/example/","tags":"","title":"代码示例"},{"body":"我们先来讲一下，我们为什么要 Context。\ntower 中的 Service 签名如下\npubtraitService\u003cRequest\u003e{type Response;type Error;type Future: Future\u003cOutput=Result\u003cSelf::Response,Self::Error\u003e\u003e;fn poll_ready(\u0026mutself,cx: \u0026mutContext\u003c'_\u003e)-\u003e Poll\u003cResult\u003c(),Self::Error\u003e\u003e;fn call(\u0026mutself,req: Request)-\u003e Self::Future;}在 call 方法中并没有 Context 这个概念，那么为什么我们在 Motore 中引入这个概念呢？\n实现一个 Log 中间件 假设我们的需求是在 call 成功或者失败的时候打印 LogId，首先我们需要考虑 LogId 存放在哪儿？\n我们想要使用 Context 来存放所有与该请求上下文有关的信息，那么 LogId 应该可以存放到 Context 中。\npubstruct Context{log_id: String,}按照 tower 的设计，那么 Context 应该可以被放到 Request 中。\n那么我们可以这么来实现我们的中间件。\npubstruct VoloRequest\u003cReq\u003e{cx: Context,data: Req,}pubstruct LogService\u003cS\u003e{inner: S,}impl\u003cReq,S\u003eService\u003cVoloRequest\u003cReq\u003e\u003eforLogService\u003cS\u003e{// 这里省略 poll_ready 实现 fn call(\u0026mutself,req: VoloRequest\u003cReq\u003e)-\u003e Self::Future{async{letlog_id=req.cx.log_id.clone();letresp=self.inner.call(req).await;matchresp{Ok(_)=\u003e{tracing::info(\"log id: {}\",log_id);},Err(_)=\u003e{tracing::error(\"log id: {}\",log_id);},}resp}}}因为我们需要把 VoloRequest 向之后的 Service 传递所有权，那么这里我们就需要把 log_id clone 一下。\n这里的 clone 会有潜在的开销，这个时候我们可以把 log_id 的类型从 String 改为 Arc\u003cString\u003e 来降低开销。\n但是我们真的需要 clone 嘛？\n我们这里打印 log_id 的这个需求其实只需要使用 Context 的引用。并且我们期望 Context 结构的生命周期在整个请求执行阶段都是有效的， 在 inner service 执行完之后，我们仍然可以访问 Context 中的数据。\n所以我们尝试在 Request 之外引入 Context 这个概念，并且让 call 方法可以使用 \u0026mut Context。\n这样我们可以使用这种方式来实现我们的 LogService:\nimpl\u003cReq,S\u003eService\u003cReq\u003eforLogService\u003cS\u003e{// 这里省略 poll_ready 实现 fn call(\u0026mutself,cx: \u0026mutContext,req: Req)-\u003e Self::Future{async{letresp=self.inner.call(cx,req).await;matchresp{Ok(_)=\u003e{tracing::info(\"log id: {}\",cx.log_id);},Err(_)=\u003e{tracing::error(\"log id: {}\",cx.log_id);},}resp}}}Context 的生命周期 Service 使用的是 \u0026mut Context，那么这个引用的生命周期应该是什么呢？\nContext 里面存放的是一个请求的上下文，那么 Context 的生命周期应该是请求级别的。所以我们引入了生命周期 'cx。\n那么 Service call 方法的 \u0026mut self 的生命周期又应该是什么呢？我们的 Rpc Server 是由一个又一个 Service 组成的。那么 Service 的生命周期其实应该是我们的 Rpc Server 的生命周期。\n因为 call 方法返回的 Future 中可能会依赖 Context 中的数据，那么 Future 的 lifetime 至少也应该是 'cx。\n所以最后我们的 call 方法的签名就会变成:\nfn call\u003c'cx,'s\u003e(\u0026'smutself,cx: \u0026'cxmutContext,req: Request)-\u003e Future\u003c'cx\u003e;那么我们该怎么约束返回的 Future 的 lifetime 为 'cx 呢\n 使用 Pin\u003cBox\u003cdyn Future\u003cOutput = Result\u003cSelf::Response, Self::Error\u003e\u003e + 'cx\u003e\u003e GAT  如果我们使用方案1的话就会存在 overhead，不可避免的需要一大堆 Box::pin。\n因此我们这里选择直接使用 GAT（GAT 马上也会 stable）\n","categories":"","description":"","excerpt":"我们先来讲一下，我们为什么要 Context。\ntower 中的 Service …","ref":"/zh/docs/motore/faq/q1_gat/","tags":"","title":"使用 GAT，解决了什么问题？"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/security/safety-bulletin/detail/","tags":"","title":"公告详情"},{"body":"案例介绍   本文将从以下 4 个方面介绍华兴证券基于 Kitex 在多机房 K8s 集群下的实践经验，包括：\n 针对 Kitex 的可观测性系统搭建经验； 服务压力测试中遇到的问题以及解决方案； Kitex 的不同连接类型在 K8s 同集群/跨集群调用下的一些问题和解决方案； 实践中遇到的其他问题以及解决方案；  Kitex 的可观性系统搭建 华兴证券 CloudWeGo-Kitex 使用情况 去年 6 月 1 日，华兴证券相关研发团队成立。Kitex 在 7 月 12 日发布了首个版本，10 天后就引入了 Kitex。\n选择 Kitex 的原因是：团队早期成员比较了解 Kitex，为了快速支撑业务迭代和验证，选择最熟悉的框架，不但使用上比较习惯，对性能和功能方面也比较有把握。\n后来也支撑了华兴证券 APP 的快速上线，大约 4 个月之后就上线了 APP 的第一个版本。\n下图是业务的微服务调用关系图，一共有三十多个微服务，调用链路数超过 70。服务分别部署在两个机房。核心业务比如交易、行情等部署在私有机房。 非核心的业务，比如资讯、股票信息等部署在阿里的金融云，这样能够更好地利用金融云已有的基础设施比如 MySQL、Kafka 等，作为初创团队，能够降低整体的运维压力。 考虑到性能以及安全方面的因素，两个机房之间专门拉了专线。服务之间存在一些跨机房的依赖。跨机房调用会产生很多问题，后文会详细说明。\nTracing 选型 服务数多了之后，我们需要一套链路追踪系统来描绘调用链路每个环节的耗时情况。考虑到 Kitex 原生支持 Opentracing，为减少集成成本，我们调研了符合 Opentracing 规范的产品。\n排除掉收费的、客户端不支持 Go 之后，就剩阿里云的链路追踪产品和 Uber 公司出品的 Jaeger，考虑到私有机房也要部署，最终选择了 Jaeger。\nKitex 接入 Tracing 选定方案之后，开始对 Kitex 的这个功能进行测试，结果发现当时去年 9 月初的 Kitex 版本并不支持跨服务的 Tracing，原因是调用的时候，没有把 Trace 信息发送给下游，如图所示， 这样上下游是两个孤立的 Trace（OpenTracing 规范里称为 Span），于是就无法通过一个 TraceID 去串起整条链路。当时任务比较急，于是我们没有等 Kitex 官方的实现，决定自研。\n为了自研，我们结合 Kitex 的源码，梳理出客户端和服务端的流程。可以看出 Kitex 的上下游都内置了 Tracer 的 Hook。这里我们要解决的问题是，如何把 Span 信息进行跨服务传输？\n经调研，实现透传有三种方案。\n第一种是在消息层搞一个 Thrift 协议的拓展，把 Trace 信息塞进去。原因是 Thrift 本身没有 Header 结构，只能进行协议的拓展。好在 Kitex 支持自定义的协议拓展，因此具备可行性，然而开发成本较高，所以没选择这种方案。\n第二种是在 IDL 里增加通用参数，在字段里存 Trace 信息。缺点是业务无关的字段要在 IDL 里，对性能有一定的影响。毕竟需要通过 Kitex 的中间件，通过反射来提取。\n第三种是利用了 Kitex 提供的传输层透传能力，对业务没有侵入性。最后选择了这一种方案。\n透传方案定了之后，整体的流程就清晰了。首先客户端会在 metaHandler.write 里通过 CTX 获取当前 Span，提取并写入 spanContext 到 TransInfo 中。\n然后服务端，在 metaHandler.Read 里读取 spanContext 并创建 ChildOf 关系的 Span，中间件结束时 span.finish()，最后为了防止产生孤立 Trace，New 服务端时不使用 Kitex 提供的 Tracing 的 Option。\n这里是因为同一个服务可能分别作为 Kitex 上下游，Tracer 如果共用，需要分别加特殊逻辑，实现上有点复杂。\nTracing 基础库 为了充分利用 Tracing 的能力，除了 Kitex，我们在基础库中也增加了 Gin、Gorm、Redis、Kafka 等组件的 Tracing。\n下面展示实际的一条链路。功能是通过短信验证码进行登录。先是作为 HTTP 服务的 API 入口，然后调用了一个短信的 RPC 服务，RPC 服务里面通过 Redis 来检查验证码。 通过之后调用用户服务，里面可能进行一些增加用户的 MySQL 操作。最后把用户登录事件发给 Kafka，然后运营平台进行消费，驱动一些营销活动。可以看出最耗时的部分是关于新增用户的一堆 MySQL 操作。\n对错误的监控 Tracing 一般只关注调用耗时，然而一条链路中可能出现各种错误：\n Kitex   Kitex RPC 返回的 err（Conn Timeout、Read Timeout 等）； IDL 里自定义的业务 Code（111: 用户不存在）。  2.HTTP\n 返回的 HTTP 状态码（404、503）； JSON 里的业务 Code（-1: 内部错误）。  如何对这类错误进行监控？主要有以下三种方案：\n  打日志 + 日志监控，然后通过监控组件，这种方案需要解析日志，所以不方便；\n  写个中间件上报到自定义指标收集服务，这种方案优点是足够通用，但是需要新增中间件。同时自定义指标更关注具体的业务指标；\n  利用 Tracing 的 Tag，这种方案通用且集成成本低。\n  具体实现如下：\n Kitex 的 err、以及 HTTP 的状态码，定义为系统码； IDL 里的 Code 以及 HTTP 返回的 JSON 里的 Code，定义成业务码； Tracing 基础库里提取相应的值，设置到 span.tag 里； Jaeger 的 tag-as-field 配置里加上相应的字段（原始的 Tags，为 es 里的 Nested 对象，无法在 Grafana 里使用 Group By）。  监控告警 在增加错误监控的基础上，我们构建了一套监控告警系统体系。\n这里重点看一下刚才的链路追踪相关的内容。首先每个业务容器会把指标发送到 Jaeger 服务里。Jaeger 最终把数据落盘到 es 中。然后我们在 Grafana 上配置了一堆看板以及对应的告警规则。\n触发报警时，最终会发送到我们自研的 alert-webhook 里。\n自研的部分首先进行告警内容的解析，提取服务名等信息，然后根据服务的业务分类，分发到不同的飞书群里，级别高的报警会打加急电话。这里也是用到了飞书的功能。\nGrafana 里我们配置了各类型服务调用耗时、错误码一体化看板，描述了一个服务的方方面面的指标。包括日志监控、错误码监控、QPS 和调用耗时、容器事件监控、容器资源监控等。\n下图展示了飞书告警卡片。包括 RPC 调用超时、系统码错误、业务码错误。\n这里我们做了两个简单的工作，一个是带上了 TraceID，方便查询链路情况。另一个是把业务码对应的含义也展示出来，研发收到报警之后就不用再去查表了。\n本章小结\n 完成了 Tracing 接入 Kitex，实现跨服务传递； 对 Tracing 基础库扩展了其他类型中间件（Gin、Gorm、Redis、Kafka）的支持； 对 Tracing 基础库增加了系统码、错误码实现对错误的监控； 配置了全方位的服务指标看板； 结合 es、Grafana、飞书以及自研告警服务，搭建了针对微服务的监控告警系统。  这样我们就完成了可观测性体系的搭建。\n服务压力测试中遇到的问题以及解决方案 完成了监控告警体系之后，我们希望对服务进行压测，来找出性能瓶颈。第二部分介绍一下服务压测中遇到的问题和解决方案。\nKitex v0.0.8：连接超时问题 首先我们发现，QPS=150 左右，Kitex 出现连接建立超时的错误。当时我们检查了下 CPU、网络、内存等均没有达到限制。先是怀疑连接池大小不太够，于是测了下 10 和 1000，如上图所示，结果在报错数目上没有区别。 另外观察到的一个现象是，压测期间出现接近 5000 的 Time Wait 状态。\n5000 的限制，是因为达到了 tcp_max_tw_buckets 的设置的值。超过这个值之后，新的处于 Time Wait 状态的连接会被销毁，这样最大值就保持在 5000 了。 于是我们尝试进行排查，但没有思路，于是去翻看 Kitex 的 Issue，发现有人遇到相同的问题。\n原来，v0.0.8 版本的 Kitex，在使用域名的方式来新建 Client 的时候，会导致连接池失效。因为把连接放回连接池时，用的 Key 是解析之后的 IP，而 GET 的时候，用的是解析前的域名，这样根本 Get 不到连接，于是不停创建短连接。 这样的两个后果是：建立连接比较耗时，另一方面请求执行完毕之后都会关闭掉连接，于是导致了大量的 Time Wait。\n为了进行验证，我把测试服务改成了 IP 访问，然后比较了 IP 访问和域名访问以及不同连接池大小的情况。可以看出：IP 访问（连接池有效），但是连接池比较小的情况，出现减少的 Timeout。 连接池 100，Timeout 消失。而中间的域名访问的情况下，出现大量 Timeout。\nKitex v0.1.3：连接池问题修复 看代码得知在 Kitex v0.1.3 修复了这个问题。\n于是我们打算升级 Kitex 的版本，因为当时已经上了生产环境，在升级基础组件之前，需要进行验证，看一下不同连接池大小状态下的表现。还是域名模式，QPS 为 150 的情况下，随着连接池大小的增加，Timeout 的情况逐渐变少到消失。\n继续进行压测，我们发现 QPS=2000 的时候又出现了报错。结合监控，发现原因是连接建立的时候超过了默认的 50ms。\n我们讨论了几种解决方案：\n 修改超时配置。然而，交易日的 9:30-9:35 有⼀堆集中交易请求，突发的流量，耗时长了体验不好，可能会影响 APP 收入，我们希望系统性能保持稳定。 进行连接耗时的优化。然而 Kitex 已经使用了 Epoll 来处理创建连接的事件，作为使用方，进一步优化的难度和成本都太大。 MaxidleTimeout 参数改成无限大？比如先创建一个足够大的池，然后随着用户请求，池变得越来越大，最终稳定下来。但是每次服务升级之后，这个池就空了，需要慢慢恢复。 进行连接预热。  其实连接预热就相当于压测结束之后立马趁热再压一次，如图，可以发现 QPS=2000 的情况下，几乎都走了连接池，没有报错。因此，如果服务启动时能够进行连接预热，就可以省下建立连接的时间，使服务的性能保持稳定。\n当时 CloudWeGo 团队针对我们公司建了企业用户交流群，于是我们就向群里的 Kitex 研发提了连接预热的需求。其开发之后提供了连接预热个数的选项。我们也进行了测试。按照 QPS=2000 进行测试，\n WARM_UP_CONN_NUM=0：大约 1s 报错； WARM_UP_CONN_NUM=100：大约 4s 报错；   WARM_UP_CONN_NUM=1000：大约 4s 报错，但可以看出一开始都无需新建连接； WARM_UP_CONN_NUM=2000：无报错。  本章小结如下：\n Kitex v0.0.8：域名模式下存在连接池失效问题，v0.1.3 中修复； Kitex v0.1.3：可进一步通过连接预热功能提高系统性能。  Kitex 的不同连接类型在 K8s 同集群/跨集群调用下的一些问题和解决方案 长连接的问题：跨集群调用 第三部分我们讨论一下 Kitex 的不同连接类型在 K8s 同集群/跨集群调用下的一些问题和解决方案。\n首先是长连接跨集群调用下的问题。服务在跨集群调用时，其源 IP: 端口为宿主机的，数量有限，而目的 IP: 端口为下游集群的 LB，一般是固定的。\n那么，当长连接池数目比较大（比如数千），且上游较多（各种服务、每个都多副本，加起来可能数十个）的情况下，请求高峰时段可能导致上游宿主机的源端口不够用。同集群内跨机器调用走了 vxlan，因此没有这个问题。\n解决方案有两类：\n 硬件方案：机器； 软件方案：对于下游为 Kitex 服务，改用 Mux 模式（这样少量连接就可以处理大量并发的请求）。下游不是 Kitex 框架，因为 Mux 是私有协议，不支持非 Kitex。此时可考虑增加下游服务的 LB 数量，比如每个 LB 上分配多个端口。  比较起来，改造成 Mux 模式成本最低。\n连接多路复用的问题：滚动升级 但是多路复用模式，在 K8s 场景下，存在一个滚动升级相关的问题。我们先介绍下 Service 模式， K8s 的 Service 模式采用了 IPVS 的 Nat 模式（DR 和隧道模式不支持端口映射），链路为：\n上游容器←→ClusterIP（服务的虚拟 IP）←→下游容器\n然后我们看看滚动升级流程：\n 新容器启动。 新容器 Readiness Check 通过，之后做两件事情：  更新 Endpoints 列表：新增新容器，删除旧容器； 发送 sigTerm 到旧容器的 1 号进程。   由于更新了 Endpoints 列表，Endpoints 列表发生更新事件，立即回调触发规则更新逻辑（syncProxyRules）：  添加新容器到 IPVS 的 rs，权重为 1； 如果此时 IPVS 的旧容器的中 ActiveConn + InactiveConn \u003e 0（即已有连接还在），旧容器的权重会改成 0，但不会删除 rs。    经过步骤 3 之后，已有的连接仍然能够正常工作（因为旧容器 rs 未删），但新建的连接会走到新的容器上（因为旧容器权重 =0）。\n在 Service 模式下，上游通过一个固定的 IP: 端口来访问下游，当下游滚动升级的时候，上游看到的地址并未变化，即无法感知到滚动升级。于是，下游即使有优雅退出，但上游并不知道下游开始优雅退出了。之后可能的情况是：\n 下游发现连接繁忙，一直没有主动关闭，导致 K8s 配置的优雅升级时间超时，强制 Kill 进程，连接关闭，上游报错。 下游发现连接空闲，主动关闭，然而客户端在关闭之前恰好拿到了连接（且认为可用），然后发起请求，实际上由于连接关闭，发起请求失败报错。  针对此问题，解决方案如下：\n 同集群调用：改用 Headless Service 模式（结合 DNSResolver）：通过 DNS 列表的增删来感知下游变动； 跨集群调用：借鉴 HTTP2 的 GOAWAY 机制。  具体，可采用如下方式：\n 收到 sigTerm 的下游直接告诉上游（通过之前建立的 Conn1），同时下游继续处理发来的请求。 上游收到关闭信息之后：  新请求通过新建 Conn2 来发； 已有的请求仍然通过 Conn1，且处理完了之后，等下游优雅关闭 Conn1。    这种方式的优点是同集群跨集群均可使用，缺点是需要 Kitex 框架支持。在我们找 Kitex 团队讨论之后，他们也提供了排期支持本需求。\n连接多路复用的滚动升级测试：Headless Service 模式 在 Kitex 团队开发期间，我们测下 Kitex 已有版本对 Headless Service 模式下的滚动升级功能。\n测试方案如下：\n Kitex 版本 v0.1.3； 上下游均为 Mux 模式； 上游的加了个自定义 DNSResolver，刷新时间为 1s，加日志打印解析结果； 下游的退出信号处理，收到 sigTerm 之后特意 Sleep 10s（用来排除这个 Case：服务端发现连接空闲关闭了，但客户端在关闭之前恰好拿到连接，接着认为未关闭，实际上已经关闭，而客户端发起了请求，于是导致报错）； QPS=100 恒定压上游，然后触发下游滚动升级。  实测报错如下图：\n时序分析如下：\n 旧下游收到 sigTerm，开始 Sleep 10s； 上游解析到旧下游的 IP，向旧下游发起请求； DNS 规则更新：旧上游 IP 解析项消失，新下游解析项出现； 上游请求报错； 旧下游sleep完成，开始退出逻辑。  可见报错时旧下游还未执行退出逻辑，排除旧下游主动关闭连接。请求旧下游期间，且此时解析到新容器 IP（移除了旧容器 IP），报错是因为还没到退出逻辑的时候。因此推测，解析条目变化导致了报错。\n根据推测，结合代码（Kitex 客户端部分）分析，可能出现以下并发问题：\n 【协程1】客户端从 Mux池里取出 conn1，即将发起请求（所以没有机会再检查 conn1 状态了）； 【协程2】DNS 更新，移除了 IP，于是 Clean 方法中关闭了 conn1； 【协程1】客户端用 conn1 发起请求，导致报错 conn closed。  于是我们向 CloudWeGo 提了 Issue，他们很快修复了这个问题。\n连接多路复用的滚动升级测试：Service 模式 同样地，在 Service 模式中，测试方案如下：\n Kitex 版本使用 Feature 分支：mux-graceful-shutdown； 上下游均为 Mux 模式、服务发现使用 Service 模式； 恒定 QPS=200 压上游，20s 触发下游滚动升级； 另外写个服务打印期间的 IPVS 的日志； 下游的退出信号处理，收到 SigTerm 之后特意 sleep 10s（保证 ipvs 规则已更新）。  测试结果如下： 报错：INFO[0050] “{\"code\":-1,\"message\":\"remote or network error: conn closed\"}\"。\n时序分析为：\n 旧下游收到 sigTerm，开始 sleep 10s。 IPVS规则变化：  新下游 weight=1，ac=0，inac=0； 旧下游 weight=0，ac=2，inac=0。   旧下游 sleep 完成，进入最长为 15s（WithExitWaitTime）的优雅退出。 上游请求报错。 旧下游打印了最后一条日志。 IPVS 规则变化：  新下游 weight=1，ac=2，inac=0 =\u003e ac=2 说明上游新建连接到新容器； 旧下游 weight=0，ac=0，inac=2 =\u003e inac=2 表示连接关闭。   IPVS 规则变化：旧下游的规则被移除。  因此我们得出结论，报错发生在优雅退出期间。最后一条日志时刻大于报错时刻，因此，排除 K8s 的问题，确认 Conn Closed 是由 Kitex 导致的。 之后我们和 Kitex 研发团队沟通了分析结果，找到了 Root Cause，是因为假设了新的下游会有一个新的地址（但实际中 Service 模式都是一个地址），导致新请求取到了老请求的连接并进行关闭。对此进行了修复：\n连接多路复用的问题：下游扩容 如果⽤ Service 模式（上游看到的下游就是体现为⼀个 IP），创建的 TCP 连接会在最开始固定的几个下游 POD 上，之后如果扩容增加 POD，新创建的 POD 就不会路由到了，导致扩容实际上无效。\n解决方案如下：\n 同集群调用：可用 Headless Service 模式，由于 DNS 解析能够得到所有 POD，路由没问题。 跨集群调用：不在同集群内， Headless Service 模式无效，考虑如下方案：   方案1：修改服务发现机制。 优点：Kitex 无需改动。 缺点：增加依赖项（服务发现组件）。 方案2：下游先升级，之后上游 Redeploy 一下，让连接分布到下游的各种实例上。 优点：Kitex 无需改动。 缺点：上游可能很多，逐个 Redeploy 非常不优雅。 方案3：上游定期把 Mux 给过期掉，然后新建连接。 优点：彻底解决。 缺点：需要 Kitex 支持。  本章小结如下：\n 首先，针对长连接模式分析了跨集群时上游源端口数问题，希望通过多路复用模式解决； 其次，针对多路复用模式 + K8s Headless Service 模式的优雅升级，实测报错，分析定位了原因，Kitex 研发团队及时解决了相应问题； 再次，针对多路复用模式 + K8s Service 模式下的优雅升级提出了方案，Kitex 团队完成了实现，迭代了一轮，测试通过； 最后，针对多路复用模式 + K8s Service 模式下的下游副本扩容时路由不到的问题分析了原因，提出了方案，目前方案待实现。  实践中遇到的其他问题以及解决方案 RPC Timeout Context Canceled 错误 第四部分我们分析下实践中遇到的其他问题以及解决方案。研发同学发现日志出现 contexe canceled 的错误，分析日志发现出现频率低，一天只有几十条，属于偶发报错。\n我们推测是用户手机因为某种原因关闭了进行中的连接所导致，对此进行本地验证。三个部分：首先Gin 客户端设置了 500ms 超时限制，去请求 Gin 服务端接口； 其次，Gin 服务端收到请求之后，转而去调用 Kitex 服务；最后，Kitex 服务端 sleep 1s 模拟耗时超时，保证 Gin 客户端在请求过程中关闭连接。\n实测能够稳定地复现。\n我们梳理了源码逻辑，客户端关闭连接之后，Gin 读取到 EOF，调用 cancelCtx，被 Kitex 客户端的 rpcTimeoutMW 捕获到，于是返回了 err。\n那么问题就变成，请求未完成时，连接为何会被关闭？我们按照设备的 ID 去分析日志，发现两类情况：一类是报错对应的请求是该设备短期内的最后一条，于是考虑 APP 被手动关闭； 二是报错对应的请求非短期内的最后一条，客户端研发反馈，有些接口例如搜索，上一条请求执行中（未返回），且新的请求来时，会 Close 掉上一次请求的连接。 第二种情况比较确定，关于第一种情况，APP 被关闭时，IOS 和 Android 是否会关闭连接？客户端同学没有给出肯定的答复。\n于是我们考虑实际测试一下，两端分别写一个测试的应用，持续发起请求，但是不释放连接，此时关闭 APP，分析 TCP 包。实测我们在两端上均看到了 4 次挥手的 Fin 包。所以这个问题得到了确认。\n那么如何进行修复呢？我们采取在 GIN 的中间件上拦截掉 Done 方法的方式。\n上线之后，再没有出现这种情况。\n还有一个问题，我们在测试环境发现，跨集群调用的时候，经常出现连接被重置的问题。生产环境搜日志，无此现象。\n我们分析了环境差异：\n 生产环境是专线直连； 测试环境，因为专线比较昂贵，机房之前通过公网访问，中间有个 NAT 设备。  我们找网络同事咨询，得知 NAT 表项的过期时间是 60s。连接过期时，NAT 设备并不会通知上下游。因此，上游调用的时候，如果 NAT 设备发现表项不存在，会认为是一个失效的连接，就返回了 rst。 于是我们的解决方案是 Kitex 上游的 MaxIdleTimeout 改成 30s。实测再未出现报错。\n本章小结如下：\n Rpc Timeout：Tontext Tanceled 问题分析和解决； Rpc Error：Connection Reset 问题分析和解决。  展望 未来我们计划把 Gin 更换为更高性能（QPS/时延）的 CloudWeGo-Hertz。因为我们 K 线服务的 Response Size 比较大（~202KiB），更换后 QPS 预计可达原先的 5 倍。 同时，为回馈开源社区，我们打算贡献 Tracing 基础库的代码到 Kitex-contrib/Tracer-opentracing。欢迎持续关注 CloudWeGo 项目，加入社区一起交流。\n","categories":"","description":"","excerpt":"案例介绍   本文将从以下 4 个方面介绍华兴证券基于 Kitex 在多机房 K8s 集群下的实践经验，包括：\n 针对 Kitex 的可观测 …","ref":"/zh/cooperation/huaxingsec/","tags":"","title":"华兴证券：混合云原生架构下的 Kitex 实践"},{"body":"为了增加框架的灵活性和易用性，Volo 允许用户在 Client 端使用 CallOpt 针对单个请求设置一些请求的元信息。\n以 Volo-Thrift 为例，CallOpt 定义如下：\npubstruct CallOpt{/// Sets the callee tags for the call. pubcallee_tags: TypeMap,/// Sets the address for the call. pubaddress: Option\u003cAddress\u003e,pubconfig: Config,/// Sets the caller tags for the call. pubcaller_tags: TypeMap,}其中 callee_tags 指代的是对端的一些元信息，caller_tags 指代的是本地的元信息，这两个 TypeMap 主要是给服务发现、负载均衡、路由等中间件扩展使用的。\naddress 代表下游的地址，如果设置了，原则上就不需要经过服务发现和负载均衡等组件了。\nconfig 中可以设置一些请求的配置，比如 RPC 超时时间等。\n我们可以通过下述方法来在请求时指定 CallOpt：\nlazy_static!{staticrefCLIENT: volo_gen::volo::example::ItemServiceClient={letaddr: SocketAddr=\"127.0.0.1:8080\".parse().unwrap();volo_gen::volo::example::ItemServiceClientBuilder::new(\"volo-example-item\").layer_inner(LogLayer).address(addr).build()};}#[volo::main]asyncfn main(){letcallopt=CallOpt::default();letreq=volo_gen::volo::example::GetItemRequest{id: 1024};letresp=CLIENT.clone().with_callopt(callopt).get_item(req).await;matchresp{Ok(info)=\u003etracing::info!(\"{:?}\",info),Err(e)=\u003etracing::error!(\"{:?}\",e),}}","categories":"","description":"","excerpt":"为了增加框架的灵活性和易用性，Volo 允许用户在 Client 端使用 CallOpt 针对单个请求设置一些请求的元信息。 …","ref":"/zh/docs/volo/guide/with_callopt/","tags":"","title":"在调用时指定 CallOpt"},{"body":"为什么需要 Plugin 为 Pilota 根据 IDL 生成的 Struct 等类型增加一些自定义的 meta 信息。\n比如增加#[derive(serde::Serialize, serde::Deserialize]) 等\n如何写一个 Plugin 实现 Plugin #[derive(Clone, Copy)]struct SerdePlugin;implpilota_build::PluginforSerdePlugin{fn on_item(\u0026mutself,cx: \u0026mutpilota_build::Context,def_id: pilota_build::DefId,// item 的 def_id item: std::sync::Arc\u003cpilota_build::rir::Item\u003e,){match\u0026*item{pilota_build::rir::Item::Message(_)|pilota_build::rir::Item::Enum(_)|pilota_build::rir::Item::NewType(_)=\u003ecx.with_adjust(def_id,|adj|{// Adjust 的 add_attrs 方法可以为 def_id 对应的 Node 增加 Attribute，在之后的 Codegen 阶段会带上这些元信息生成代码 adj.add_attrs(\u0026[parse_quote!(#[derive(::serde::Serialize, ::serde::Deserialize)])])}),_=\u003e{}};pilota_build::plugin::walk_item(self,cx,def_id,item)}}使用 Plugin 通过 Builder 提供的 plugin 方法传入即可\npilota_build::thrift().plugin(SerdePlugin).write()","categories":"","description":"","excerpt":"为什么需要 Plugin 为 Pilota 根据 IDL 生成的 Struct 等类型增加一些自定义的 meta 信息。\n比如增 …","ref":"/zh/docs/pilota/guide/plugin/","tags":"","title":"如何编写 Plugin？"},{"body":" 搜索    全部  低  中  高  致命     年份 请选择 2021 2022 2023 2024 2025       公告 摘要 严重级别 影响组件与版本 发布时间      Previous  1 2  3 Next   Total: 3    ","categories":"","description":"","excerpt":" 搜索    全部  低  中  高  致命     年份 请选择 2021 2022 2023 2024 2025 …","ref":"/zh/security/safety-bulletin/","tags":"","title":"安全公告"},{"body":"Volo 提供了同名的命令行工具，用来初始化项目、管理 IDL 等。我们可以先通过以下命令来安装 Volo：\n$ cargo install volo-cli 随后，我们输入：\n$ volo help 就能看到类似以下输出啦：\nUSAGE: volo [OPTIONS] \u003cSUBCOMMAND\u003e OPTIONS: -h, --help Print help information -n, --entry-name \u003cENTRY_NAME\u003e The entry name, defaults to 'default'. [default: default] -v, --verbose Turn on the verbose mode. -V, --version Print version information SUBCOMMANDS: help Print this message or the help of the given subcommand(s) idl manage your idl init init your project ","categories":"","description":"","excerpt":"Volo 提供了同名的命令行工具，用来初始化项目、管理 IDL 等。我们可以先通过以下命令来安装 Volo：\n$ cargo install …","ref":"/zh/docs/volo/volo-grpc/getting-started/part_1/","tags":"","title":"Part 1. 安装命令行工具"},{"body":"Volo 提供了同名的命令行工具，用来初始化项目、管理 IDL 等。我们可以先通过以下命令来安装 Volo：\n$ cargo install volo-cli 随后，我们输入：\n$ volo help 就能看到类似以下输出啦：\nUSAGE: volo [OPTIONS] \u003cSUBCOMMAND\u003e OPTIONS: -h, --help Print help information -n, --entry-name \u003cENTRY_NAME\u003e The entry name, defaults to 'default'. [default: default] -v, --verbose Turn on the verbose mode. -V, --version Print version information SUBCOMMANDS: help Print this message or the help of the given subcommand(s) idl manage your idl init init your project ","categories":"","description":"","excerpt":"Volo 提供了同名的命令行工具，用来初始化项目、管理 IDL 等。我们可以先通过以下命令来安装 Volo：\n$ cargo install …","ref":"/zh/docs/volo/volo-thrift/getting-started/part_1/","tags":"","title":"Part 1. 安装命令行工具"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/volo/volo-grpc/getting-started/","tags":"","title":"快速开始"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/volo/volo-thrift/getting-started/","tags":"","title":"快速开始"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/","tags":"","title":"文档"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/blog/news/","tags":"","title":"新闻"},{"body":"CloudWeGo-Hertz Hertz[həːts] 是一个 Golang 微服务 HTTP 框架，在设计之初参考了其他开源框架 fasthttp、gin、echo 的优势， 并结合字节跳动内部的需求，使其具有高易用性、高性能、高扩展性等特点，目前在字节跳动内部已广泛使用。 如今越来越多的微服务选择使用 Golang，如果对微服务性能有要求，又希望框架能够充分满足内部的可定制化需求，Hertz 会是一个不错的选择。\n架构设计 框架特点   高易用性\n在开发过程中，快速写出来正确的代码往往是更重要的。因此，在 Hertz 在迭代过程中，积极听取用户意见，持续打磨框架，希望为用户提供一个更好的使用体验，帮助用户更快的写出正确的代码。\n  高性能\nHertz 默认使用自研的高性能网络库 Netpoll，在一些特殊场景相较于 go net，Hertz 在 QPS、时延上均具有一定优势。关于性能数据，可参考下图 Echo 数据。 关于详细的性能数据，可参考 https://github.com/cloudwego/hertz-benchmark。\n  高扩展性\nHertz 采用了分层设计，提供了较多的接口以及默认的扩展实现，用户也可以自行扩展。同时得益于框架的分层设计，框架的扩展性也会大很多。目前仅将稳定的能力开源给社区，更多的规划参考 RoadMap。\n  多协议支持\nHertz 框架原生提供 HTTP1.1、ALPN 协议支持。除此之外，由于分层设计，Hertz 甚至支持自定义构建协议解析逻辑，以满足协议层扩展的任意需求。\n  网络层切换能力\nHertz 实现了 Netpoll 和 Golang 原生网络库 间按需切换能力，用户可以针对不同的场景选择合适的网络库，同时也支持以插件的方式为 Hertz 扩展网络库实现。\n  框架性能 性能测试只能提供相对参考，工业场景下，有诸多因素可以影响实际的性能表现。\n我们提供了 hertz-benchmark 项目用来长期追踪和比较 Hertz 与其他框架在不同情况下的性能数据以供参考。\n相关项目  Netpoll: 自研高性能网络库，Hertz 默认集成 Hertz-Contrib: Hertz 扩展仓库，提供中间件、tracer 等能力 Example: Hertz 使用例子  相关文章  字节跳动在 Go 网络库上的实践  ","categories":"","description":"","excerpt":"CloudWeGo-Hertz Hertz[həːts] 是一个 Golang 微服务 HTTP 框架， …","ref":"/zh/docs/hertz/overview/","tags":"","title":"概览"},{"body":"CloudWeGo-Kitex Kitex[kaɪt’eks] 字节跳动内部的 Golang 微服务 RPC 框架，具有高性能、强可扩展的特点，在字节内部已广泛使用。如果对微服务性能有要求，又希望定制扩展融入自己的治理体系，Kitex 会是一个不错的选择。\n架构设计 框架特点   高性能\n使用自研的高性能网络库 Netpoll，性能相较 go net 具有显著优势。\n  扩展性\n提供了较多的扩展接口以及默认扩展实现，使用者也可以根据需要自行定制扩展，具体见下面的框架扩展。\n  多消息协议\nRPC 消息协议默认支持 Thrift、Kitex Protobuf、gRPC。Thrift 支持 Buffered 和 Framed 二进制协议；Kitex Protobuf 是 Kitex 自定义的 Protobuf 消息协议，协议格式类似 Thrift；gRPC 是对 gRPC 消息协议的支持，可以与 gRPC 互通。除此之外，使用者也可以扩展自己的消息协议。\n  多传输协议\n传输协议封装消息协议进行 RPC 互通，传输协议可以额外透传元信息，用于服务治理，Kitex 支持的传输协议有 TTHeader、HTTP2。TTHeader 可以和 Thrift、Kitex Protobuf 结合使用；HTTP2 目前主要是结合 gRPC 协议使用，后续也会支持 Thrift。\n  多种消息类型\n支持 PingPong、Oneway、双向 Streaming。其中 Oneway 目前只对 Thrift 协议支持，双向 Streaming 只对 gRPC 支持，后续会考虑支持 Thrift 的双向 Streaming。\n  服务治理\n支持服务注册/发现、负载均衡、熔断、限流、重试、监控、链路跟踪、日志、诊断等服务治理模块，大部分均已提供默认扩展，使用者可选择集成。\n  代码生成\nKitex 内置代码生成工具，可支持生成 Thrift、Protobuf 以及脚手架代码。\n  框架性能 性能测试只能提供相对参考，工业场景下，有诸多因素可以影响实际的性能表现。\n由于开源社区缺少支持 thrift 的优秀 RPC 框架，当前对比项目为 grpc, rpcx, 均使用 protobuf 协议。\n我们通过 测试代码 比较了它们的性能，测试表明 Kitex 具有明显优势。\n测试环境  CPU: Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz, 4 cores Memory: 8GB OS: Debian 5.4.56.bsk.1-amd64 x86_64 GNU/Linux Go: 1.15.4  并发表现 (Echo 1KB, 改变并发量)    QPS TP99 TP999           吞吐表现 (并发 100, 改变包大小)    QPS TP99 TP999           相关项目  Netpoll: 自研的高性能网络库，Kitex 默认集成的。 kitex-contrib：Kitex 的部分扩展库，使用者可以根据需求通过 Option 集成进 Kitex 中。 Example：Kitex 的使用示例。  相关文章  字节跳动 Go RPC 框架 Kitex 性能优化实践 字节跳动在 Go 网络库上的实践  ","categories":"","description":"","excerpt":"CloudWeGo-Kitex Kitex[kaɪt’eks] 字节跳动内部的 Golang 微服务 RPC 框架，具有高性能、强可扩展的特 …","ref":"/zh/docs/kitex/overview/","tags":"","title":"概览"},{"body":"Motore 是一个使用了 GAT 和 TAIT 特性的中间件抽象层。\n基于 Motore，我们编写了一些模块化并且可服用的，用来编写 client 和 server 的组件。\nMotore 深受Tower 启发。\nMotore 使用 GAT 和 TAIT 来减轻编写异步代码的精神负担，尤其是为了避免 Box 的开销而导致的负担，以减少使用者的焦虑。\nMotore 最核心的抽象是 Service trait：\npubtraitService\u003cCx,Request\u003e{/// Responses given by the service. type Response;/// Errors produced by the service. type Error;/// The future response value. type Future\u003c'cx\u003e: Future\u003cOutput=Result\u003cSelf::Response,Self::Error\u003e\u003e+Send+'cxwhereCx: 'cx,Self: 'cx;/// Process the request and return the response asynchronously. fn call\u003c'cx,'s\u003e(\u0026'smutself,cx: \u0026'cxmutCx,req: Request)-\u003e Self::Future\u003c'cx\u003ewhere's: 'cx;}快速上手 通过将 GAT 和 TAIT 组合在一起，我们可以以非常简洁易读的方式编写异步代码：\npubstruct Timeout\u003cS\u003e{inner: S,duration: Duration,}impl\u003cCx,Req,S\u003eService\u003cCx,Req\u003eforTimeout\u003cS\u003ewhereReq: 'static+Send,S: Service\u003cCx,Req\u003e+'static+Send,Cx: 'static+Send,S::Error: Send +Sync+Into\u003cBoxError\u003e,{type Response=S::Response;type Error=BoxError;type Future\u003c'cx\u003e=implFuture\u003cOutput=Result\u003cS::Response,Self::Error\u003e\u003e+'cx;fn call\u003c'cx,'s\u003e(\u0026'smutself,cx: \u0026'cxmutCx,req: Req)-\u003e Self::Future\u003c'cx\u003ewhere's: 'cx,{asyncmove{letsleep=tokio::time::sleep(self.duration);tokio::select!{r=self.inner.call(cx,req)=\u003e{r.map_err(Into::into)},_=sleep=\u003eErr(std::io::Error::new(std::io::ErrorKind::TimedOut,\"service time out\").into()),}}}}我们还提供了#[motore::service]宏以使编写 Service 更加像编写原生异步 Rust：\nusemotore::service;pubstruct S\u003cI\u003e{inner: I,}#[service]impl\u003cCx,Req,I\u003eService\u003cCx,Req\u003eforS\u003cI\u003ewhereReq: Send +'static,I: Service\u003cCx,Req\u003e+Send+'static,Cx: Send +'static,{asyncfn call(\u0026mutself,cx: \u0026mutCx,req: Req)-\u003e Result\u003cI::Response,I::Error\u003e{self.inner.call(cx,req).await}}","categories":"","description":"","excerpt":"Motore 是一个使用了 GAT 和 TAIT 特性的中间件抽象层。\n基于 Motore，我们编写了一些模块化并且可服用的， …","ref":"/zh/docs/motore/overview/","tags":"","title":"概览"},{"body":"简介 Netpoll 是由 字节跳动 开发的高性能 NIO(Non-blocking I/O)网络库，专注于 RPC 场景。\nRPC 通常有较重的处理逻辑，因此无法串行处理 I/O。而 Go 的标准库 net 设计了 BIO(Blocking I/O) 模式的 API，使得 RPC 框架设计上只能为每个连接都分配一个 goroutine。 这在高并发下，会产生大量的 goroutine，大幅增加调度开销。此外，net.Conn 没有提供检查连接活性的 API，因此 RPC 框架很难设计出高效的连接池，池中的失效连接无法及时清理。\n另一方面，开源社区目前缺少专注于 RPC 方案的 Go 网络库。类似的项目如：evio, gnet 等，均面向 Redis, HAProxy 这样的场景。\n因此 Netpoll 应运而生，它借鉴了 evio 和 netty 的优秀设计，具有出色的 性能，更适用于微服务架构。 同时，Netpoll 还提供了一些 特性，推荐在 RPC 设计中替代 net 。\n基于 Netpoll 开发的 RPC 框架 Kitex 和 HTTP 框架 Hertz (即将开源)，性能均业界领先。\n范例 展示了如何使用 Netpoll 构建 RPC Client 和 Server。\n特性   已经支持\n LinkBuffer 提供可以流式读写的 nocopy API gopool 提供高性能的 goroutine 池 mcache 提供高效的内存复用 IsActive 支持检查连接是否存活 Dialer 支持构建 client EventLoop 支持构建 server 支持 TCP，Unix Domain Socket 支持 Linux，macOS（操作系统）    即将开源\n multisyscall 支持批量系统调用 io_uring Shared Memory IPC 串行调度 I/O，适用于纯计算 支持 TLS 支持 UDP    不被支持\n Windows（操作系统）    性能 性能测试应满足工业级使用要求，在 RPC 场景下，并发请求、等待超时是必要的支持项。\n我们提供了 netpoll-benchmark 项目用来长期追踪和比较 Netpoll 与其他框架在不同情况下的性能数据以供参考。\n更多测试参考 kitex-benchmark 和 hertz-benchmark (即将开源)\n参考  官方网站 使用文档  ","categories":"","description":"","excerpt":"简介 Netpoll 是由 字节跳动 开发的高性能 NIO(Non-blocking I/O)网络库，专注于 RPC 场景。\nRPC 通常有 …","ref":"/zh/docs/netpoll/overview/","tags":"","title":"概览"},{"body":"Volo Volo 是字节跳动服务框架团队研发的轻量级、高性能、可扩展性强、易用性好的 Rust RPC 框架，使用了 Rust 最新的 GAT 特性。\nVolo 使用 Motore 作为中间件抽象层，Motore 基于 GAT 设计。\n架构图 特性 基于 GAT 设计 我们热爱并追随最新的技术，Volo 的核心抽象使用了 Rust 最新的 GAT 特性，在这个过程中我们也借鉴了 Tower 的设计。 Tower 是一个非常优秀的抽象层设计，适用于非 GAT 的情况下，非常感谢 Tower 团队。\n通过 GAT，我们可以避免很多不必要的 Box 内存分配，以及提升易用性，给用户提供更友好的编程接口和更符合人体工程学的编程范式。\n高性能 Rust 以高性能和安全著称，我们在设计和实现过程中也时刻以高性能作为我们的目标，尽可能降低每一处的开销，提升每一处实现的性能。\n首先要说明，和 Go 的框架对比性能是极不公平的，因此我们不会着重比较 Volo 和 Kitex 的性能，并且我们给出的数据仅能作为参考，希望大家能够客观看待； 同时，由于在开源社区并没有找到另一款成熟的 Rust 语言的 Async 版本 Thrift RPC 框架，而且性能对比总是容易引战，因此我们希望尽可能弱化性能数据的对比，仅会公布我们自己极限 QPS 的数据。\n在和 Kitex 相同的测试条件（限制 4C）下，Volo 极限 QPS 为 35W；同时，我们内部正在验证基于 Monoio（CloudWeGo 开源的 Rust Async Runtime）的版本，极限 QPS 可以达到 44W。\n从我们线上业务的火焰图来看，得益于 Rust 的静态分发和优秀的编译优化，框架部分的开销基本可以忽略不计（不包含 syscall 开销）。\n易用性好 Rust 以难学难用而闻名，我们希望尽可能降低用户使用 Volo 框架以及使用 Rust 语言编写微服务的难度，提供最符合人体工程学和直觉的编码体验。因此，我们把易用性作为我们重要的目标之一。\n比如，我们提供了 volo 命令行工具，用于初始化项目以及管理 idl；同时，我们将 thrift 及 gRPC 拆分为两个独立（但共用一些组件）的框架，以提供最符合不同协议语义的编程范式及接口。\n我们还提供了#[service]宏（可以理解为不需要 Box 的 async_trait）来使得用户可以无心理负担地使用异步来编写 Service 中间件。\n扩展性强 收益于 Rust 强大的表达和抽象能力，通过灵活的中间件 Service 抽象，开发者可以以非常统一的形式，对 RPC 元信息、请求和响应做处理。\n比如，服务发现、负载均衡等服务治理功能，都可以以 Service 形式进行实现，而不需要独立实现 Trait。\n相关的扩展，我们会放在 volo-rs 组织下，也欢迎大家贡献自己的扩展到 volo-rs～\n相关生态  Volo-rs：Volo 的相关生态 Pilota：Volo 使用的 Thrift 与 Protobuf 编译器及编解码的纯 Rust 实现（不依赖 protoc） Motore：Volo 参考 Tower 设计的，使用了 GAT 的 middleware 抽象层 Metainfo：Volo 用于进行元信息透传的组件，期望定义一套元信息透传的标准  ","categories":"","description":"","excerpt":"Volo Volo 是字节跳动服务框架团队研发的轻量级、高性能、可扩展性强、易用性好的 Rust RPC 框架，使用了 Rust …","ref":"/zh/docs/volo/overview/","tags":"","title":"概览"},{"body":"框架自身不带任何监控打点，只是提供了 Tracer 接口，用户可以根据需求实现该接口，并通过 WithTracer Option 来注入。\n// Tracer is executed at the start and finish of an HTTP. type Tracer interface { Start(ctx context.Context, c *app.RequestContext) context.Context Finish(ctx context.Context, c *app.RequestContext) } hertz-contrib 中提供了默认的 prometheus 的监控扩展，能够实现:\n 请求量监控 时延监控  默认的 tag 有： HTTP Method，statusCode。请求相关的信息存在 RequestContext，在打点上报时可以获取到该变量，用户可以根据自己的需要自行扩展打点功能。 使用方式：\nServer\nimport ( \"context\" \"github.com/cloudwego/hertz/pkg/app\" \"github.com/cloudwego/hertz/pkg/app/server\" \"github.com/cloudwego/hertz/pkg/common/utils\" \"github.com/hertz-contrib/monitor-prometheus\" ) func main() { ··· h := server.Default(server.WithTracer(prometheus.NewServerTracer(\":9091\", \"/hertz\"))) h.GET(\"/ping\", func(c context.Context, ctx *app.RequestContext) { ctx.JSON(200, utils.H{\"ping\": \"pong\"}) }) h.Spin() ··· } 目前 Client 暂没有暴露 Tracer 接口，但是提供了中间件能力，可以通过中间件实现监控能力。\n仓库 https://github.com/hertz-contrib/monitor-prometheus\n","categories":"","description":"","excerpt":"框架自身不带任何监控打点，只是提供了 Tracer 接口，用户可以根据需求实现该接口，并通过 WithTracer Option 来注入。 …","ref":"/zh/docs/hertz/tutorials/service-governance/monitoring/","tags":"","title":"监控"},{"body":"用户如果需要更详细的打点，例如包大小，或者想要更换其他数据源，例如 influxDB，用户可以根据自己的需求实现 Tracer 接口，并通过 WithTracer Option 来注入。\n// Tracer is executed at the start and finish of an HTTP. type Tracer interface { Start(ctx context.Context, c *app.RequestContext) context.Context Finish(ctx context.Context, c *app.RequestContext) } 从 ctx 中可以获得 TraceInfo，进一步的从 TraceInfo 中获取请求耗时、包大小和请求返回的错误信息等，举例：\ntype ServerTracer struct{ // contain entities which recording metric } // Start record the beginning of an RPC invocation. func (s *ServerTracer) Start(ctx context.Context, _ *app.RequestContext) context.Context { // do nothing \treturn ctx } // Finish record after receiving the response of server. func (s *ServerTracer) Finish(ctx context.Context, c *app.RequestContext) { ti := c.GetTraceInfo() rpcStart := ti.Stats().GetEvent(stats.HTTPStart) rpcFinish := ti.Stats().GetEvent(stats.HTTPFinish) cost := rpcFinish.Time().Sub(rpcStart.Time()) // TODO: record the cost of request } ","categories":"","description":"","excerpt":"用户如果需要更详细的打点，例如包大小，或者想要更换其他数据源，例如 influxDB，用户可以根据自己的需求实现 Tracer 接口， …","ref":"/zh/docs/hertz/tutorials/framework-exten/monitor/","tags":"","title":"监控扩展"},{"body":"Hertz 提供了网络库扩展的能力。用户如果需要更换其他的网络库，可以根据需求实现对应的接口。Server 需要实现 network.Conn 接口，Client 需要实现 network.Dialer 接口。\n接口定义 接口在 pkg/network/connection.go 中\ntype Conn interface { net.Conn Reader Writer SetReadTimeout(t time.Duration) error } // Reader is for buffered Reader type Reader interface { // Peek returns the next n bytes without advancing the reader.  Peek(n int) ([]byte, error) // Skip discards the next n bytes.  Skip(n int) error // Release the memory space occupied by all read slices. This method needs to be executed actively to  // recycle the memory after confirming that the previously read data is no longer in use.  // After invoking Release, the slices obtained by the method such as Peek will  // become an invalid address and cannot be used anymore.  Release() error // Len returns the total length of the readable data in the reader.  Len() int // ReadByte is used to read one byte with advancing the read pointer.  ReadByte() (byte, error) // ReadBinary is used to read next n byte with copy, and the read pointer will be advanced.  ReadBinary(n int) (p []byte, err error) } type Writer interface { // Malloc will provide a n bytes buffer to send data.  Malloc(n int) (buf []byte, err error) // WriteBinary will use the user buffer to flush.  // NOTE: Before flush successfully, the buffer b should be valid.  WriteBinary(b []byte) (n int, err error) // Flush will send data to the peer end.  Flush() error } 对于 Client 来说，实现了以下接口就可以替换 Client 侧的网络库。\ntype Dialer interface { DialConnection(network, address string, timeout time.Duration, tlsConfig *tls.Config) (conn Conn, err error) DialTimeout(network, address string, timeout time.Duration, tlsConfig *tls.Config) (conn net.Conn, err error) AddTLS(conn Conn, tlsConfig *tls.Config) (Conn, error) } 自定义网络库 Hertz 的 Server 和 Client 分别提供了初始化配置项\nServer\nserver.New(server.WithTransport(YOUR_TRANSPORT)) Client\nclient.NewClient(client.WithDialer(YOUR_DIALER)) ","categories":"","description":"","excerpt":"Hertz 提供了网络库扩展的能力。用户如果需要更换其他的网络库，可以根据需求实现对应的接口。Server …","ref":"/zh/docs/hertz/tutorials/framework-exten/advanced-exten/network-lib/","tags":"","title":"网络库扩展"},{"body":"Server Server 侧的配置项均在初始化 Server 时采用 server.xxx 的方式，如\npackage main import \"github.com/cloudwego/hertz/pkg/app/server\" func main() { h := server.New(server.WithXXXX()) ... }    配置名称 类型 说明     WithTransport network.NewTransporter 更换底层 transport，默认值：netpoll.NewTransporter   WithHostPorts string 指定监听的地址和端口   WithKeepAliveTimeout time.Duration tcp 长连接保活时间，一般情况下不用修改，更应该关注 idleTimeout。默认值：1min   WithReadTimeout time.Duration 底层读取数据超时时间。默认值：3min   WithIdleTimeout time.Duration 长连接请求链接空闲超时时间。默认值：3min   WithMaxRequestBodySize int 配置最大的请求体大小，默认4M（4M对应的填的值是4*1024*1024）   WithRedirectTrailingSlash bool 自动根据末尾的 / 转发，例如：如果 router 只有 /foo/，那么 /foo 会重定向到 /foo/ ；如果只有 /foo，那么 /foo/ 会重定向到 /foo。默认开启   WithRemoveExtraSlash bool RemoveExtraSlash 当有额外的 / 时也可以当作参数。如: user/:name，如果开启该选项 user//xiaoming 也可匹配上参数。默认关闭   WithUnescapePathValues bool 如果开启，请求路径会被自动转义（eg. ‘%2F’ -\u003e ‘/'）。如果 UseRawPath 为 false（默认情况），则 UnescapePathValues 实际上为 true，因为 .URI().Path() 将被使用，它已经是转义后的。设置该参数为 false，需要配合 WithUseRawPath(true)。 默认开启(true)   WithUseRawPath bool 如果开启， 会使用原始 path 进行路由匹配。默认关闭   WithHandleMethodNotAllowed bool 如果开启，当当前路径不能被匹配上时，server 会去检查其他方法是否注册了当前路径的路由，如果存在则会响应\"Method Not Allowed\"，并返回状态码405; 如果没有，则会用 NotFound 的 handler 进行处理。默认关闭   WithDisablePreParseMultipartForm bool 如果开启，则不会预处理 multipart form。可以通过 ctx.Request.Body() 获取到 body 后由用户处理。默认关闭   WithStreamBody bool 如果开启，则会使用流式处理 body。默认关闭   WithNetwork string 设置网络协议，可选：tcp，udp，unix（unix domain socket），默认为tcp   ContinueHandler func(header *RequestHeader) bool 在接收到 Expect 100 Continue 头之后调用 ContinueHandler。使用 ContinueHandler，服务器可以决定是否根据标头读取可能很大的请求正文   PanicHandler HandlerFunc 处理 panic，用来生成错误页面并返回500   NotFound HandlerFunc 当路由匹配不上时被调用的 handler   WithExitWaitTime time.Duration 设置优雅退出时间。Server 会停止建立新的连接，并对关闭后的每一个请求设置 Connection: Close 的 header，当到达设定的时间关闭 Server。当所有连接已经关闭时，Server 可以提前关闭。默认 5s   WithTLS tls.Config 配置 server tls 能力   WithListenConfig net.ListenConfig 设置监听器配置，可用于设置是否允许 reuse port 等   WithALPN bool 是否开启 ALPN。默认关闭   WithTracer tracer.Tracer 注入 tracer 实现，如不注入 Tracer 实现，默认关闭   WithTraceLevel stats.Level 设置 trace level，默认 LevelDetailed    Client Client 侧的配置项均在初始化 Client 时采用 client.xxx 的方式\npackage main import \"github.com/cloudwego/hertz/pkg/app/client\" func main() { c, err := client.NewClient(client.WithXxx()) ... }    配置名称 类型 说明     WithDialTimeout time.Duration 连接建立超时时间，默认 1s   WithMaxConnsPerHost int 设置为每个 host 建立的最大连接数，默认 512   WithMaxIdleConnDuration time.Duration 设置空闲连接超时时间，当超时后会关闭该连接，默认10s   WithMaxConnDuration time.Duration 设置连接存活的最大时长，超过这个时间的连接在完成当前请求后会被关闭，默认无限长   WithMaxConnWaitTimeout time.Duration 设置等待空闲连接的最大时间，默认不等待   WithKeepAlive bool 是否使用长连接，默认开启   WithMaxIdempotentCallAttempts int 设置失败重试次数，默认5次   WithClientReadTimeout time.Duration 设置读取 response 的最长时间，默认无限长   WithTLSConfig *tls.Config 双向 TLS 认证时，设置 client 的 TLS config   WithDialer network.Dialer 设置 client 使用的网络库，默认 netpoll   WithResponseBodyStream bool 设置是否使用流式处理，默认关闭   WithDialFunc client.DialFunc 设置 Dial Function    ","categories":"","description":"","excerpt":"Server Server 侧的配置项均在初始化 Server 时采用 server.xxx 的方式，如\npackage main …","ref":"/zh/docs/hertz/reference/config/","tags":"","title":"配置说明"},{"body":"Hertz provides command-line tools (hz) that support custom template features, including:\n Customize the layout template (i.e., the directory structure of the generated code) Custom package templates (i.e. service-related code structures, including handler, router, etc.)  Users can provide their own templates and rendering parameters, combined with the ability of hz, to complete the custom code generation structure.\nCustom layout template  Users can modify or rewrite according to the default template to meet their own needs\n Hz takes advantage of the “go template” capability to support defining templates in “yaml” format and uses “json” format to define rendering data.\nThe so-called layout template refers to the structure of the entire project. These structures have nothing to do with the specific idl definition, and can be directly generated without idl. The default structure is as follows:\n. ├── biz │ ├── handler │ │ └── ping.go │ │ └── ****.go // Set of handlers divided by service, the position can be changed according to handler_dir │ ├── model │ │ └── model.go // idl generated struct, the position can be changed according to model_dir │ └── router //undeveloped custom dir │ └── register.go // Route registration, used to call the specific route registration │ └── route.go // Specific route registration location │ └── middleware.go // Default middleware build location ├── .hz // hz Create code flags ├── go.mod ├── main.go // Start the entrance ├── router.go // User-defined route write location └── router_gen.go // hz generated route registration call IDL // hello.thrift namespace go hello.example struct HelloReq { 1: string Name (api.query=\"name\"); } struct HelloResp { 1: string RespBody; } service HelloService { HelloResp HelloMethod(1: HelloReq request) (api.get=\"/hello\"); } Command hz new --mod=github.com/hertz/hello --idl=./hertzDemo/hello.thrift --customize_layout=template/layout.yaml:template/data.json The meaning of the default layout template  Note: The following bodies are all go templates\n layouts: # The directory of the generated handler will only be generated if there are files in the directory - path: biz/handler/ delims: - \"\" - \"\" body: \"\" # The directory of the generated model will only be generated if there are files in the directory - path: biz/model/ delims: - \"\" - \"\" body: \"\" # project main file - path: main.go delims: - \"\" - \"\" body: |- // Code generated by hertz generator. package main import ( \"github.com/cloudwego/hertz/pkg/app/server\" ) func main() { h := server.Default() register(h) h.Spin() } # go.mod file, need template rendering data {{.GoModule}} to generate - path: go.mod delims: - '{{' - '}}' body: |- module {{.GoModule}} {{- if .UseApacheThrift}} replace github.com/apache/thrift =\u003e github.com/apache/thrift v0.13.0 {{- end}} # .gitignore file - path: .gitignore delims: - \"\" - \"\" body: \"*.o\\n*.a\\n*.so\\n_obj\\n_test\\n*.[568vq]\\n[568vq].out\\n*.cgo1.go\\n*.cgo2.c\\n_cgo_defun.c\\n_cgo_gotypes.go\\n_cgo_export.*\\n_testmain.go\\n*.exe\\n*.exe~\\n*.test\\n*.prof\\n*.rar\\n*.zip\\n*.gz\\n*.psd\\n*.bmd\\n*.cfg\\n*.pptx\\n*.log\\n*nohup.out\\n*settings.pyc\\n*.sublime-project\\n*.sublime-workspace\\n!.gitkeep\\n.DS_Store\\n/.idea\\n/.vscode\\n/output\\n*.local.yml\\ndumped_hertz_remote_config.json\\n\\t\\t \\ \" # .hz file, containing hz version, is the logo of the project created by hz, no need to transfer rendering data - path: .hz delims: - '{{' - '}}' body: |- // Code generated by hz. DO NOT EDIT. hz version: {{.hzVersion}} # ping comes with ping handler - path: biz/handler/ping.go delims: - \"\" - \"\" body: |- // Code generated by hertz generator. package handler import ( \"context\" \"github.com/cloudwego/hertz/pkg/app\" \"github.com/cloudwego/hertz/pkg/common/utils\" ) // Ping . func Ping(ctx context.Context, c *app.RequestContext) { c.JSON(200, utils.H{ \"message\": \"pong\", }) } # `router_gen.go` is the file that defines the route registration - path: router_gen.go delims: - \"\" - \"\" body: |- // Code generated by hertz generator. DO NOT EDIT. package main import ( \"github.com/cloudwego/hertz/pkg/app/server\" router \"{{.RouterPkgPath}}\" ) // register registers all routers. func register(r *server.Hertz) { router.GeneratedRegister(r) customizedRegister(r) } # Custom route registration file - path: router.go delims: - \"\" - \"\" body: |- // Code generated by hertz generator. package main import ( \"github.com/cloudwego/hertz/pkg/app/server\" handler \"{{.HandlerPkgPath}}\" ) // customizeRegister registers customize routers. func customizedRegister(r *server.Hertz){ r.GET(\"/ping\", handler.Ping) // your code ... } # Default route registration file, do not modify it - path: biz/router/register.go delims: - \"\" - \"\" body: |- // Code generated by hertz generator. DO NOT EDIT. package router import ( \"github.com/cloudwego/hertz/pkg/app/server\" ) // GeneratedRegister registers routers generated by IDL. func GeneratedRegister(r *server.Hertz){ //INSERT_POINT: DO NOT DELETE THIS LINE! } The meaning of template rendering parameter file When a custom template and render data are specified, the options specified on the command line will not be used as render data, so the render data in the template needs to be defined by the user.\nHz uses “json” to specify render data, as described below\n{ // global render parameters \"*\": { \"GoModule\": \"github.com/hz/test\", // must be consistent with the command line, otherwise the subsequent generation of model, handler and other code will use the mod specified by the command line, resulting in inconsistency. \"ServiceName\": \"p.s.m\", // as specified on the command line \"UseApacheThrift\": false // Set \"true\"/\"false\" depending on whether to use \"thrift\" }, // router_gen.go route the registered render data, // \"biz/router\"points to the module of the routing code registered by the default idl, do not modify it \"router_gen.go\": { \"RouterPkgPath\": \"github.com/hz/test/biz/router\" } } Customize a layout template  At present, the project layout generated by hz is already the most basic skeleton of a hertz project, so it is not recommended to delete the files in the existing template.\nHowever, if the user wants a different layout, of course, you can also delete the corresponding file according to your own needs (except “biz/register.go”, the rest can be modified)\nWe welcome users to contribute their own templates\n Assuming that the user only wants “main.go” and “go.mod” files, then we modify the default template as follows:\ntemplate: // layout.yaml layouts: # project main file - path: main.go delims: - \"\" - \"\" body: |- // Code generated by hertz generator. package main import ( \"github.com/cloudwego/hertz/pkg/app/server\" \"{{.GoModule}}/biz/router\" ) func main() { h := server.Default() router.GeneratedRegister(h) // do what you wanted // add some render data: {{.MainData}} h.Spin() } # go.mod file, need template rendering data {{.GoModule}} to generate - path: go.mod delims: - '{{' - '}}' body: |- module {{.GoModule}} {{- if .UseApacheThrift}} replace github.com/apache/thrift =\u003e github.com/apache/thrift v0.13.0 {{- end}} # Default route registration file, no need to modify - path: biz/router/register.go delims: - \"\" - \"\" body: |- // Code generated by hertz generator. DO NOT EDIT. package router import ( \"github.com/cloudwego/hertz/pkg/app/server\" ) // GeneratedRegister registers routers generated by IDL. func GeneratedRegister(r *server.Hertz){ //INSERT_POINT: DO NOT DELETE THIS LINE! } render data: { \"*\": { \"GoModule\": \"github.com/hertz/hello\", \"ServiceName\": \"hello\", \"UseApacheThrift\": true }, \"main.go\": { \"MainData\": \"this is customized render data\" } } Command:\nhz new --mod=github.com/hertz/hello --idl=./hertzDemo/hello.thrift --customize_layout=template/layout.yaml:template/data.json Custom package template  The template address of the hz template:\nUsers can modify or rewrite according to the default template to meet their own needs\n  The so-called package template refers to the code related to the idl definition. This part of the code involves the service, go_package/namespace, etc. specified when defining idl. It mainly includes the following parts: handler.go: Handling function logic router.go: the route registration logic of the service defined by the specific idl register.go: logic for calling the content in router.go Model code: generated go struct; however, since the tool that uses plugins to generate model code currently does not have permission to modify the model template, this part of the function is not open for now  Command # After that, the package template rendering data will be provided, so the form of \"k-v\" is retained when entering the command, and \":\" needs to be added after customize_package. hz new --mod=github.com/hertz/hello --handler_dir=handler_test --idl=hertzDemo/hello.thrift --customize_package=template/package.yaml: Default package template Note: Custom package templates do not provide the ability to render data.\nlayouts: # Path only represents handler.go template, the specific handler path is determined by the default path and handler_dir - path: handler.go delims: - '{{' - '}}' body: |- // Code generated by hertz generator. package {{.PackageName}} import ( \"context\" \"github.com/cloudwego/hertz/pkg/app\" {{- range $k, $v := .Imports}} {{$k}} \"{{$v.Package}}\" {{- end}} ) {{range $_, $MethodInfo := .Methods}} {{$MethodInfo.Comment}} func {{$MethodInfo.Name}}(ctx context.Context, c *app.RequestContext) { var err error {{if ne $MethodInfo.RequestTypeName \"\" -}} var req {{$MethodInfo.RequestTypeName}} err = c.BindAndValidate(\u0026req) if err != nil { c.String(400, err.Error()) return } {{end}} resp := new({{$MethodInfo.ReturnTypeName}}) c.{{.Serializer}}(200, resp) } {{end}} # path only represents router.go template, its path is fixed at: biz/router/namespace/ - path: router.go delims: - '{{' - '}}' body: |- // Code generated by hertz generator. DO NOT EDIT. package {{$.PackageName}} import ( \"github.com/cloudwego/hertz/pkg/app/server\" {{range $k, $v := .HandlerPackages}}{{$k}} \"{{$v}}\"{{end}} ) /* This file will register all the routes of the services in the master idl. And it will update automatically when you use the \"update\" command for the idl. So don't modify the contents of the file, or your code will be deleted when it is updated. */ {{define \"g\"}} {{- if eq .Path \"/\"}}r {{- else}}{{.GroupName}}{{end}} {{- end}} {{define \"G\"}} {{- if ne .Handler \"\"}} {{- .GroupName}}.{{.HttpMethod}}(\"{{.Path}}\", append({{.MiddleWare}}Mw(), {{.Handler}})...) {{- end}} {{- if ne (len .Children) 0}} {{.MiddleWare}} := {{template \"g\" .}}.Group(\"{{.Path}}\", {{.MiddleWare}}Mw()...) {{- end}} {{- range $_, $router := .Children}} {{- if ne .Handler \"\"}} {{template \"G\" $router}} {{- else}} {\t{{template \"G\" $router}} } {{- end}} {{- end}} {{- end}} // Register register routes based on the IDL 'api.${HTTP Method}' annotation. func Register(r *server.Hertz) { {{template \"G\" .Router}} } # path only represents register.go template, the path of register is fixed to biz/router/register.go - path: register.go delims: - \"\" - \"\" body: |- // Code generated by hertz generator. DO NOT EDIT. package router import ( \"github.com/cloudwego/hertz/pkg/app/server\" {{$.PkgAlias}} \"{{$.Pkg}}\" ) // GeneratedRegister registers routers generated by IDL. func GeneratedRegister(r *server.Hertz){ //INSERT_POINT: DO NOT DELETE THIS LINE! {{$.PkgAlias}}.Register(r) } - path: model.go delims: - \"\" - \"\" body: \"\" # path only represents middleware.go template, the path of middleware is the same as router.go: biz/router/namespace/ - path: middleware.go delims: - '{{' - '}}' body: |- // Code generated by hertz generator. package {{$.PackageName}} import ( \"github.com/cloudwego/hertz/pkg/app\" ) {{define \"M\"}} func {{.MiddleWare}}Mw() []app.HandlerFunc { // your code... return nil } {{range $_, $router := $.Children}}{{template \"M\" $router}}{{end}} {{- end}} {{template \"M\" .Router}} # path only represents the template of the client.go, the generation path of the client code is specified by the user \"${client_dir}\" - path: client.go delims: - '{{' - '}}' body: |- // Code generated by hertz generator. package {{$.PackageName}} import ( \"github.com/cloudwego/hertz/pkg/app/client\" \"github.com/cloudwego/hertz/pkg/common/config\" ) type {{.ServiceName}}Client struct { client * client.Client } func New{{.ServiceName}}Client(opt ...config.ClientOption) (*{{.ServiceName}}Client, error) { c, err := client.NewClient(opt...) if err != nil { return nil, err } return \u0026{{.ServiceName}}Client{ client: c, }, nil } # handler_single means a separate handler template, which is used to update each new handler when updating - path: handler_single.go delims: - '{{' - '}}' body: |+ {{.Comment}} func {{.Name}}(ctx context.Context, c *app.RequestContext) { // this my demo var err error {{if ne .RequestTypeName \"\" -}} var req {{.RequestTypeName}} err = c.BindAndValidate(\u0026req) if err != nil { c.String(400, err.Error()) return } {{end}} resp := new({{.ReturnTypeName}}) c.{{.Serializer}}(200, resp) } # middleware_single means a separate middleware template, used to update each new middleware when updating - path: middleware_single.go delims: - '{{' - '}}' body: |+ func {{.MiddleWare}}Mw() []app.HandlerFunc { // your code... return nil } Customize a package template  Like layout templates, users can also customize package templates.\nAs far as the templates provided by the package are concerned, the average user may only need to customize handler.go templates, because router.go/middleware.go/register.go are generally related to the idl definition and the user does not need to care, so hz currently also fixes the location of these templates, and generally does not need to be modified.\nTherefore, users can customize the generated handler template according to their own needs to speed up development; however, since the default handler template integrates some model information and package information, the hz tool is required to provide rendering data. This part of the user can modify it according to their own situation, and it is generally recommended to leave model information.\n Here is an example of a simple custom handler template, adding some comments to the handler:\ntemplate: layouts: # Path only represents handler.go template, the specific handler path is determined by the default path and handler_dir - path: handler.go delims: - '{{' - '}}' body: |- // this is my custom handler. package {{.PackageName}} import ( \"context\" \"github.com/cloudwego/hertz/pkg/app\" {{- range $k, $v := .Imports}} {{$k}} \"{{$v.Package}}\" {{- end}} ) {{range $_, $MethodInfo := .Methods}} {{$MethodInfo.Comment}} func {{$MethodInfo.Name}}(ctx context.Context, c *app.RequestContext) { // you can code something var err error {{if ne $MethodInfo.RequestTypeName \"\" -}} var req {{$MethodInfo.RequestTypeName}} err = c.BindAndValidate(\u0026req) if err != nil { c.String(400, err.Error()) return } {{end}} resp := new({{$MethodInfo.ReturnTypeName}}) c.{{.Serializer}}(200, resp) } {{end}} # path only represents router.go template, its path is fixed at: biz/router/namespace/ - path: router.go delims: - '{{' - '}}' body: |- // Code generated by hertz generator. DO NOT EDIT. package {{$.PackageName}} import ( \"github.com/cloudwego/hertz/pkg/app/server\" {{range $k, $v := .HandlerPackages}}{{$k}} \"{{$v}}\"{{end}} ) /* This file will register all the routes of the services in the master idl. And it will update automatically when you use the \"update\" command for the idl. So don't modify the contents of the file, or your code will be deleted when it is updated. */ {{define \"g\"}} {{- if eq .Path \"/\"}}r {{- else}}{{.GroupName}}{{end}} {{- end}} {{define \"G\"}} {{- if ne .Handler \"\"}} {{- .GroupName}}.{{.HttpMethod}}(\"{{.Path}}\", append({{.MiddleWare}}Mw(), {{.Handler}})...) {{- end}} {{- if ne (len .Children) 0}} {{.MiddleWare}} := {{template \"g\" .}}.Group(\"{{.Path}}\", {{.MiddleWare}}Mw()...) {{- end}} {{- range $_, $router := .Children}} {{- if ne .Handler \"\"}} {{template \"G\" $router}} {{- else}} {\t{{template \"G\" $router}} } {{- end}} {{- end}} {{- end}} // Register register routes based on the IDL 'api.${HTTP Method}' annotation. func Register(r *server.Hertz) { {{template \"G\" .Router}} } # path only represents register.go template, the path of register is fixed to biz/router/register.go - path: register.go delims: - \"\" - \"\" body: |- // Code generated by hertz generator. DO NOT EDIT. package router import ( \"github.com/cloudwego/hertz/pkg/app/server\" {{$.PkgAlias}} \"{{$.Pkg}}\" ) // GeneratedRegister registers routers generated by IDL. func GeneratedRegister(r *server.Hertz){ //INSERT_POINT: DO NOT DELETE THIS LINE! {{$.PkgAlias}}.Register(r) } - path: model.go delims: - \"\" - \"\" body: \"\" # path only represents middleware.go template, the path of middleware is the same as router.go: biz/router/namespace/ - path: middleware.go delims: - '{{' - '}}' body: |- // Code generated by hertz generator. package {{$.PackageName}} import ( \"github.com/cloudwego/hertz/pkg/app\" ) {{define \"M\"}} func {{.MiddleWare}}Mw() []app.HandlerFunc { // your code... return nil } {{range $_, $router := $.Children}}{{template \"M\" $router}}{{end}} {{- end}} {{template \"M\" .Router}} # path only represents the template of the client.go, the generation path of the client code is specified by the user \"${client_dir}\" - path: client.go delims: - '{{' - '}}' body: |- // Code generated by hertz generator. package {{$.PackageName}} import ( \"github.com/cloudwego/hertz/pkg/app/client\" \"github.com/cloudwego/hertz/pkg/common/config\" ) type {{.ServiceName}}Client struct { client * client.Client } func New{{.ServiceName}}Client(opt ...config.ClientOption) (*{{.ServiceName}}Client, error) { c, err := client.NewClient(opt...) if err != nil { return nil, err } return \u0026{{.ServiceName}}Client{ client: c, }, nil } # handler_single means a separate handler template, which is used to update each new handler when updating - path: handler_single.go delims: - '{{' - '}}' body: |+ {{.Comment}} func {{.Name}}(ctx context.Context, c *app.RequestContext) { // this my demo var err error {{if ne .RequestTypeName \"\" -}} var req {{.RequestTypeName}} err = c.BindAndValidate(\u0026req) if err != nil { c.String(400, err.Error()) return } {{end}} resp := new({{.ReturnTypeName}}) c.{{.Serializer}}(200, resp) } # middleware_single means a separate middleware template, used to update each new middleware when updating - path: middleware_single.go delims: - '{{' - '}}' body: |+ func {{.MiddleWare}}Mw() []app.HandlerFunc { // your code... return nil } Command:\n# After that, the package template rendering data will be provided, so the form of \"k-v\" is retained when entering the command, and \":\" needs to be added after customize_package. hz new --mod=github.com/hertz/hello --handler_dir=handler_test --idl=hertzDemo/hello.thrift --customize_package=template/package.yaml: The handler is generated as follows:\n// this is my custom handler. package example import ( \"context\" \"github.com/cloudwego/hertz/pkg/app\" example \"test/test2/biz/model/hello/example\" ) // HelloMethod . // @router /hello [GET] func HelloMethod(ctx context.Context, c *app.RequestContext) { // you can code something var err error var req example.HelloReq err = c.BindAndValidate(\u0026req) if err != nil { c.String(400, err.Error()) return } resp := new(example.HelloResp) c.JSON(200, resp) } // OtherMethod . // @router /other [POST] func OtherMethod(ctx context.Context, c *app.RequestContext) { // you can code something var err error var req example.OtherReq err = c.BindAndValidate(\u0026req) if err != nil { c.String(400, err.Error()) return } resp := new(example.OtherResp) c.JSON(200, resp) } Precautions Precautions for using layout templates When the user uses the layout custom template, the generated layout and rendering data are taken over by the user, so the user needs to provide the rendering data of the defined layout.\nPrecautions for using package templates Generally speaking, when users use package templates, most of them are to modify the default handler template; however, hz does not provide a single handler template at present, so when updating an existing handler file, the default handler template will be used to append a new handler function to the end of the handler file. When the corresponding handler file does not exist, a custom template will be used to generate the handler file.\n","categories":"","description":"","excerpt":"Hertz provides command-line tools (hz) that support custom template …","ref":"/docs/hertz/tutorials/toolkit/template/","tags":"","title":"hz custom template use"},{"body":"Hertz provides logger extension, and the interface is defined in pkg/common/hlog.\nInterface Definition In Hertz, the interfaces Logger, CtxLogger, FormatLogger are defined in pkg/common/hlog, and these interfaces are used to output logs in different ways, and a Control interface is defined to control the logger. If you’d like to inject your own logger implementation, you must implement all the above interfaces (i.e. FullLogger). Hertz already provides a default implementation of FullLogger.\n// FullLogger is the combination of Logger, FormatLogger, CtxLogger and Control. type FullLogger interface { Logger FormatLogger CtxLogger Control } Note that the default logger makes use of the standard library log.Logger as its underlying output. So the filenames and line numbers shown in the log messages depend on the settings of call depth. Thus wrapping the implementation of hlog may cause inaccuracies for these two values.\nInject your own logger Hertz provides SetLogger interface to allow injection of your own logger. Besides, SetOutput can be used to redirect the default logger output, and then middlewares and the other components of the framework can use global methods in hlog to output logs. By default, Hertz’s default logger is used.\n","categories":"","description":"","excerpt":"Hertz provides logger extension, and the interface is defined in …","ref":"/docs/hertz/tutorials/framework-exten/log/","tags":"","title":"Logger Extension"},{"body":"Usage Add some options when creating a server：\nsvr := api.NewServer(new(DemoImpl), server.WithXXX...) Basic Options WithServerBasicInfo func WithServerBasicInfo(ebi *rpcinfo.EndpointBasicInfo) Option Set the basic infos for server, such as ServiceName, Method and Tags.\nWithServiceAddr func WithServiceAddr(addr net.Addr) Option Set the listen address for server. Default port is 8888, you can reset the port to 9999 like this:\naddr, _ := net.ResolveTCPAddr(\"tcp\", \"127.0.0.1:9999\") svr := api.NewServer(new(HelloImpl), server.WithServiceAddr(addr)) When local server has multiple IP addresses, you can also use this method to specify them.\nWithMuxTransport func WithMuxTransport() Option Enable Kitex multiplexing transport feature on the server side. Client side also need to turn on this option, or it won’t work. More\nWithMiddleware func WithMiddleware(mw endpoint.Middleware) Option Add a middleware. More\nWithMiddlewareBuilder func WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder, funcName ...string) Option Add middleware depends on the context passed in by the framework that contains runtime configuration information (the context of non-RPC calls), so that the middleware can take advantage of the framework’s information when initializing.\nWithLimit func WithLimit(lim *limit.Option) Option Set the throttling threshold, which allows you to set a limit on QPS and the number of connections, which uses a built-in throttling implementation that can be scaled if there is a custom throttling requirement, integrating your own throttling policy via WithConcurrencyLimiter or WithQPSLimiter.\nWithReadWriteTimeout func WithReadWriteTimeout(d time.Duration) Option Set the server-side read and write timeout.\nNote: This feature may be changed or removed in subsequent releases.\nWithExitWaitTime func WithExitWaitTime(timeout time.Duration) Option Set the wait time for graceful shutdown of graceful shutdown on the server side.\nWithMaxConnIdleTime func WithMaxConnIdleTime(timeout time.Duration) Option Set the maximum amount of idle time allowed for the server-side connection to the client.\nWithStatsLevel func WithStatsLevel(level stats.Level) Option Set the stats level for the server. More\ngRPC Options  These options only works for scenarios where the transport protocol uses gRPC, with some parameter adjustments to gRPC transfers.\n WithGRPCWriteBufferSize func WithGRPCWriteBufferSize(s uint32) Option WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire. The corresponding memory allocation for this buffer will be twice the size to keep syscalls low. The default value for this buffer is 32KB. Zero will disable the write buffer such that each write will be on underlying connection. Note: A Send call may not directly translate to a write. It corresponds to the WriteBufferSize ServerOption of gRPC.\nWithGRPCReadBufferSize func WithGRPCReadBufferSize(s uint32) Option WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most for one read syscall. The default value for this buffer is 32KB. Zero will disable read buffer for a connection so data framer can access the underlying conn directly. It corresponds to the ReadBufferSize ServerOption of gRPC.\nWithGRPCInitialWindowSize func WithGRPCInitialWindowSize(s uint32) Option WithGRPCInitialWindowSize returns a Option that sets window size for stream. The lower bound for window size is 64K and any value smaller than that will be ignored. It corresponds to the InitialWindowSize ServerOption of gRPC.\nWithGRPCInitialConnWindowSize func WithGRPCInitialConnWindowSize(s uint32) Option WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection. The lower bound for window size is 64K and any value smaller than that will be ignored. It corresponds to the InitialConnWindowSize ServerOption of gRPC.\nWithGRPCKeepaliveParams func WithGRPCKeepaliveParams(kp grpc.ServerKeepalive) Option WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server. It corresponds to the KeepaliveParams ServerOption of gRPC.\nWithGRPCKeepaliveEnforcementPolicy func WithGRPCKeepaliveEnforcementPolicy(kep grpc.EnforcementPolicy) Option WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server. It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\nWithGRPCMaxConcurrentStreams func WithGRPCMaxConcurrentStreams(n uint32) Option WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number of concurrent streams to each ServerTransport. It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\nWithGRPCMaxHeaderListSize func WithGRPCMaxHeaderListSize(s uint32) Option WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size of header list that the server is prepared to accept. It corresponds to the MaxHeaderListSize ServerOption of gRPC.\nAdvanced Options WithSuite func WithSuite(suite Suite) Option Set up a specific configuration, customize according to the scene, configure multiple options and middlewares combinations and encapsulations in the Suite. More\nWithProxy func WithProxy(p proxy.ReverseProxy) Option If the server has a proxy, such as Mesh Ingress, you can modify the listening address through this configuration to communicate with the proxy, such as in the proxy. ReverseProxy modifies to the uds address.\nWithRegistryInfo func WithRegistryInfo(info *registry.Info) Option Customize the registration information reported by the service. More\nWithGeneric func WithGeneric(g generic.Generic) Option Specify the generalization call type, which needs to be used in conjunction with the generalization Client/Server. More\nWithErrorHandler func WithErrorHandler(f func(error) error) Option Set the error handler function, which is executed after the server handler is executed and before the middleware executes.\nWithACLRules func WithACLRules(rules ...acl.RejectFunc) Option Set ACL permission access control, which is executed before service discovery. More\nWithExitSignal func WithExitSignal(f func() \u003c-chan error) Option Set the server exit signal. Kitex has a built-in implementation, if you need some customization can be implemented yourself.\nWithReusePort func WithReusePort(reuse bool) Option Set port reuse, that is, whether to enable the underlying TCP port multiplexing mechanism.\nExtended Options WithRegistry func WithRegistry(r registry.Registry) Option Specify a Registry for service discovery registration reporting. More\nWithTracer func WithTracer(c stats.Tracer) Option Add an additional Tracer. More\nWithCodec func WithCodec(c remote.Codec) Option Specify a Codec for scenarios that require custom protocol. More\nWithPayloadCodec func WithPayloadCodec(c remote.PayloadCodec) Option Specifie a PayloadCodec. More\nWithMetaHandler func WithMetaHandler(h remote.MetaHandler) Option Add a meta handler for customizing transparent information in conjunction with the transport protocol, such as service name, invocation method, machine room, cluster, env, tracerInfo. More\nWithBoundHandler func WithBoundHandler(h remote.BoundHandler) Option Set IO Bound handlers. More\nWithConcurrencyLimiter func WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option Set the concurrency limit for server.\nWithQPSLimiter func WithQPSLimiter(qpsLimit limiter.RateLimiter) Option Set the QPS limit for server.\nWithLimitReporter func WithLimitReporter(r limiter.LimitReporter) Option Set LimitReporter, and when QPS throttling or connection limiting occurs, you can customize the escalation through LimitReporter.\nWithTransHandlerFactory func WithTransHandlerFactory(f remote.ServerTransHandlerFactory) Option Set transHandlerFactory. More\nWithTransServerFactory func WithTransServerFactory(f remote.TransServerFactory) Option Set transServerFactory. More\nWithDiagnosisService func WithDiagnosisService(ds diagnosis.Service) Option Set diagnosis service. More\n","categories":"","description":"","excerpt":"Usage Add some options when creating a server：\nsvr := …","ref":"/docs/kitex/tutorials/options/server_options/","tags":"","title":"Server Option"},{"body":"用法 在创建服务端时，带上 Option 参数即可：\nsvr := api.NewServer(new(DemoImpl), server.WithXXX...) 基础 Option 基本信息 - WithServerBasicInfo func WithServerBasicInfo(ebi *rpcinfo.EndpointBasicInfo) Option 设置 Server 侧的 RPC 调用基本信息，例如 ServiceName，Method 和 Tags。\n指定地址 - WithServiceAddr func WithServiceAddr(addr net.Addr) Option 指定服务端监听地址，默认是 8888 端口，配置示例-配置端口为 9999：\naddr, _ := net.ResolveTCPAddr(\"tcp\", \"127.0.0.1:9999\") svr := api.NewServer(new(HelloImpl), server.WithServiceAddr(addr)) 在遇到本机有多个 IP 地址时，例如服务发现等场景需要 内网/外网 IP 地址，也可以用这个方法进行指定。\n多路复用 - WithMuxTransport func WithMuxTransport() Option 服务端启用多路复用。需要配合客户端的同时开启，详见连接类型-连接多路复用。\n中间件扩展 - WithMiddleware func WithMiddleware(mw endpoint.Middleware) Option 添加一个中间件，使用方式和 client 一致。用法参考 Middleware 扩展。\n中间件扩展 - WithMiddlewareBuilder func WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder, funcName ...string) Option 用于创建并添加中间件，可以根据 ctx 判断场景并创建中间件。 ctx 是框架传入的包含运行时配置信息的上下文（非 RPC 调用的上下文），以便中间件初始化时能利用框架的信息。\n限流控制 - WithLimit func WithLimit(lim *limit.Option) Option 设置限流阈值，可以设置对 QPS 和连接数的限制，该配置使用内置的限流实现，如果有定制的限流需求可以自行扩展，通过 WithConcurrencyLimiter 或者 WithQPSLimiter 集成自己的限流策略。\n超时设置 - WithReadWriteTimeout func WithReadWriteTimeout(d time.Duration) Option 设置服务端读写超时的时间。\n注意：这个功能在后续版本中可能会有改动或者删除。\n退出等待 - WithExitWaitTime func WithExitWaitTime(timeout time.Duration) Option 设置服务端 Graceful Shutdown 优雅关闭的等待的时间。\n连接闲置设置 - WithMaxConnIdleTime func WithMaxConnIdleTime(timeout time.Duration) Option 设置服务端对客户端连接的最大允许空闲的时间。\n埋点粒度 - WithStatsLevel func WithStatsLevel(level stats.Level) Option 为 Server 设置埋点粒度，详见埋点粒度。\ngRPC 相关配置  这类设置只对传输协议使用 gRPC 的场景生效，对 gRPC 传输进行一些参数调整。\n WithGRPCWriteBufferSize func WithGRPCWriteBufferSize(s uint32) Option 设置 gRPC 写缓冲大小，写缓冲决定了每次批量调用底层写发送数据的大小。默认值为32KB，如果设置为0，则相当于禁用缓冲区，每次写操作都直接调用底层连接进行发送。该设置只对传输协议使用 gRPC 的场景生效。\nWithGRPCReadBufferSize func WithGRPCReadBufferSize(s uint32) Option 设置 gRPC 的读缓冲大小，读缓冲决定了每次批量从底层读取多少数据。默认值为32KB，如果设置为0，则相当于禁用缓冲区，每次读操作都直接从底层连接进行读操作。该设置只对传输协议使用 gRPC 的场景生效。\nWithGRPCInitialWindowSize func WithGRPCInitialWindowSize(s uint32) Option 设置 gRPC 每个 Stream 的初始收发窗口大小，最低为64KB，若设置的值小于最低值，则会被忽略。该设置只对传输协议使用 gRPC 的场景生效。\nWithGRPCInitialConnWindowSize func WithGRPCInitialConnWindowSize(s uint32) Option 设置 gRPC 单条连接上的初始窗口大小，最低为64KB，若设置的值小于最低值，则会被忽略。该设置只对传输协议使用 gRPC 的场景生效。\nWithGRPCKeepaliveParams func WithGRPCKeepaliveParams(kp grpc.ServerKeepalive) Option 设置 gRPC 服务端 Keepalive 的各项参数。该设置只对传输协议使用 gRPC 的场景生效。\nWithGRPCKeepaliveEnforcementPolicy func WithGRPCKeepaliveEnforcementPolicy(kep grpc.EnforcementPolicy) Option 设置 gRPC 服务端 Keepalive 里对于客户端策略的一些检查标准。\nWithGRPCMaxConcurrentStreams func WithGRPCMaxConcurrentStreams(n uint32) Option 设置 gRPC 服务端最大能接受的 Stream 数量限制。\nWithGRPCMaxHeaderListSize func WithGRPCMaxHeaderListSize(s uint32) Option 设置 gRPC MaxHeaderListSize 参数，该参数决定了每次调用允许发送的header的最大条数。该设置只对传输协议使用 gRPC 的场景生效。\n高级 Option 配套扩展 - WithSuite func WithSuite(suite Suite) Option 设置一套特定配置，可根据场景进行定制，在 Suite 中配置多个 Option 和 Middleware 的组合和封装，详见 Suite 扩展。\n代理 - WithProxy func WithProxy(p proxy.ReverseProxy) Option 如果服务端有代理，如 Mesh Ingress，可以通过该配置修改监听地址，用于与 Proxy 通信，比如在 proxy.ReverseProxy 修改为 uds 地址。\n注册信息 - WithRegistryInfo func WithRegistryInfo(info *registry.Info) Option 自定义服务上报的注册信息，用法详见服务发现。\n泛化调用 - WithGeneric func WithGeneric(g generic.Generic) Option 指定泛化调用类型，泛化需要结合泛化 Client/Server 使用，详见 Kitex 泛化调用使用指南。\n异常处理 - WithErrorHandler func WithErrorHandler(f func(error) error) Option 设置异常处理函数，该函数会在服务端 handler 执行后，中间件执行前被执行。\n权限控制 - WithACLRules func WithACLRules(rules ...acl.RejectFunc) Option 设置 ACL 权限访问控制，该模块会在服务发现之前执行，具体用法详见自定义访问控制。\n退出信号 - WithExitSignal func WithExitSignal(f func() \u003c-chan error) Option 设置服务端退出信号，Kitex 有内置实现，如果需要一些定制可以自行实现。\n端口重用 - WithReusePort func WithReusePort(reuse bool) Option 设置端口重用，即是否开启底层的 TCP 端口复用机制。\n扩展 Option 服务发现 - WithRegistry func WithRegistry(r registry.Registry) Option 指定一个 Registry 进行服务发现的注册上报，用法详见服务发现。\n链路监控 - WithTracer func WithTracer(c stats.Tracer) Option 额外添加一个 Tracer 进行链路监控，详见链路跟踪-自定义 tracer。\n编解码 - WithCodec func WithCodec(c remote.Codec) Option 指定 Codec，用于需要自定义协议的场景，详见编解码协议扩展。\nPayload 编解码 - WithPayloadCodec func WithPayloadCodec(c remote.PayloadCodec) Option 指定 PayloadCodec，详见编解码协议扩展。\n元信息处理 - WithMetaHandler func WithMetaHandler(h remote.MetaHandler) Option 添加一个元信息处理器，用于结合传输协议定制透传信息，如服务名、调用方法、机房、集群、env、TracerInfo，用法详见元信息传递扩展。\nIO Bound 扩展 - WithBoundHandler func WithBoundHandler(h remote.BoundHandler) Option 自定义 IO Bound，详见 Transport Pipeline-Bound 扩展。\n并发限制 - WithConcurrencyLimiter func WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option 设置服务端的连接数限制。\nQPS 限制 - WithQPSLimiter func WithQPSLimiter(qpsLimit limiter.RateLimiter) Option 设置服务端的 QPS 限制。\n限流报告器 - WithLimitReporter func WithLimitReporter(r limiter.LimitReporter) Option 设置 LimitReporter，当发生 QPS 限流或连接数限流时，可以通过 LimitReporter 进行定制上报。\n传输扩展 - WithTransHandlerFactory func WithTransHandlerFactory(f remote.ServerTransHandlerFactory) Option 自定义传输模块，详见传输模块扩展。\n传输扩展 - WithTransServerFactory func WithTransServerFactory(f remote.TransServerFactory) Option 自定义传输模块，详见传输模块扩展。\n诊断扩展 - WithDiagnosisService func WithDiagnosisService(ds diagnosis.Service) Option 添加一个自定义的 DiagnosisService，用来获取更多的诊断信息，详见诊断模块扩展。\n","categories":"","description":"","excerpt":"用法 在创建服务端时，带上 Option 参数即可：\nsvr := api.NewServer(new(DemoImpl), …","ref":"/zh/docs/kitex/tutorials/options/server_options/","tags":"","title":"Server Option"},{"body":"Hertz 提供的命令行工具(以下称为\"hz\")支持自定义模板功能，包括：\n 自定义 layout 模板(即生成代码的目录结构) 自定义 package 模板(即与 service 相关的代码结构，包括 handler、router 等)  用户可自己提供模板以及渲染参数，并结合 hz 的能力，来完成自定义的代码生成结构。\n自定义 layout 模板  用户可根据默认模板来修改或重写，从而满足自身需求\n hz 利用了 go template 支持以 “yaml” 的格式定义模板，并使用 “json” 定义模板渲染数据。\n所谓的 layout 模板是指整个项目的结构，这些结构与具体的 idl 定义无关，不需要 idl 也可以直接生成，默认的结构如下：\n. ├── biz │ ├── handler │ │ └── ping.go │ │ └── ****.go // 按照服务划分的 handler 集合，位置可根据 handler_dir 改变 │ ├── model │ │ └── model.go // idl 生成的 struct，位置可根据 model_dir 改变 │ └── router // 未开发自定义 dir │ └── register.go // 路由注册，用来调用具体的路由注册 │ └── route.go // 具体路由注册位置 │ └── middleware.go // 默认 middleware 生成位置 ├── .hz // hz 创建代码标志 ├── go.mod ├── main.go // 启动入口 ├── router.go // 用户自定义路由写入位置 └── router_gen.go // hz 生成的路由注册调用 IDL // hello.thrift namespace go hello.example struct HelloReq { 1: string Name (api.query=\"name\"); } struct HelloResp { 1: string RespBody; } service HelloService { HelloResp HelloMethod(1: HelloReq request) (api.get=\"/hello\"); } 命令 hz new --mod=github.com/hertz/hello --idl=./hertzDemo/hello.thrift --customize_layout=template/layout.yaml:template/data.json 默认 layout 模板的含义  注：以下的 body 均为 go template\n layouts: # 生成的 handler 的目录，只有目录下有文件才会生成 - path: biz/handler/ delims: - \"\" - \"\" body: \"\" # 生成的 model 的目录，只有目录下有文件才会生成 - path: biz/model/ delims: - \"\" - \"\" body: \"\" # 项目 main 文件， - path: main.go delims: - \"\" - \"\" body: |- // Code generated by hertz generator. package main import ( \"github.com/cloudwego/hertz/pkg/app/server\" ) func main() { h := server.Default() register(h) h.Spin() } # go.mod 文件，需要模板渲染数据{{.GoModule}}才能生成 - path: go.mod delims: - '{{' - '}}' body: |- module {{.GoModule}} {{- if .UseApacheThrift}} replace github.com/apache/thrift =\u003e github.com/apache/thrift v0.13.0 {{- end}} # .gitignore 文件 - path: .gitignore delims: - \"\" - \"\" body: \"*.o\\n*.a\\n*.so\\n_obj\\n_test\\n*.[568vq]\\n[568vq].out\\n*.cgo1.go\\n*.cgo2.c\\n_cgo_defun.c\\n_cgo_gotypes.go\\n_cgo_export.*\\n_testmain.go\\n*.exe\\n*.exe~\\n*.test\\n*.prof\\n*.rar\\n*.zip\\n*.gz\\n*.psd\\n*.bmd\\n*.cfg\\n*.pptx\\n*.log\\n*nohup.out\\n*settings.pyc\\n*.sublime-project\\n*.sublime-workspace\\n!.gitkeep\\n.DS_Store\\n/.idea\\n/.vscode\\n/output\\n*.local.yml\\ndumped_hertz_remote_config.json\\n\\t\\t \\ \" # .hz 文件，包含 hz 版本，是 hz 创建的项目的标志，不需要传渲染数据 - path: .hz delims: - '{{' - '}}' body: |- // Code generated by hz. DO NOT EDIT. hz version: {{.hzVersion}} # ping 自带 ping 的 handler - path: biz/handler/ping.go delims: - \"\" - \"\" body: |- // Code generated by hertz generator. package handler import ( \"context\" \"github.com/cloudwego/hertz/pkg/app\" \"github.com/cloudwego/hertz/pkg/common/utils\" ) // Ping . func Ping(ctx context.Context, c *app.RequestContext) { c.JSON(200, utils.H{ \"message\": \"pong\", }) } # 定义路由注册的文件，需要模板渲染数据{{.RouterPkgPath}}才能生成 - path: router_gen.go delims: - \"\" - \"\" body: |- // Code generated by hertz generator. DO NOT EDIT. package main import ( \"github.com/cloudwego/hertz/pkg/app/server\" router \"{{.RouterPkgPath}}\" ) // register registers all routers. func register(r *server.Hertz) { router.GeneratedRegister(r) customizedRegister(r) } # 自定义路由注册的文件 - path: router.go delims: - \"\" - \"\" body: |- // Code generated by hertz generator. package main import ( \"github.com/cloudwego/hertz/pkg/app/server\" handler \"{{.HandlerPkgPath}}\" ) // customizeRegister registers customize routers. func customizedRegister(r *server.Hertz){ r.GET(\"/ping\", handler.Ping) // your code ... } # 默认路由注册文件，不要修改 - path: biz/router/register.go delims: - \"\" - \"\" body: |- // Code generated by hertz generator. DO NOT EDIT. package router import ( \"github.com/cloudwego/hertz/pkg/app/server\" ) // GeneratedRegister registers routers generated by IDL. func GeneratedRegister(r *server.Hertz){ //INSERT_POINT: DO NOT DELETE THIS LINE! } 模板渲染参数文件的含义 当指定了自定义模板以及渲染数据后，此时命令行指定的选项将不会被作为渲染数据，因此，模板中的渲染数据需要用户自己定义。\nhz 使用了\"json\"来指定渲染数据，下面进行介绍\n{ // 全局的渲染参数 \"*\": { \"GoModule\": \"github.com/hz/test\", // 要和命令行指定的一致，否则后续生成model、handler等代码将使用命令行指定的mod，导致出现不一致。 \"ServiceName\": \"p.s.m\", // 要和命令行指定的一致 \"UseApacheThrift\": false // 根据是否使用\"thrift\"设置\"true\"/\"false\" }, // router_gen.go 路由注册的渲染数据， // \"biz/router\"指向默认idl注册的路由代码的module，不要修改 \"router_gen.go\": { \"RouterPkgPath\": \"github.com/hz/test/biz/router\" } } 自定义一个 layout 模板  目前，hz 生成的项目 layout 已经是一个 hertz 项目最最最基础的骨架了，所以不建议删除现有的模板里的文件。\n不过如果用户想要一个别的 layout ，当然也可以根据自身需求来删除相应的文件(除\"biz/register.go\"外，其余都可以动)\n我们十分欢迎用户来贡献自己的模板\n 下面假设用户只想要 “main.go” 以及 “go.mod” 文件，那么我们对默认模板进行修改，如下：\ntemplate: // layout.yaml layouts: # 项目 main 文件， - path: main.go delims: - \"\" - \"\" body: |- // Code generated by hertz generator. package main import ( \"github.com/cloudwego/hertz/pkg/app/server\" \"{{.GoModule}}/biz/router\" ) func main() { h := server.Default() router.GeneratedRegister(h) // do what you wanted // add some render data: {{.MainData}} h.Spin() } # go.mod 文件，需要模板渲染数据{{.GoModule}}才能生成 - path: go.mod delims: - '{{' - '}}' body: |- module {{.GoModule}} {{- if .UseApacheThrift}} replace github.com/apache/thrift =\u003e github.com/apache/thrift v0.13.0 {{- end}} # 默认路由注册文件，没必要修改 - path: biz/router/register.go delims: - \"\" - \"\" body: |- // Code generated by hertz generator. DO NOT EDIT. package router import ( \"github.com/cloudwego/hertz/pkg/app/server\" ) // GeneratedRegister registers routers generated by IDL. func GeneratedRegister(r *server.Hertz){ //INSERT_POINT: DO NOT DELETE THIS LINE! } render data: { \"*\": { \"GoModule\": \"github.com/hertz/hello\", \"ServiceName\": \"hello\", \"UseApacheThrift\": true }, \"main.go\": { \"MainData\": \"this is customized render data\" } } 命令：\nhz new --mod=github.com/hertz/hello --idl=./hertzDemo/hello.thrift --customize_layout=template/layout.yaml:template/data.json 自定义 package 模板  hz 模板的模板地址：\n用户可根据默认模板来修改或重写，从而符合自身需求\n  所谓的 package 模板是指与 idl 定义相关的服务代码，这部分代码涉及到定义 idl 时指定的service、go_package/namespace等，主要包括以下几部分： handler.go : 处理函数逻辑 router.go：具体的 idl 定义的服务的路由注册逻辑 register.go：调用router.go中内容的逻辑 model代码：生成的 go struct；不过由于目前使用插件来生成model代码工具没权限来修改model的模板，所以这部分功能先不开放  命令 # 之后会提供 package 模板渲染数据，所以输入命令的时候先保留了\"k-v\"的形式，customize_package 后需要加\":\" hz new --mod=github.com/hertz/hello --handler_dir=handler_test --idl=hertzDemo/hello.thrift --customize_package=template/package.yaml: 默认 package 模板 注意：自定义 package 模板没有提供渲染数据的功能，这里主要是因为这些渲染数据是 hz 工具解析生成的，所以暂时不提供自己写渲染数据的功能。可以修改下模板里面与渲染数据无关的部分，以满足自身需求。\n# 以下数据都是 yaml marshal 得到的，所以可能看起来比较乱 layouts: # path只表示handler.go的模板，具体的handler路径由默认路径和handler_dir决定 - path: handler.go delims: - '{{' - '}}' body: |- // Code generated by hertz generator. package {{.PackageName}} import ( \"context\" \"github.com/cloudwego/hertz/pkg/app\" {{- range $k, $v := .Imports}} {{$k}} \"{{$v.Package}}\" {{- end}} ) {{range $_, $MethodInfo := .Methods}} {{$MethodInfo.Comment}} func {{$MethodInfo.Name}}(ctx context.Context, c *app.RequestContext) { var err error {{if ne $MethodInfo.RequestTypeName \"\" -}} var req {{$MethodInfo.RequestTypeName}} err = c.BindAndValidate(\u0026req) if err != nil { c.String(400, err.Error()) return } {{end}} resp := new({{$MethodInfo.ReturnTypeName}}) c.{{.Serializer}}(200, resp) } {{end}} # path只表示router.go的模板，其路径固定在：biz/router/namespace/ - path: router.go delims: - '{{' - '}}' body: |- // Code generated by hertz generator. DO NOT EDIT. package {{$.PackageName}} import ( \"github.com/cloudwego/hertz/pkg/app/server\" {{range $k, $v := .HandlerPackages}}{{$k}} \"{{$v}}\"{{end}} ) /* This file will register all the routes of the services in the master idl. And it will update automatically when you use the \"update\" command for the idl. So don't modify the contents of the file, or your code will be deleted when it is updated. */ {{define \"g\"}} {{- if eq .Path \"/\"}}r {{- else}}{{.GroupName}}{{end}} {{- end}} {{define \"G\"}} {{- if ne .Handler \"\"}} {{- .GroupName}}.{{.HttpMethod}}(\"{{.Path}}\", append({{.MiddleWare}}Mw(), {{.Handler}})...) {{- end}} {{- if ne (len .Children) 0}} {{.MiddleWare}} := {{template \"g\" .}}.Group(\"{{.Path}}\", {{.MiddleWare}}Mw()...) {{- end}} {{- range $_, $router := .Children}} {{- if ne .Handler \"\"}} {{template \"G\" $router}} {{- else}} {\t{{template \"G\" $router}} } {{- end}} {{- end}} {{- end}} // Register register routes based on the IDL 'api.${HTTP Method}' annotation. func Register(r *server.Hertz) { {{template \"G\" .Router}} } # path只表示register.go的模板，register的路径固定为biz/router/register.go - path: register.go delims: - \"\" - \"\" body: |- // Code generated by hertz generator. DO NOT EDIT. package router import ( \"github.com/cloudwego/hertz/pkg/app/server\" {{$.PkgAlias}} \"{{$.Pkg}}\" ) // GeneratedRegister registers routers generated by IDL. func GeneratedRegister(r *server.Hertz){ //INSERT_POINT: DO NOT DELETE THIS LINE! {{$.PkgAlias}}.Register(r) } - path: model.go delims: - \"\" - \"\" body: \"\" # path只表示middleware.go的模板，middleware的路径和router.go一样为：biz/router/namespace/ - path: middleware.go delims: - '{{' - '}}' body: |- // Code generated by hertz generator. package {{$.PackageName}} import ( \"github.com/cloudwego/hertz/pkg/app\" ) {{define \"M\"}} func {{.MiddleWare}}Mw() []app.HandlerFunc { // your code... return nil } {{range $_, $router := $.Children}}{{template \"M\" $router}}{{end}} {{- end}} {{template \"M\" .Router}} # path只表示client.go的模板，client代码的生成路径由用户指定\"${client_dir}\" - path: client.go delims: - '{{' - '}}' body: |- // Code generated by hertz generator. package {{$.PackageName}} import ( \"github.com/cloudwego/hertz/pkg/app/client\" \"github.com/cloudwego/hertz/pkg/common/config\" ) type {{.ServiceName}}Client struct { client * client.Client } func New{{.ServiceName}}Client(opt ...config.ClientOption) (*{{.ServiceName}}Client, error) { c, err := client.NewClient(opt...) if err != nil { return nil, err } return \u0026{{.ServiceName}}Client{ client: c, }, nil } # handler_single表示单独的handler模板，用于update的时候更新每一个新增的handler - path: handler_single.go delims: - '{{' - '}}' body: |+ {{.Comment}} func {{.Name}}(ctx context.Context, c *app.RequestContext) { // this my demo var err error {{if ne .RequestTypeName \"\" -}} var req {{.RequestTypeName}} err = c.BindAndValidate(\u0026req) if err != nil { c.String(400, err.Error()) return } {{end}} resp := new({{.ReturnTypeName}}) c.{{.Serializer}}(200, resp) } # middleware_single表示单独的middleware模板，用于update的时候更新每一个新增的middleware_single - path: middleware_single.go delims: - '{{' - '}}' body: |+ func {{.MiddleWare}}Mw() []app.HandlerFunc { // your code... return nil } 自定义一个 package 模板  与 layout 模板一样，用户同样可以自定义 package 模板。\n就 package 提供的模板来说，一般用户可能只有自定义 handler.go 的模板的需求，因为router.go/middleware.go/register.go 一般与 idl 定义相关而用户无需关心，因此 hz 目前也将这些模板生成的位置固定了，一般也无需修改。\n因此，用户可根据自身的需求来自定义生成的 handler 模板，加速开发速度；但是由于默认的 handler 模板集成了一些 model 的信息以及 package 信息，所以需要 hz 工具来提供渲染数据。这部分用户可根据自身情况酌情来修改，一般建议留下 model 信息。\n 下面给出一个简单的自定义 handler 模板的示例，在 handler 里加入一些注释：\ntemplate: # 以下数据都是 yaml marshal 得到的，所以可能看起来比较乱 layouts: # path只表示handler.go的模板，具体的handler路径由默认路径和handler_dir决定 - path: handler.go delims: - '{{' - '}}' body: |- // this is my custom handler. package {{.PackageName}} import ( \"context\" \"github.com/cloudwego/hertz/pkg/app\" {{- range $k, $v := .Imports}} {{$k}} \"{{$v.Package}}\" {{- end}} ) {{range $_, $MethodInfo := .Methods}} {{$MethodInfo.Comment}} func {{$MethodInfo.Name}}(ctx context.Context, c *app.RequestContext) { // you can code something var err error {{if ne $MethodInfo.RequestTypeName \"\" -}} var req {{$MethodInfo.RequestTypeName}} err = c.BindAndValidate(\u0026req) if err != nil { c.String(400, err.Error()) return } {{end}} resp := new({{$MethodInfo.ReturnTypeName}}) c.{{.Serializer}}(200, resp) } {{end}} # path只表示router.go的模板，其路径固定在：biz/router/namespace/ - path: router.go delims: - '{{' - '}}' body: |- // Code generated by hertz generator. DO NOT EDIT. package {{$.PackageName}} import ( \"github.com/cloudwego/hertz/pkg/app/server\" {{range $k, $v := .HandlerPackages}}{{$k}} \"{{$v}}\"{{end}} ) /* This file will register all the routes of the services in the master idl. And it will update automatically when you use the \"update\" command for the idl. So don't modify the contents of the file, or your code will be deleted when it is updated. */ {{define \"g\"}} {{- if eq .Path \"/\"}}r {{- else}}{{.GroupName}}{{end}} {{- end}} {{define \"G\"}} {{- if ne .Handler \"\"}} {{- .GroupName}}.{{.HttpMethod}}(\"{{.Path}}\", append({{.MiddleWare}}Mw(), {{.Handler}})...) {{- end}} {{- if ne (len .Children) 0}} {{.MiddleWare}} := {{template \"g\" .}}.Group(\"{{.Path}}\", {{.MiddleWare}}Mw()...) {{- end}} {{- range $_, $router := .Children}} {{- if ne .Handler \"\"}} {{template \"G\" $router}} {{- else}} {\t{{template \"G\" $router}} } {{- end}} {{- end}} {{- end}} // Register register routes based on the IDL 'api.${HTTP Method}' annotation. func Register(r *server.Hertz) { {{template \"G\" .Router}} } # path只表示register.go的模板，register的路径固定为biz/router/register.go - path: register.go delims: - \"\" - \"\" body: |- // Code generated by hertz generator. DO NOT EDIT. package router import ( \"github.com/cloudwego/hertz/pkg/app/server\" {{$.PkgAlias}} \"{{$.Pkg}}\" ) // GeneratedRegister registers routers generated by IDL. func GeneratedRegister(r *server.Hertz){ //INSERT_POINT: DO NOT DELETE THIS LINE! {{$.PkgAlias}}.Register(r) } - path: model.go delims: - \"\" - \"\" body: \"\" # path只表示middleware.go的模板，middleware的路径和router.go一样为：biz/router/namespace/ - path: middleware.go delims: - '{{' - '}}' body: |- // Code generated by hertz generator. package {{$.PackageName}} import ( \"github.com/cloudwego/hertz/pkg/app\" ) {{define \"M\"}} func {{.MiddleWare}}Mw() []app.HandlerFunc { // your code... return nil } {{range $_, $router := $.Children}}{{template \"M\" $router}}{{end}} {{- end}} {{template \"M\" .Router}} # path只表示client.go的模板，client代码的生成路径由用户指定\"${client_dir}\" - path: client.go delims: - '{{' - '}}' body: |- // Code generated by hertz generator. package {{$.PackageName}} import ( \"github.com/cloudwego/hertz/pkg/app/client\" \"github.com/cloudwego/hertz/pkg/common/config\" ) type {{.ServiceName}}Client struct { client * client.Client } func New{{.ServiceName}}Client(opt ...config.ClientOption) (*{{.ServiceName}}Client, error) { c, err := client.NewClient(opt...) if err != nil { return nil, err } return \u0026{{.ServiceName}}Client{ client: c, }, nil } # handler_single表示单独的handler模板，用于update的时候更新每一个新增的handler - path: handler_single.go delims: - '{{' - '}}' body: |+ {{.Comment}} func {{.Name}}(ctx context.Context, c *app.RequestContext) { // this my demo var err error {{if ne .RequestTypeName \"\" -}} var req {{.RequestTypeName}} err = c.BindAndValidate(\u0026req) if err != nil { c.String(400, err.Error()) return } {{end}} resp := new({{.ReturnTypeName}}) c.{{.Serializer}}(200, resp) } # middleware_single表示单独的middleware模板，用于update的时候更新每一个新增的middleware_single - path: middleware_single.go delims: - '{{' - '}}' body: |+ func {{.MiddleWare}}Mw() []app.HandlerFunc { // your code... return nil } 命令：\n# 之后会提供 package 模板渲染数据，所以输入命令的时候先保留了\"k-v\"的形式，customize_package 后需要加\":\" hz new --mod=github.com/hertz/hello --handler_dir=handler_test --idl=hertzDemo/hello.thrift --customize_package=template/package.yaml: 生成handler如下：\n// this is my custom handler. package example import ( \"context\" \"github.com/cloudwego/hertz/pkg/app\" example \"test/test2/biz/model/hello/example\" ) // HelloMethod . // @router /hello [GET] func HelloMethod(ctx context.Context, c *app.RequestContext) { // you can code something var err error var req example.HelloReq err = c.BindAndValidate(\u0026req) if err != nil { c.String(400, err.Error()) return } resp := new(example.HelloResp) c.JSON(200, resp) } // OtherMethod . // @router /other [POST] func OtherMethod(ctx context.Context, c *app.RequestContext) { // you can code something var err error var req example.OtherReq err = c.BindAndValidate(\u0026req) if err != nil { c.String(400, err.Error()) return } resp := new(example.OtherResp) c.JSON(200, resp) } 注意事项 使用 layout 模板的注意事项 当用户使用了 layout 自定义模板后，那么生成的 layout 和渲染数据都由用户接管，所以用户需要提供其定义的 layout 的渲染数据。\n使用 package 模板的注意事项 一般来说，用户使用 package 模板的时候大多数是为了修改默认的 handler 模板；不过，目前 hz 没有提供单个 handler 的模板，所以当 update 已经存在的 handler 文件时，会使用默认 handler 模板在 handler 文件尾追加新的 handler function。当对应的 handler 文件不存在的时候，才会使用自定义模板来生成 handler 文件。\n","categories":"","description":"","excerpt":"Hertz 提供的命令行工具(以下称为\"hz\")支持自定义模板功能，包括：\n 自定义 layout 模板(即生成代码的目录结构) …","ref":"/zh/docs/hertz/tutorials/toolkit/template/","tags":"","title":"hz 自定义模板使用"},{"body":"In HTTP, Basic Access Authentication is a form of login authentication that allows web browsers or other client programs to provide credentials in the form of a username and password upon request. Hertz also provides an implementation of Basic Auth, referencing gin’s implementation.\nAs for usage, you may refer to hertz example and gin documentation 。\npackage main import ( \"context\" \"github.com/cloudwego/hertz/pkg/app\" \"github.com/cloudwego/hertz/pkg/app/middlewares/server/basic_auth\" \"github.com/cloudwego/hertz/pkg/app/server\" \"github.com/cloudwego/hertz/pkg/protocol/consts\" ) func main() { h := server.Default(server.WithHostPorts(\"127.0.0.1:8080\")) h.Use(basic_auth.BasicAuth(map[string]string{ \"test1\": \"value1\", \"test2\": \"value2\", })) h.GET(\"/basicAuth\", func(ctx context.Context, c *app.RequestContext) { c.String(consts.StatusOK, \"hello hertz\") }) h.Spin() } ","categories":"","description":"","excerpt":"In HTTP, Basic Access Authentication is a form of login authentication …","ref":"/docs/hertz/tutorials/basic-feature/middleware/basic-auth/","tags":"","title":"Basic Auth"},{"body":" This feature only works when using Thrift\n Usage Scenarios There could be decades method defined in service, but only one or two methods is needed by client. Combine Service provides a way to split one service into several services. For Example, there is a ExampleService\nserviceExampleService{ExampleResponseMethod0(3:ExampleRequestreq)ExampleResponseMethod1(3:ExampleRequestreq)ExampleResponseMethod2(3:ExampleRequestreq)}We can split it into three services:\nserviceExampleService0{ExampleResponseMethod0(3:ExampleRequestreq)}serviceExampleService1{ExampleResponseMethod1(3:ExampleRequestreq)}serviceExampleService2{ExampleResponseMethod2(3:ExampleRequestreq)}Client can use one of them to generate code.\nPractice root thrift:\nserviceExampleService0{ExampleResponseMethod0(3:ExampleRequestreq)}serviceExampleService1{ExampleResponseMethod1(3:ExampleRequestreq)}serviceExampleService2{ExampleResponseMethod2(3:ExampleRequestreq)}with --combine-service parameter, it will generate a new service named CombineService and also client/server code. It’s definition:\nserviceCombineService{ExampleResponseMethod0(3:ExampleRequestreq)ExampleResponseMethod1(3:ExampleRequestreq)ExampleResponseMethod2(3:ExampleRequestreq)}When used with -service at the same time, it will use CombineService to generate main package. Attention: CombineService just combine methods defined in services, it won’t generate code when method method name conflicts.\nTips: You can use extends to combine services defined in several thrift files. like:\nservice ExampleService0 extends thriftA.Service0 { } service ExampleService1 extends thriftB.Service1 { } service ExampleService2 extends thriftC.Service2 { } Example This feature only supports Thrift.\nFor Example, now there are 3 Services need to be combined, and the Thrift IDL file demo.thrift is as follow:\nnamespacegoapistructExampleRequest{1:stringmessage}structExampleResponse{1:stringmessage}serviceExampleService0{ExampleResponseMethod0(1:ExampleRequestreq)}serviceExampleService1{ExampleResponseMethod1(1:ExampleRequestreq)}serviceExampleService2{ExampleResponseMethod2(1:ExampleRequestreq)}Execute Kitex code generating command with --combine-service to combine these services:\nkitex --combine-service -service demo.kitex.combine demo.thrift The directory generated is like this:\n├── kitex_gen └── api ├── combineservice │ ├── client.go │ ├── combineservice.go │ ├── invoker.go │ └── server.go ├── demo.go ├── exampleservice0 │ ├── client.go │ ├── exampleservice0.go │ ├── invoker.go │ └── server.go ├── exampleservice1 │ ├── client.go │ ├── exampleservice1.go │ ├── invoker.go │ └── server.go ├── exampleservice2 │ ├── client.go │ ├── exampleservice2.go │ ├── invoker.go │ └── server.go ├── k-consts.go └── k-demo.go exampleservice0, exampleservice1 and exampleservice2 are normally generated codes.\ncombineservice is the code of the combined service generated by --combine-service, in which each method is an aggregation of another service, which can be used uniformly through this service.\nSo when the server starts, you only need to run the Service of this merged service, and you can run all the methods together:\nfunc main() { svr := api.NewServer(new(combineservice.CombineService)) err := svr.Run() if err != nil { log.Println(err.Error()) } } ","categories":"","description":"","excerpt":" This feature only works when using Thrift\n Usage Scenarios There …","ref":"/docs/kitex/tutorials/code-gen/combine_service/","tags":"","title":"Combine Service"},{"body":" 本功能仅支持 Thrift 场景\n 使用场景 有些服务提供了几十个方法，而对于调用方可能只请求其中一两个方法，为了避免这种大型 Service 带来的庞大的生成代码，Combine Service 可以让用户将原来一个 Service 的几十个方法拆分成多个 Service。比如原来的 Service 是：\nserviceExampleService{ExampleResponseMethod0(3:ExampleRequestreq)ExampleResponseMethod1(3:ExampleRequestreq)ExampleResponseMethod2(3:ExampleRequestreq)}用户 IDL 定义可以拆分为三个 Service：\nserviceExampleService0{ExampleResponseMethod0(3:ExampleRequestreq)}serviceExampleService1{ExampleResponseMethod1(3:ExampleRequestreq)}serviceExampleService2{ExampleResponseMethod2(3:ExampleRequestreq)}调用方可以只保留其中一个 Service 生成代码，方法名和参数保持一致不影响 RPC 调用。\n具体描述 当 root thrift 文件中存在形如下述定义时：\nserviceExampleService0{ExampleResponseMethod0(3:ExampleRequestreq)}serviceExampleService1{ExampleResponseMethod1(3:ExampleRequestreq)}serviceExampleService2{ExampleResponseMethod2(3:ExampleRequestreq)}带上--combine-service 参数后，会生成一个名为 CombineService 的新 service 及其对应的 client/server 代码。 其定义为：\nserviceCombineService{ExampleResponseMethod0(3:ExampleRequestreq)ExampleResponseMethod1(3:ExampleRequestreq)ExampleResponseMethod2(3:ExampleRequestreq)}当同时使用了-service 参数时，会使用 CombineService 作为 main package 中 server 对应的 service 。 注意： CombineService 只是 method 的聚合，因此当 method 名冲突时将无法生成 CombineService 。\nTips：\n配合 extends 关键字，可以实现跨文件的 CombineService\n如：\nservice ExampleService0 extends thriftA.Service0 { } service ExampleService1 extends thriftB.Service1 { } service ExampleService2 extends thriftC.Service2 { } 使用示例 本功能只支持 Thrift 场景。例如目前有三个 Service 需要合并，编写 Thrift IDL 文件 demo.thrift 如下：\nnamespacegoapistructExampleRequest{1:stringmessage}structExampleResponse{1:stringmessage}serviceExampleService0{ExampleResponseMethod0(1:ExampleRequestreq)}serviceExampleService1{ExampleResponseMethod1(1:ExampleRequestreq)}serviceExampleService2{ExampleResponseMethod2(1:ExampleRequestreq)}执行如下命令，添加 --combine-service 进行合并服务的代码生成：\nkitex --combine-service -service demo.kitex.combine demo.thrift 得到的生成内容如下：\n├── kitex_gen └── api ├── combineservice │ ├── client.go │ ├── combineservice.go │ ├── invoker.go │ └── server.go ├── demo.go ├── exampleservice0 │ ├── client.go │ ├── exampleservice0.go │ ├── invoker.go │ └── server.go ├── exampleservice1 │ ├── client.go │ ├── exampleservice1.go │ ├── invoker.go │ └── server.go ├── exampleservice2 │ ├── client.go │ ├── exampleservice2.go │ ├── invoker.go │ └── server.go ├── k-consts.go └── k-demo.go 其中，exampleservice0，exampleservice1，exampleservice2 都是正常生成的代码\n而 combineservice 则为 --combine-service 生成的合并服务的代码，其中各个方法都是对另外的 Service 进行的聚合，可以通过这个 Service 进行统一的使用。\n所以在服务端启动时，只需要运行这个合并服务的 Service，就可以将所有的方法一起运行：\nfunc main() { svr := api.NewServer(new(combineservice.CombineService)) err := svr.Run() if err != nil { log.Println(err.Error()) } } ","categories":"","description":"","excerpt":" 本功能仅支持 Thrift 场景\n 使用场景 有些服务提供了几十个方法，而对于调用方可能只请求其中一两个方法， …","ref":"/zh/docs/kitex/tutorials/code-gen/combine_service/","tags":"","title":"Combine Service"},{"body":"Server         Figure 1: middleware call chain    Implement customized middleware There are two ways to implement server side middleware:\n When the middleware only has pre-handle logic and there is no requirement to be in a function call stack with real handler, the .Next can be omitted. If there is other processing logic (post-handle) after the business handler, or there is a strong requirement for the function call chain (stack), then the .Next must be called explicitly, see middleware C in Figure 1.  // One way func MyMiddleware() app.HandlerFunc { return func(ctx context.Context, c *app.RequestContext) { // pre-handle  ... } } // The other way func MyMiddleware() app.HandlerFunc { return func(ctx context.Context, c *app.RequestContext) { // pre-handle  ... c.Next(ctx) // call the next middleware(handler)  // post-handle  ... } } If you want to terminate the middleware call quickly, you can use the following methods, noting that the current middleware will still execute.\n Abort()：terminate subsequent calls AbortWithMsg(msg string, statusCode int)：terminates subsequent calls and sets the body and status code for the Response AbortWithStatus(code int)：terminates subsequent calls and sets the status code  Register customized middleware h := server.Default() h.Use(MyMiddleware()) The Hertz framework currently supports middleware registration on Server, routing groups, and single routes, using the Use method.\nActivate default middleware The Hertz framework already presets the commonly used Recover middleware, which can be registered by Default with server.Default().\nMiddlewares we provide Hertz provides frequently-used middlewares such as BasicAuth, CORS, JWT…If you need others, please make an issue.\nClient-side Middleware Implement customized middleware The middleware implementation on the Client side is different from that on the Server side. The Client side cannot get the index of the middleware to increase, so the Client middleware uses nested functions to build the middleware in advance. When implementing client-side customized middleware, you can refer to the following code.\nfunc MyMiddleware(next client.Endpoint) client.Endpoint { return func(ctx context.Context, req *protocol.Request, resp *protocol.Response) (err error) { // pre-handle  ... err = next(ctx, req, resp) if err != nil { return } // post-handle  ... } } Note: the next method must be executed to continue calls to the subsequent middleware. If you want to stop the middleware call, just return before next.\nRegister customized middleware Registering custom middleware is the same as on the server side.\nc, err := client.NewClient() c.Use(MyMiddleware) ","categories":"","description":"","excerpt":"Server         Figure 1: middleware call chain    Implement customized …","ref":"/docs/hertz/tutorials/basic-feature/middleware/","tags":"","title":"Middleware Overview"},{"body":"在 HTTP 中，基本认证（Basic access authentication）是一种用来允许网页浏览器或其他客户端程序在请求时提供用户名和口令形式的身份凭证的一种登录验证方式。 hertz 也提供了 basic auth 的实现 ，参考了 gin 的实现 。\n使用方法可参考如下 example 以及 gin 的文档 。\npackage main import ( \"context\" \"github.com/cloudwego/hertz/pkg/app\" \"github.com/cloudwego/hertz/pkg/app/middlewares/server/basic_auth\" \"github.com/cloudwego/hertz/pkg/app/server\" \"github.com/cloudwego/hertz/pkg/protocol/consts\" ) func main() { h := server.Default(server.WithHostPorts(\"127.0.0.1:8080\")) h.Use(basic_auth.BasicAuth(map[string]string{ \"test1\": \"value1\", \"test2\": \"value2\", })) h.GET(\"/basicAuth\", func(ctx context.Context, c *app.RequestContext) { c.String(consts.StatusOK, \"hello hertz\") }) h.Spin() } ","categories":"","description":"","excerpt":"在 HTTP 中，基本认证（Basic access authentication）是一种用来允许网页浏览器或其他客户端程序在请求时提供用户 …","ref":"/zh/docs/hertz/tutorials/basic-feature/middleware/basic-auth/","tags":"","title":"基本认证"},{"body":"Server         Figure 1: middleware call chain    实现一个中间件 Hertz 有两种方式实现中间件：\n 当中间件只有初始化（pre-handle）相关逻辑，且没有和 real handler 在一个函数调用栈中的需求时，中间件中可以省略掉最后的.Next，如图1的中间件 B。 如果在业务 handler 处理之后有其它处理逻辑（ post-handle ），或对函数调用链（栈）有强需求，则必须显示调用.Next，如图1的中间件 C。  // 方式一 func MyMiddleware() app.HandlerFunc { return func(ctx context.Context, c *app.RequestContext) { // pre-handle  ... } } // 方式二 func MyMiddleware() app.HandlerFunc { return func(ctx context.Context, c *app.RequestContext) { // pre-handle  ... c.Next(ctx) // call the next middleware(handler)  // post-handle  ... } } 如果想快速终止中间件调用，可以使用以下方法，注意当前中间件仍将执行。\n Abort()：终止后续调用 AbortWithMsg(msg string, statusCode int)：终止后续调用，并设置 response中body，和状态码 AbortWithStatus(code int)：终止后续调用，并设置状态码  注册一个中间件 h := server.Default() h.Use(MyMiddleware()) Hertz 框架目前支持在 Server、路由组、单一路由上注册中间件，使用 Use 方法即可注册。\n使用默认中间件 Hertz 框架已经预置了常用的 recover 中间件，使用 server.Default() 默认可以注册该中间件。\n常用中间件 Hertz 提供了常用的 BasicAuth、CORS、JWT等中间件，其他中间件如有需求，可提 issue 告诉我们。\nClient 实现一个中间件 Client 的中间件实现和 Server 不同。Client 侧无法拿到中间件 index 实现递增，因此 Client 中间件采用提前构建嵌套函数的形式实现中间件，在实现一个中间件时，可以参考下面的代码。\nfunc MyMiddleware(next client.Endpoint) client.Endpoint { return func(ctx context.Context, req *protocol.Request, resp *protocol.Response) (err error) { // pre-handle  ... err = next(ctx, req, resp) if err != nil { return } // post-handle  ... } } 注意：必须执行 next 方法才能继续调用后续中间件。如果想停止中间件调用，在 next 之前返回就可以了。\n注册一个中间件 注册中间件的方式和 Server 相同\nc, err := client.NewClient() c.Use(MyMiddleware) ","categories":"","description":"","excerpt":"Server         Figure 1: middleware call chain    实现一个中间件 Hertz 有两种方式实 …","ref":"/zh/docs/hertz/tutorials/basic-feature/middleware/","tags":"","title":"中间件概览"},{"body":"会议主题： CloudWeGo 社区会议 3.11\n参会人员： CoderPoet, liu-song, GuangmingLuo, Zheming Li, YangruiEmma, li-jin-gou, simon0-o, Dianjun Suo, jasondeng1997, lvnszn, baiyutang, Duslia, joway, Xuewu Jiang, AshleeT, yccpt.\n会前必读： http://www.cloudwego.io/; https://github.com/cloudwego\n议程 1 ：新人自我介绍 内容：社区新成员和首次参加社区会议的内部成员分别进行自我介绍，主要包含个人基本情况和历史贡献。\n议程 2 ：CloudWeGo 仓库介绍  对 CloudWeGo 主仓库进行了简要介绍，欢迎社区成员对仓库进行补充加强。例如：欢迎大家在 Kitex_examples 仓库提交一些 Business demo，例如电商、医疗等不同行业场景下的典型案例 。 Community 仓库：首先，Community 仓库刚成立不久，主要用于归档社区相关的材料，包括双周会的会议纪要（meeting_notes）和周报（weekly_report）。其次，也欢迎大家成为该仓库的正式成员，后续的活动可以第一时间通知到大家，便于大家参与到核心功能的讨论与开发。 Kitex-contrib 仓库：该仓库包含了各种扩展的对接实现，比如对接 Prometheus，对接Opentracing 等。其中，OpenTelementry 对接项目正处于提交 PR 的状态，欢迎大家参与到项目的共建和 review。  议程 3：社区后续工作介绍  源码解析和微服务实践解析系列文章志愿者筹集，以及宣传运营支持：譬如：我们可以通过开源中国等渠道去发布一些优质文章，欢迎大家在源码解析和微服务实践方面文章的投稿。 开放服务治理：后续会和其他的一些厂商以及开源社区共同合作，去完成服务治理标准的制定。 官网文档和页面优化：对 CloudWeGo 官网的 Document、About、Blog 和 Community 页面提出意见，进行优化。  议程 4： Kitex 3、4月 TO DO/DOING 事项介绍\n 性能优化   Kitex-gRPC Streaming 性能提升。 Protobuf 编解码性能优化，初步完成，完善边界 case 。 Frugal - 无生成代码的高性能动态 Thrift编解码库。  新特性支持   Thrift 泛化调用 新增对 Protobuf 的支持用于网关 Protobuf \u003c-\u003e Thrift 高性能的协议转换 重试:支持用户自定义异常重试 Proxyless 支持:完成服务发现/路由对 xds 接口扩展  功能优化:   重写连接池逻辑，支持更加优雅的空闲连接清理 增加字段 Size 校验  外部需求   连接预热、连接多路复用通知上游退出  Action Items\n Kitex 开源库单元测试补全任务：希望社区同学能够加入进行补全。有助于促进大家熟悉源代码，帮助大家的后续开发。重点需要补充的 package 后续会在 Kitex 仓库创建独立的 Issue, 欢迎大家认领。  补充单元测试原则\n  补充的单测必须是有意义的，验证某个逻辑的正确性，或者异常表现是否符合预期。\n  杜绝为了覆盖率而补全单测，宁可不加。\n  每个单测必须要有断言。\n  可以添加 mock 辅助单测。\n  建议单测通过注释明确验证的逻辑。\n  不要在单测代码里用 printf 等手段打日志人肉去检验。\n  议程 5：Q\u0026A Q：Kitex 啥时候支持 Thrift Streaming？\nA：Kitex 支持 Thrift streaming 我们刚开始是计划要做的，但是之后了解到目前没有应用场景，没有用户提出需要用到 Thrift Streaming， 因此，这个计划我们就搁置了。如果没有收到真实的业务场景需求，我们暂时不去安排这个功能支持。\nQ：Proxyless 支持这块是一个 doing 状态吗？\nA：之前是有一个同学在跟进，但是后来因为内部有其它事情处理就没有再继续做了。如果你感兴趣的话，可以加入进来一起支持。\nQ：字段 size 是说大包性能的问题么。类似拆包去分发？\nA：首先，大包这一块的问题，我们目前是在 v1.8.0版本，就支持了可以去自定义整个包的 size。 其次，字段 size 校验的话，有可能有时我们的包出现错误，这个时候如果我们没有去校验 size， 那在解码的过程中，会因为这个错误的 size 可能导致去分配很大的内存。所以我们想对这个字段 size 增加一个校验。\nQ：连接池优化是指高并发的时候，长连接变成短连接的问题吗？\nA：不是的。我们的连接池有一个空闲连接的策略，空闲连接是指你配置了空闲时间，那么到了这个空闲时间，你这个连接就应该被清理掉。但实际上目前不是这样的逻辑，目前是我在用到这个连接的时候，我发现这个连接可能已经达到了我的空闲时间了，然后我才会把它给清理掉，这个是不合理的。基于此，我们打算重写这块的逻辑。\n","categories":"","description":"","excerpt":"会议主题： CloudWeGo 社区会议 3.11\n参会人员： CoderPoet, liu-song, GuangmingLuo, …","ref":"/zh/community/meeting_notes/2022-03-11/","tags":"","title":"CloudWeGo 社区会议 3.11"},{"body":"Meta Information As an RPC framework, Kitex services communicate with each other through protocols described by IDL (thrift, protobuf, etc.). The interface defined in an IDL determines the data structures that could be transmitted between the client and the server.\nHowever, in the production environment, we somehow may need to send special information to a remote server and that information is temporary or has an unstable format which can not be explicitly defined in the IDL. Such a situation requests the framework to be capable of sending meta information.\nWhen the underlying transport protocol supports (such as TTHeader, HTTP), then Kitex can transmit meta information.\nTo decouple with the underlying transport protocols, and interoperate with other frameworks, Kitex does not provide APIs to read or write meta information directly. Instead, it uses a stand-alone library metainfo to support meta information transmitting.\nForward Meta Information Transmitting Package metainfo provides two kinds of API for sending meta information forward – transient and persistent. The former is for ordinary needs of sending meta information; while the later is used when the meta information needs to be kept and sent to the next service and on, like a log ID or a dying tag. Of course, the persistent APIs works only when the next service and its successors all supports the meta information tarnsmitting convention.\nA client side example:\nimport \"github.com/bytedance/gopkg/cloud/metainfo\" func main() { ... ctx := context.Background() cli := myservice.MustNewClient(...) req := myservice.NewSomeRequest() ctx = metainfo.WithValue(ctx, \"temp\", \"temp-value\") // attach the meta information to the context  ctx = metainfo.WithPersistentValue(ctx, \"logid\", \"12345\") // attach persistent meta information  resp, err := cli.SomeMethod(ctx, req) // pass the context as an argument  ... } A server side example:\nimport ( \"context\" \"github.com/bytedance/gopkg/cloud/metainfo\" ) var cli2 = myservice2.MustNewClient(...) // the client for next service  func (MyServiceImpl) SomeMethod(ctx context.Context, req *SomeRequest) (res *SomeResponse, err error) { temp, ok1 := metainfo.GetValue(ctx, \"temp\") logid, ok2 := metainfo.GetPersistentValue(ctx, \"logid\") if !(ok1 \u0026\u0026 ok2) { panic(\"It looks like the protocol does not support transmitting meta information\") } println(temp) // \"temp-value\"  println(logid) // \"12345\"  // if we need to call another service  req2 := myservice2.NewRequset() res2, err2 := cli2.SomeMethod2(ctx, req2) // pass the context to other service for the persistent meta information to be transmitted continuously  ... } Backward Meta Information Transmitting Some transport protocols also support backward meta information transmitting. So Kitex supports that through metainfo, too.\nA client side example:\nimport \"github.com/bytedance/gopkg/cloud/metainfo\" func main() { ... ctx := context.Background() cli := myservice.MustNewClient(...) req := myservice.NewSomeRequest() ctx = metainfo.WithBackwardValues(ctx) // mark the context to receive backward meta information  resp, err := cli.SomeMethod(ctx, req) // pass the context as an argument  if err == nil { val, ok := metainfo.RecvBackwardValue(ctx, \"something-from-server\") // receive the meta information from server side  println(val, ok) } ... } A server side example:\nimport ( \"context\" \"github.com/bytedance/gopkg/cloud/metainfo\" ) func (MyServiceImpl) SomeMethod(ctx context.Context, req *SomeRequest) (res *SomeResponse, err error) { ok := metainfo.SendBackwardValue(ctx, \"something-from-server\") if !ok { panic(\"It looks like the protocol does not support transmitting meta information backward\") } ... } ","categories":"","description":"","excerpt":"Meta Information As an RPC framework, Kitex services communicate with …","ref":"/docs/kitex/tutorials/advanced-feature/metainfo/","tags":"","title":"Metainfo"},{"body":"元信息 作为一个 RPC 框架，Kitex 服务之间的通信都是基于 IDL（thrift、protobuf 等）描述的协议进行的。IDL 定义的服务接口决定了客户端和服务端之间可以传输的数据结构。\n然而在实际生产环境，我们偶尔会有特殊的信息需要传递给对端服务，而又不希望将这些可能是临时或者格式不确定的内容显式定义在 IDL 里面，这就需要框架能够支持一定的元信息传递能力。\n如果底层传输协议支持（例如 TTheader、HTTP），那么 Kitex 可以进行元信息的透传。\n为了和底层的协议解耦，同时也为了支持与不同框架之间的互通，Kitex 并没有直接提供读写底层传输协议的元信息的 API，而是通过一个独立维护的基础库 metainfo 来支持元信息的传递。\n正向元信息传递 包 metainfo 提供了两种类型的正向元信息传递 API：临时的（transient）和持续的（persistent）。前者适用于通常的元信息传递的需求；后者是在对元信息有持续传递需求的场合下使用，例如日志 ID、染色等场合，当然，持续传递的前提是下游以及更下游的服务都是支持这一套数据透传的约定，例如都是 Kitex 服务。\n客户端的例子：\nimport \"github.com/bytedance/gopkg/cloud/metainfo\" func main() { ... ctx := context.Background() cli := myservice.MustNewClient(...) req := myservice.NewSomeRequest() ctx = metainfo.WithValue(ctx, \"temp\", \"temp-value\") // 附加元信息到 context 里  ctx = metainfo.WithPersistentValue(ctx, \"logid\", \"12345\") // 附加能持续透传的元信息  resp, err := cli.SomeMethod(ctx, req) // 将得到的 context 作为客户端的调用参数  ... } 服务端的例子：\nimport ( \"context\" \"github.com/bytedance/gopkg/cloud/metainfo\" ) var cli2 = myservice2.MustNewClient(...) // 更下游的服务的客户端  func (MyServiceImpl) SomeMethod(ctx context.Context, req *SomeRequest) (res *SomeResponse, err error) { temp, ok1 := metainfo.GetValue(ctx, \"temp\") logid, ok2 := metainfo.GetPersistentValue(ctx, \"logid\") if !(ok1 \u0026\u0026 ok2) { panic(\"It looks like the protocol does not support transmitting meta information\") } println(temp) // \"temp-value\"  println(logid) // \"12345\"  // 如果需要调用其他服务的话  req2 := myservice2.NewRequset() res2, err2 := cli2.SomeMethod2(ctx, req2) // 在调用其他服务时继续传递收到的 context，可以让持续的元信息继续传递下去  ... } 反向元信息传递 一些传输协议还支持反向的元数据传递，因此 Kitex 也利用 metainfo 做了支持。\n客户端的例子：\nimport \"github.com/bytedance/gopkg/cloud/metainfo\" func main() { ... ctx := context.Background() cli := myservice.MustNewClient(...) req := myservice.NewSomeRequest() ctx = metainfo.WithBackwardValues(ctx) // 标记要接收反向传递的数据的 context  resp, err := cli.SomeMethod(ctx, req) // 将得到的 context 作为客户端的调用参数  if err == nil { val, ok := metainfo.RecvBackwardValue(ctx, \"something-from-server\") // 获取服务端传回的元数据  println(val, ok) } ... } 服务端的例子：\nimport ( \"context\" \"github.com/bytedance/gopkg/cloud/metainfo\" ) func (MyServiceImpl) SomeMethod(ctx context.Context, req *SomeRequest) (res *SomeResponse, err error) { ok := metainfo.SendBackwardValue(ctx, \"something-from-server\") if !ok { panic(\"It looks like the protocol does not support transmitting meta information backward\") } ... } ","categories":"","description":"","excerpt":"元信息 作为一个 RPC 框架，Kitex 服务之间的通信都是基于 IDL（thrift、protobuf 等）描述的协议进行的。IDL 定 …","ref":"/zh/docs/kitex/tutorials/advanced-feature/metainfo/","tags":"","title":"Metainfo"},{"body":"The framework itself doesn‘t providing any monitoring, but only provides a Tracer interface, which users can implement it and inject by WithTracer Option.\n// Tracer is executed at the start and finish of an RPC. type Tracer interface { Start(ctx context.Context) context.Context Finish(ctx context.Context) } The monitoring extension of prometheus is provided in kitex-contrib, usage example:\nClient Side:\nimport ( \"github.com/kitex-contrib/monitor-prometheus\" kClient \"github.com/cloudwego/kitex/client\" ) ... client, _ := testClient.NewClient( \"DestServiceName\", kClient.WithTracer(prometheus.NewClientTracer(\":9091\", \"/kitexclient\"))) resp, _ := client.Send(ctx, req) ... Server Side:\nimport ( \"github.com/kitex-contrib/monitor-prometheus\" kServer \"github.com/cloudwego/kitex/server\" ) func main() {... svr := xxxservice.NewServer( \u0026myServiceImpl{}, kServer.WithTracer(prometheus.NewServerTracer(\":9092\", \"/kitexserver\"))) svr.Run() ... } ","categories":"","description":"","excerpt":"The framework itself doesn‘t providing any monitoring, but only …","ref":"/docs/kitex/tutorials/service-governance/monitoring/","tags":"","title":"Monitoring"},{"body":"Kitex supports two serialization protocols: Thrift and Protobuf.\nThrift Kitex only support Thrift Binary protocol codec, Compact currently is not supported.\nIf you are using thrift protocol encoding, codes should be generate by kitex cmd:\nClient side：\nkitex -type thrift ${service_name} ${idl_name}.thrift Server side:\nkitex -type thrift -service ${service_name} ${idl_name}.thrift We have optimized Thrift’s Binary protocol codec. For details of the optimization, please refer to the “Reference - High Performance Thrift Codec” chapter. If you want to close these optimizations, you can add the -no-fast-api argument when generating code.\nProtobuf Protocol Type Kitex supports two types of protocol for protobuf:\n Custom message protocol: it’s been considered as kitex protobuf, the way of generated code is consistent with Thrift. gRPC protocol: it can communication with grpc directly, and support streaming.  If the streaming method is defined in the IDL, the serialization protocol would adopt gRPC protocol, otherwise Kitex protobuf would be adopted. If you want using gRPC protocol, but without stream definition in your proto file, you need specify the transport protocol when initializing client (No changes need to be made on the server because protocol detection is supported)：\n// Using WithTransportProtocol specify the transport protocol cli, err := service.NewClient(destService, client.WithTransportProtocol(transport.GRPC)) Generated Code Only support proto3, the grammar reference: https://developers.google.com/protocol-buffers/docs/gotutorial.\nNotice:\n What is different from other languages, generating go codes must define go_package in the proto file Instead of the full path, just using go_package specify the package name, such as: go_package = “pbdemo” Download the protoc binary and put it in the $PATH directory  Client side：\nkitex -type protobuf -I idl/ idl/${proto_name}.proto Server side:\nkitex -type protobuf -service ${service_name} -I idl/ idl/${proto_name}.proto ","categories":"","description":"","excerpt":"Kitex supports two serialization protocols: Thrift and Protobuf. …","ref":"/docs/kitex/tutorials/basic-feature/serialization_protocol/","tags":"","title":"Serialization Protocol"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kitex/tutorials/service-governance/","tags":"","title":"Service Governance"},{"body":"Encapsulating Custom Governance Modules Suite is a high-level abstraction of extensions, a combination and encapsulation of Option and Middleware.\nAs mentioned in the middleware extensions document, there are two principles should be remembered in extensions:\n Middleware and Suit are only allowed to be set before initializing Server and Client, do not allow modified dynamically. Behind override ahead.  These tow principle is also valid for Suite.\nSuite is defined as follows:\ntype Suite interface { Options() []Option } // TODO: Add example.\nBoth Server side and Client side use the WithSuite method to enable new Suite.\nWhen initializing Server and Client, Suite is setup in DFS(Deep First Search) way.\nFor example, if we have the following code:\ntype s1 struct { timeout time.Duration } func (s s1) Options() []client.Option { return []client.Option { client.WithRPCTimeout(s.timeout)} } type s2 struct { } func (s2) Options() []client.Option { return []client.Option{client.WithSuite(s1{timeout:1*time.Second}), client.WithRPCTimeout(2*time.Second)} } Then if we use client.WithSuite(s2{}), client.WithRPCTimeout(3*time.Second), it will execute client.WithSuite(s1{}) first, followed by client. WithRPCTimeout(1*time.Second), followed by client.WithRPCTimeout(2*time.Second), and finally client.WithRPCTimeout(3*time.Second). After this initialization, the value of RPCTimeout will be set to 3s (see the principle described at the beginning).\nSummary Suite is a higher-level combination and encapsulation, and it is recommended that third-party developers provide Kitex extensions based on Suite. Suite allows dynamically injecting values at creation time, or dynamically specifying values in its own middleware at runtime, making it easier for users and third-party developers to use and develop without relying on global variables, and making it possible to use different configurations for each client.\n","categories":"","description":"","excerpt":"Encapsulating Custom Governance Modules Suite is a high-level …","ref":"/docs/kitex/tutorials/framework-exten/suite/","tags":"","title":"Suite Extensions"},{"body":"Suite 扩展 - 封装自定义治理模块 Suite（套件）是一种对于扩展的高级抽象，可以理解为是对于 Option 和 Middleware 的组合和封装。\n在 middleware 扩展一文中我们有说到，在扩展过程中，要记得两点原则：\n 中间件和套件都只允许在初始化 Server、Client 的时候设置，不允许动态修改。 后设置的会覆盖先设置的。  这个原则针对 Suite 也是一样有效的。\nSuite 的定义如下：\ntype Suite interface { Options() []Option } 这也是为什么说，Suite 是对于 Option 和 Middleware（通过 Option 设置）的组合和封装。\n// TODO: 增加示例。\nServer 端和 Client 端都是通过 WithSuite 这个方法来启用新的套件。\n在初始化 Server 和 Client 的时候，Suite 是采用 DFS(Deep First Search) 方式进行设置。\n举个例子，假如我有以下代码：\ntype s1 struct { timeout time.Duration } func (s s1) Options() []client.Option { return []client.Option{client.WithRPCTimeout(s.timeout)} } type s2 struct { } func (s2) Options() []client.Option { return []client.Option{client.WithSuite(s1{timeout:1*time.Second}), client.WithRPCTimeout(2*time.Second)} } 那么如果我在创建 client 时传入 client.WithSuite(s2{}), client.WithRPCTimeout(3*time.Second)，在初始化的时候，会先执行到 client.WithSuite(s1{})，然后是 client.WithRPCTimeout(1*time.Second)，接着是 client.WithRPCTimeout(2*time.Second)，最后是 client.WithRPCTimeout(3*time.Second)。这样初始化之后，RPCTimeout 的值会被设定为 3s（参见开头所说的原则）。\n总结 Suite 是一种更高层次的组合和封装，更加推荐第三方开发者能够基于 Suite 对外提供 Kitex 的扩展，Suite 可以允许在创建的时候，动态地去注入一些值，或者在运行时动态地根据自身的某些值去指定自己的 middleware 中的值，这使得用户的使用以及第三方开发者的开发都更加地方便，无需再依赖全局变量，也使得每个 client 使用不同的配置成为可能。\n","categories":"","description":"","excerpt":"Suite 扩展 - 封装自定义治理模块 Suite（套件）是一种对于扩展的高级抽象，可以理解为是对于 Option …","ref":"/zh/docs/kitex/tutorials/framework-exten/suite/","tags":"","title":"Suite 扩展"},{"body":"目前，Kitex 支持了 Thrift 和 Protobuf 两种编解码。\nThrift Kitex 支持了 Thrift 的 Binary 协议，暂时没有支持 Compact 协议。\n生成代码时指定 Thrift 协议，也可以不指定，默认就是 Thrift：\n  客户端\nkitex -type thrift ${service_name} ${idl_name}.thrift   服务端\nkitex -type thrift -service ${service_name} ${idl_name}.thrift   我们针对 Thrift 的 binary 协议编解码进行了优化，具体优化细节参考 “Reference - 高性能 Thrift 编解码 \" 篇章，假如想要关闭这些优化，生成代码时可以加上 -no-fast-api 参数。\nProtobuf 协议说明 Kitex 对 protobuf 支持的协议有两种：\n 自定义的消息协议，可以理解为 Kitex Protobuf，使用方式与 thrift 一样 gRPC 协议，可以与 gRPC 互通，并且支持 streaming 调用  如果 IDL 文件中定义了 streaming 方法则走 gRPC 协议，否则走 Kitex Protobuf。没有 streaming 方法，又想指定 gRPC 协议，需要 client 初始化做如下配置（server 支持协议探测无需配置） ：\n// 使用 WithTransportProtocol 指定 transport cli, err := service.NewClient(destService, client.WithTransportProtocol(transport.GRPC)) 生成代码 只支持 proto3，语法参考 https://developers.google.com/protocol-buffers/docs/gotutorial\n注意：\n 相较其他语言，必须定义 go_package ，以后 pb 官方也会将此作为必须约束 go_package 和 thrift 的 namespace 定义一样，不用写完整的路径，只需指定包名，相当于 thrift 的 namespace，如：go_package = “pbdemo” 提前下载好 protoc 二进制放在 $PATH 目录下  生成代码时需要指定 protobuf 协议：\n  客户端\nkitex -type protobuf -I idl/ idl/${proto_name}.proto   服务端\nkitex -type protobuf -service ${service_name} -I idl/ idl/${proto_name}.proto   ","categories":"","description":"","excerpt":"目前，Kitex 支持了 Thrift 和 Protobuf 两种编解码。\nThrift Kitex 支持了 Thrift 的 Binary …","ref":"/zh/docs/kitex/tutorials/basic-feature/serialization_protocol/","tags":"","title":"序列化协议"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kitex/tutorials/service-governance/","tags":"","title":"治理特性"},{"body":"框架自身不带任何监控打点，只是提供了 Tracer 接口，用户可以根据需求实现该接口，并通过 WithTracer Option 来注入。\n// Tracer is executed at the start and finish of an RPC. type Tracer interface { Start(ctx context.Context) context.Context Finish(ctx context.Context) } kitex-contrib 中提供了 prometheus 的监控扩展，使用方式：\nClient\nimport ( \"github.com/kitex-contrib/monitor-prometheus\" kClient \"github.com/cloudwego/kitex/client\" ) ... client, _ := testClient.NewClient( \"DestServiceName\", kClient.WithTracer(prometheus.NewClientTracer(\":9091\", \"/kitexclient\"))) resp, _ := client.Send(ctx, req) ... Server\nimport ( \"github.com/kitex-contrib/monitor-prometheus\" kServer \"github.com/cloudwego/kitex/server\" ) func main() { ... svr := xxxservice.NewServer( \u0026myServiceImpl{}, kServer.WithTracer(prometheus.NewServerTracer(\":9092\", \"/kitexserver\"))) svr.Run() ... } ","categories":"","description":"","excerpt":"框架自身不带任何监控打点，只是提供了 Tracer 接口，用户可以根据需求实现该接口，并通过 WithTracer Option 来注入。 …","ref":"/zh/docs/kitex/tutorials/service-governance/monitoring/","tags":"","title":"监控"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/hertz/tutorials/basic-feature/","tags":"","title":"Basic Feature"},{"body":"Exception Type Defined in github.com/cloudwego/kitex/pkg/kerrors\ninternal exception ErrInternalException, framework internal error, it cloud be:\n ErrNotSupported, \"operation not supported\", have some operation not supported yet ErrNoResolver, \"no resolver available\", no resolver is available ErrNoDestService, \"no dest service\", target service is not specified ErrNoDestAddress, \"no dest address\", target address is not specified ErrNoConnection, \"no connection available\", not connection is available ErrNoIvkRequest, \"invoker request not set\", request is not set in invoker mode  service discovery error ErrServiceDiscovery, service discovery error, see error message for detail.\nget connection error ErrGetConnection, get connection error, see error message for detail.\nloadbalance error ErrLoadbalance, loadbalance error, see error message for detail.\nno more instances to retry ErrNoMoreInstance, no more instance to retry, see last call error message for detail.\nrpc timeout ErrRPCTimeout, RPC timeout, see error message for detail.\nrequest forbidden ErrACL, RPC is rejected by ACL, see error message for detail.\nforbidden by circuitbreaker ErrCircuitBreak, request is circuitbreaked, it could be two type of circuitbreak:\n ErrServiceCircuitBreak, \"service circuitbreak\", service level circuitbreak encountered, request is rejected. ErrInstanceCircuitBreak, \"instance circuitbreak\", instance level circuitbreak encountered, request is rejected.  remote or network error ErrRemoteOrNetwork, remote server error, or network error, see error message for detail.\n[remote] indicates the error is returned by server\nrequest over limit ErrOverlimit, overload protection error, it cloud be:\n ErrConnOverLimit, \"too many connections\", connection overload, connection number is over limit ErrQPSOverLimit, \"request too frequent\", concurrent request overload, concurrent request number is over limit  panic ErrPanic, panic detected.\n[happened in biz handler] indicated panic happened in server handler, usually call stack will attached to error message.\nbiz error ErrBiz, server handler error.\nretry error ErrRetry, retry error, see error message for detail.\nTHRIFT Error Code These error is thrift Application Exception, usually these error will be wrapped to remote or network error.\n   Code Name Meaning     0 UnknownApplicationException Unknown Error   1 UnknownMethod Unknown Function   2 InValidMessageTypeException Invalid Message Type   3 WrongMethodName Wrong Method Name   4 BadSequenceID Bad Sequence ID   5 MissingResult Result is missing   6 InternalError Internal Error   7 ProtocolError Protocol Error    Exception Check Check whether a Kitex error Use IsKitexError in kerrors package\nimport \"github.com/cloudwego/kitex/pkg/kerrors\" ... isKitexErr := kerrors.IsKitexError(kerrors.ErrInternalException) // return true Check Specified Error type Use errors.Is, detailed error cloud use to check detailed error:\nimport \"errors\" import \"github.com/cloudwego/kitex/client\" import \"github.com/cloudwego/kitex/pkg/kerrors\" ... _, err := echo.NewClient(\"echo\", client.WithResolver(nil)) // return kerrors.ErrNoResolver ... isKitexErr := errors.Is(err, kerrors.ErrNoResolver) // return true detailed error is also a basic error:\nimport \"errors\" import \"github.com/cloudwego/kitex/client\" import \"github.com/cloudwego/kitex/pkg/kerrors\" ... _, err := echo.NewClient(\"echo\", client.WithResolver(nil)) // return kerrors.ErrNoResolver ... isKitexErr := errors.Is(err, kerrors.ErrInternalException) // return true Specially, you can use IsTimeoutError in kerrors to check whether it is a timeout error\nGet Detailed Error Message All detailed errors is defined by DetailedError in kerrors, so you can use errors.As to get specified DetailedError, like:\nimport \"errors\" import \"github.com/cloudwego/kitex/client\" import \"github.com/cloudwego/kitex/pkg/kerrors\" ... _, err := echo.NewClient(\"echo\", client.WithResolver(nil)) // return kerrors.ErrNoResolver ... var de *kerrors.DetailedError ok := errors.As(err, \u0026ke) // return true if de.ErrorType() == kerrors.ErrInternalException {} // return true DetailedError provide following functions to get detail message.\n ErrorType() error, used to get basic error type Stack() string, used to get stack (for now only works for ErrPanic)  ","categories":"","description":"","excerpt":"Exception Type Defined in github.com/cloudwego/kitex/pkg/kerrors …","ref":"/docs/kitex/reference/exception/","tags":"","title":"Exception Instruction"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/motore/faq/","tags":"","title":"FAQ"},{"body":"Set Up Golang Development Environment  If you haven’t set up your Golang Environment, you can refer to Golang Install. We recommend you to use the Golang latest version, or make sure it’s \u003e= v1.15. You can choose to use the earlier versions, but the compatibility and stability are not assured. Make sure the go mod support is on (for Golang versions \u003e= v1.15, it is on by default).   Currently, Hertz supports Linux, macOS, and Windows systems.\n Quick Start After you have prepared the Golang environment, the chapter will help you to quickly get familiar with Hertz.\nInstall the commend tool of hz First, you need to install the commend tool hz which is used in this chapter\n Confirm the GOPATH environment has been defined correctly (For example export GOPATH=~/go) and the $GOPATH/bin has been added to PATH environment (For example export PATH=$GOPATH/bin:$PATH); Attention, do not set GOPATH to a directory that the current user does not have read/write access to. Install hz: go install github.com/cloudwego/hertz/cmd/hz@latest  For more information on how to use hz, please refer to: hz\nDetermine Where to Store Your Code  If your codes are placed under $GOPATH/src, you need to create additional dictionary under $GOPATH/src and retrieve your code under the dictionary.  $ mkdir -p $(go env GOPATH)/src/github.com/cloudwego $ cd $(go env GOPATH)/src/github.com/cloudwego If your codes are not placed under GOPATH, you can retrieve them directly.  Generate/Complete the Sample Code  Create the hertz_demo folder in the current directory and go to that directory Generate code hz new Tidy \u0026 get dependencies  $ go mod tidy If you are currently using a Windows system, you can write the following sample code.\n Create the hertz_demo folder in the current directory and go to that directory Create the main.go file Add the following code to the main.go file  package main import ( \"context\" \"github.com/cloudwego/hertz/pkg/app\" \"github.com/cloudwego/hertz/pkg/app/server\" \"github.com/cloudwego/hertz/pkg/common/utils\" \"github.com/cloudwego/hertz/pkg/protocol/consts\" ) func main() { h := server.Default() h.GET(\"/ping\", func(c context.Context, ctx *app.RequestContext) { ctx.JSON(consts.StatusOK, utils.H{\"ping\": \"pong\"}) }) h.Spin() } Generate the go.mod file  $ go mod init hertz_demo Tidy \u0026 get dependencies  $ go mod tidy Run the Sample Code After you have completed the previous steps, you are able to compile \u0026 launch the server\n$ go build -o hertz_demo \u0026\u0026 ./hertz_demo If the server is launched successfully, you will see following message\n2022/05/17 21:47:09.626332 engine.go:567: [Debug] HERTZ: Method=GET absolutePath=/ping --\u003e handlerName=main.main.func1 (num=2 handlers) 2022/05/17 21:47:09.629874 transport.go:84: [Info] HERTZ: HTTP server listening on address=[::]:8888 Then, we can test the interface\n$ curl http://127.0.0.1:8888/ping If nothing goes wrong, we can see the following output\n$ {\"ping\":\"pong\"} Now, you have already launched Hertz Server successfully and completed an API call. More API examples can be found at API Examples.\nAs for the project dictionary layout, here is a project layout sample that you can refer to. You can also organize the layout based on your business scenario.\nDirectory Structure As for the project directory structure, you may check Project Layout for reference, it can be organized according to the actual situation of the business logic.\nMore examples Please refer：hertz-examples\n","categories":"","description":"","excerpt":"Set Up Golang Development Environment  If you haven’t set up your …","ref":"/docs/hertz/getting-started/","tags":"","title":"Getting Started"},{"body":"Prerequisites  If you don’t setup golang development environment, please follow Install Go to install go. We strongly recommend you use latest golang version. And compatibility is guaranteed within three latest minor release version (for now \u003e= v1.16). Ensure GO111MODULE is set to on. Currently Windows is not particularly supported by Kitex, if your development environment is Windows, you are suggested to use WSL2.  Quick Start This chapter gonna get you started with Kitex with a simple executable example.\nInstall Compiler First of all, let’s install compilers we gonna work with.\n Ensure GOPATH environment variable is defined properly (for example export GOPATH=~/go), then add $GOPATH/bin to PATH environment variable (for example export PATH=$GOPATH/bin:$PATH). Make sure GOPATH is accessible. Install kitex: go install github.com/cloudwego/kitex/tool/cmd/kitex@latest Install thriftgo: go install github.com/cloudwego/thriftgo@latest  Now you can run kitex --version and thriftgo --version, and you can see some outputs just like below if you setup compilers successfully.\n$ kitex --version vx.x.x $ thriftgo --version thriftgo x.x.x Tips: If you encounter any problem during installation, it’s probably you don’t setup golang develop environment properly. In most cases you can search error message to find solution.\nGet the example  You can just click HERE to download the example Or you can clone the example repository git clone https://github.com/cloudwego/kitex-examples.git  Run the example Run by go   enter hello directory\ncd kitex-examples/hello\n  run server\ngo run .\n  run client\nopen a another terminal, and go run ./client\n  Run by docker   enter the example directory\ncd kitex-examples\n  build the example project\ndocker build -t kitex-examples .\n  run server\ndocker run --network host kitex-examples ./hello-server\n  run client\nopen another terminal, and docker run --network host kitex-examples ./hello-client\n  congratulation! You successfully use Kitex to complete a RPC.\nAdd a new method open hello.thrift, you will see code below:\nnamespacegoapistructRequest{1:stringmessage}structResponse{1:stringmessage}serviceHello{Responseecho(1:Requestreq)}Now let’s define a new request and responseAddRequest 和 AddResponse, after that add add method to service Hello:\nnamespacegoapistructRequest{1:stringmessage}structResponse{1:stringmessage}structAddRequest{1:i64first2:i64second}structAddResponse{1:i64sum}serviceHello{Responseecho(1:Requestreq)AddResponseadd(1:AddRequestreq)}When you complete it, hello.thrift should be just like above.\nRegenerate code Run below command, then kitex compiler will recompile hello.thrift and update generated code.\nkitex -service a.b.c hello.thrift # If the current directory is not under $GOPATH/src, you need to add the -module parameter which usually is same as the module name in go.mod kitex -module \"your_module_name\" -service a.b.c hello.thrift After you run the above command, kitex compiler will update these files:\n1. update `./handler.go`, add a basic implementation of `add` method. 2. update `./kitex_gen`, updates client and server implementation.  Update handler When you complete Regenerate Code chapter, kitex will add a basic implementation of Add to ./handler.go, just like:\n// Add implements the HelloImpl interface. func (s *HelloImpl) Add(ctx context.Context, req *api.AddRequest) (resp *api.AddResponse, err error) { // TODO: Your code here...  return } Let’s complete process logic, like:\n// Add implements the HelloImpl interface. func (s *HelloImpl) Add(ctx context.Context, req *api.AddRequest) (resp *api.AddResponse, err error) { // TODO: Your code here...  resp = \u0026api.AddResponse{Sum: req.First + req.Second} return } Call “add” method Let’s add add RPC to client example.\nYou can see something like below in ./client/main.go:\nfor { req := \u0026api.Request{Message: \"my request\"} resp, err := client.Echo(context.Background(), req) if err != nil { log.Fatal(err) } log.Println(resp) time.Sleep(time.Second) } Let’s add add RPC:\nfor { req := \u0026api.Request{Message: \"my request\"} resp, err := client.Echo(context.Background(), req) if err != nil { log.Fatal(err) } log.Println(resp) time.Sleep(time.Second) addReq := \u0026api.AddRequest{First: 512, Second: 512} addResp, err := client.Add(context.Background(), addReq) if err != nil { log.Fatal(err) } log.Println(addResp) time.Sleep(time.Second) } Run application again Shutdown server and client we have run, then:\n  run server\n `go run .`    run client\n open another terminal, and `go run ./client` Now, you can see outputs of `add` RPC.    Tutorial About Kitex Kitex is a RPC framework which supports multiple serialization protocols and transport protocols.\nKitex compiler supports both thrift and proto3 IDL, and fairly Kitex supports thrift and protobuf serialization protocol. Kitex extends thrift as transport protocol, and also supports gRPC protocol.\nWHY IDL We use IDL to define interface.\nThrift IDL grammar: Thrift interface description language.\nproto3 grammar: Language Guide(proto3).\nCreate project directory Let’s create a directory to setup project.\n$ mkdir example\nenter directory\n$ cd example\nKitex compiler kitex is a compiler which has the same name as Kitex framework, it can generate a project including client and server conveniently.\nInstall You can use following command to install and upgrade kitex:\n$ go install github.com/cloudwego/kitex/tool/cmd/kitex\nAfter that, you can just run it to check whether it’s installed successfully.\n$ kitex\nIf you see some outputs like below, congratulation!\n$ kitex\nNo IDL file found.\nIf you see something like command not found, you should add $GOPATH/bin to $PATH. For detail, see chapter Prerequisites .\nUsage You can visit Compiler for detailed usage.\nWrite IDL For example, a thrift IDL.\ncreate a echo.thrift file, and define a service like below:\nnamespacegoapistructRequest{1:stringmessage}structResponse{1:stringmessage}serviceEcho{Responseecho(1:Requestreq)}Generate echo service code We can use kitex compiler to compile the IDL file to generate whole project.\n$ kitex -module example -service example echo.thrift\n-module indicates go module name of project，-service indicates expected to generate a executable service named example, the last parameter is path to IDL file.\nGenerated project layout:\n. |-- build.sh |-- echo.thrift |-- handler.go |-- kitex_gen | `-- api | |-- echo | | |-- client.go | | |-- echo.go | | |-- invoker.go | | `-- server.go | |-- echo.go | `-- k-echo.go |-- main.go `-- script |-- bootstrap.sh `-- settings.py Get latest Kitex Kitex expect project to use go module as dependency manager. It cloud be easy to upgrade Kitex:\n$ go get github.com/cloudwego/kitex@latest $ go mod tidy If you encounter something like below :\ngithub.com/apache/thrift/lib/go/thrift: ambiguous import: found package github.com/apache/thrift/lib/go/thrift in multiple modules\nRun following command, and try again:\ngo mod edit -droprequire=github.com/apache/thrift/lib/go/thrift go mod edit -replace=github.com/apache/thrift=github.com/apache/thrift@v0.13.0 Write echo service process All method process entry should be in handler.go, you should see something like below in this file:\npackage main import ( \"context\" \"example/kitex_gen/api\" ) // EchoImpl implements the last service interface defined in the IDL. type EchoImpl struct{} // Echo implements the EchoImpl interface. func (s *EchoImpl) Echo(ctx context.Context, req *api.Request) (resp *api.Response, err error) { // TODO: Your code here... \treturn } Echo method represents the echo we defined in thrift IDL.\nNow let’s make Echo a real echo.\nmodify Echo method:\nfunc (s *EchoImpl) Echo(ctx context.Context, req *api.Request) (resp *api.Response, err error) { return \u0026api.Response{Message: req.Message}, nil } Compile and Run kitex compiler has generated scripts to compile and run the project:\nCompile:\n$ sh build.sh\nThere should be a output directory After you execute above command, which includes compilation productions .\nRun:\n$ sh output/bootstrap.sh\nNow, Echo service is running!\nWrite Client Let’s write a client to call Echo server.\ncreate a directory as client package:\n$ mkdir client\nenter directory:\n$ cd client\ncreate a main.go file.\nCreate Client Let’s new a client to do RPC：\nimport \"example/kitex_gen/api/echo\" import \"github.com/cloudwego/kitex/client\" ... c, err := echo.NewClient(\"example\", client.WithHostPorts(\"0.0.0.0:8888\")) if err != nil { log.Fatal(err) } echo.NewClient is used to new a client, the first parameter is service name, the second parameter is options which is used to pass options. client.WithHostPorts is used to specify server address, see chapter Basic Feature for details.\nDo RPC Let’s write call code:\nimport \"example/kitex_gen/api\" ... req := \u0026api.Request{Message: \"my request\"} resp, err := c.Echo(context.Background(), req, callopt.WithRPCTimeout(3*time.Second)) if err != nil { log.Fatal(err) } log.Println(resp) We new a request req, then we use c.Echo to do a RPC call.\nThe first parameter context.Context, is used to transfer information or to control some call behaviors. You will see detailed usage in behind chapters.\\\nThe seconde parameter is request.\nThe third parameter is call options, which is called callopt, these options only works for this RPC call. callopt.WithRPCTimeout is used to specify timeout for this RPC call. See chapter Basic Feature for detail.\nRun Client You can run following command to run a client:\n$ go run main.go\nYou should see some outputs like below:\n2021/05/20 16:51:35 Response({Message:my request})\nCongratulation! You have written a Kitex server and client, and have done a RPC call.\n","categories":"","description":"","excerpt":"Prerequisites  If you don’t setup golang development environment, …","ref":"/docs/kitex/getting-started/","tags":"","title":"Getting Started"},{"body":" This tutorial gets you started with Netpoll through some simple examples, includes how to use Server, Client and nocopy APIs.\n 1. Use Server Here is a simple server demo, we will explain how it is constructed next.\n1.1 Create Listener First we need to get a Listener, it can be net.Listener or netpoll.Listener, which is no difference for server usage. Create a Listener as shown below:\npackage main import \"net\" func main() { listener, err := net.Listen(network, address) if err != nil { panic(\"create net listener failed\") } ... } or\npackage main import \"github.com/cloudwego/netpoll\" func main() { listener, err := netpoll.CreateListener(network, address) if err != nil { panic(\"create netpoll listener failed\") } ... } 1.2 New EventLoop EventLoop is an event-driven scheduler, a real NIO Server, responsible for connection management, event scheduling, etc.\nparams:\n OnRequest is an interface that users should implement by themselves to process business logic. Code Comment describes its behavior in detail. Option is used to customize the configuration when creating EventLoop, and the following example shows its usage. For more details, please refer to options.  The creation process is as follows:\npackage main import ( \"time\" \"github.com/cloudwego/netpoll\" ) var eventLoop netpoll.EventLoop func main() { ... eventLoop, _ := netpoll.NewEventLoop( handle, netpoll.WithOnPrepare(prepare), netpoll.WithReadTimeout(time.Second), ) ... } 1.3 Run Server EventLoop provides services by binding Listener, as shown below. Serve function will block until an error occurs, such as a panic or the user actively calls Shutdown.\npackage main import ( \"github.com/cloudwego/netpoll\" ) var eventLoop netpoll.EventLoop func main() { ... // start listen loop ... \teventLoop.Serve(listener) } 1.4 Shutdown Server EventLoop provides the Shutdown function, which is used to stop the server gracefully. The usage is as follows.\npackage main import ( \"context\" \"time\" \"github.com/cloudwego/netpoll\" ) var eventLoop netpoll.EventLoop func main() { // stop server ... \tctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() eventLoop.Shutdown(ctx) } 2. Use Dialer Netpoll also has the ability to be used on the Client side. It provides Dialer, similar to net.Dialer. Again, here is a simple client demo, and then we introduce it in detail.\n2.1 The Fast Way Similar to Net, Netpoll provides several public functions for directly dialing a connection. such as:\nDialConnection(network, address string, timeout time.Duration) (connection Connection, err error) DialTCP(ctx context.Context, network string, laddr, raddr *TCPAddr) (*TCPConnection, error) DialUnix(network string, laddr, raddr *UnixAddr) (*UnixConnection, error) 2.2 Create Dialer Netpoll also defines the Dialer interface. The usage is as follows: (of course, you can usually use the fast way)\npackage main import ( \"github.com/cloudwego/netpoll\" ) func main() { // Dial a connection with Dialer. \tdialer := netpoll.NewDialer() conn, err := dialer.DialConnection(network, address, timeout) if err != nil { panic(\"dial netpoll connection failed\") } ... } 3. Use Nocopy API Connection provides Nocopy APIs - Reader and Writer, to avoid frequent copying. Let’s introduce their simple usage.\npackage main type Connection interface { // Recommended nocopy APIs \tReader() Reader Writer() Writer ... // see code comments for more details } 3.1 Simple Usage Nocopy APIs is designed as a two-step operation.\nOn Reader, after reading data through Next, Peek, ReadString, etc., you still have to actively call Release to release the buffer(Nocopy reads the original address of the buffer, so you must take the initiative to confirm that the buffer is no longer used).\nSimilarly, on Writer, you first need to allocate a buffer to write data, and then call Flush to confirm that all data has been written. Writer also provides rich APIs to allocate buffers, such as Malloc, WriteString and so on.\nThe following shows some simple examples of reading and writing data. For more details, please refer to the code comments.\npackage main import ( \"github.com/cloudwego/netpoll\" ) func main() { var conn netpoll.Connection var reader, writer = conn.Reader(), conn.Writer() // reading \tbuf, _ := reader.Next(n) ... parse the read data ... reader.Release() // writing \tvar write_data []byte ... make the write data ... alloc, _ := writer.Malloc(len(write_data)) copy(alloc, write_data) // write data \twriter.Flush() } 3.2 Advanced Usage If you want to use the connection to send (or receive) multiple sets of data, then you will face the work of packing and unpacking the data.\nOn net, this kind of work is generally done by copying. An example is as follows:\npackage main import ( \"net\" ) func main() { var conn net.Conn var buf = make([]byte, 8192) // reading \tfor { n, _ := conn.Read(buf) ... unpacking \u0026 handling ... var i int for i = 0; i \u003c= n-pkgsize; i += pkgsize { pkg := append([]byte{}, buf[i:i+pkgsize]...) go func() { ... handling pkg ... } } buf = append(buf[:0], buf[i:n]...) } // writing \tvar write_datas \u003c-chan []byte ... packing write ... for { pkg := \u003c-write_datas conn.Write(pkg) } } But, this is not necessary in Netpoll, nocopy APIs supports operations on the original address of the buffer, and realizes automatic recycling and reuse of resources through reference counting.\nExamples are as follows(use function Reader.Slice and Writer.Append):\npackage main import ( \"github.com/cloudwego/netpoll\" ) func main() { var conn netpoll.Connection // reading \treader := conn.Reader() for { ... unpacking \u0026 handling ... pkg, _ := reader.Slice(pkgsize) go func() { ... handling pkg ... pkg.Release() } } // writing \tvar write_datas \u003c-chan netpoll.Writer ... packing write ... writer := conn.Writer() for { select { case pkg := \u003c-write_datas: writer.Append(pkg) default: if writer.MallocLen() \u003e 0 { writer.Flush() } } } } ","categories":"","description":"","excerpt":" This tutorial gets you started with Netpoll through some simple …","ref":"/docs/netpoll/getting-started/","tags":"","title":"Getting Started"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/releases/hertz/","tags":"","title":"Hertz Release"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/hertz/","tags":"","title":"Hertz"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/blog/releases/hertz/","tags":"","title":"Hertz Release"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/hertz/","tags":"","title":"Hertz"},{"body":"在 Tower 的 Service 中，有一个方法 poll_ready，用来在请求之前先确定下游 Service 有足够的处理能力，并在处理能力不足时提供背压。 这是一个非常精妙的设计，Tower 在 inventing-the-service-trait 这篇介绍文章中，也有详细介绍这么设计的原因。\n但是在我们真实的开发体验中，我们总结出了以下的经验：\n 绝大多数的 poll_ready 的实现都是直接 self.inner.poll_ready(cx)；剩下的 poll_ready 实现更干脆，直接 Poll::Ready(Ok(()))。 poll_ready 一般不会真正跨服务去 check 负载（也就是说，不会真的发个请求问下游“大兄弟，你还能支棱起来不？”），所以一般也就是在本地的中间件（比如 Tower 的例子是速率限制中间件）里面根据某些特定条件判断一下。 基于上两条，几乎所有的 poll_ready 场景，我们都可以直接在 call 里面做达到一样的效果，因为实践中外层的 service 在返回 Poll::Pending 的时候就是空等，不如直接采用 async-await 的方式来编写代码，更符合人体工程学。 至于（可能的）资源浪费的问题，一般来说可能发生拦截的中间件，肯定是放在越前面越好，所以通过合理地排布中间件的顺序，就能解决这个问题。  因此，出于“如无必要勿增实体”的原则，也为了提升易用性，我们最终决定在我们的设计中不包含 poll_ready 方法。\n","categories":"","description":"","excerpt":"在 Tower 的 Service 中，有一个方法 poll_ready，用来在请求之前先确定下游 Service 有足够的处理能力，并在处 …","ref":"/zh/docs/motore/faq/q2_pull_ready/","tags":"","title":"poll_ready（背压）哪去了？"},{"body":"Overview Thanks to the layered design of Hertz, in addition to the HTTP1/HTTP2 (to be open source) protocol server that comes with the Hertz framework by default, users can easily add/customize protocol processing logic that meets the needs of their own business scenarios according to their own needs.\nIn short, a server that implements the following interface can be added to Hertz as a custom extension server:\ntype Server interface { Serve(c context.Context, conn network.Conn) error } Three elements of protocol layer extension Protocol layer server initialization Because the interface mentioned in the overview is actually a standard callback after the data is prepared at the network layer, the processing logic of our protocol layer will only be entered after a new request is established for a connection.\nIn this logic, we can customize the protocol parsing method, introduce business Handler execution, write data back and other standard behaviors of the protocol layer. This is also the core logic of our custom server.\ntype myServer struct{ xxx xxx } func (s *myServer) Serve (c context.Context, conn network.Conn) error{ // protocol parsing \t... // Go to the logic function of business registration (Route, Middleware, Handler...) \t... // write data back \t... } Defining a protocol processing logic is as simple as that! However, the two steps of parsing the protocol and writing data back can be easily achieved through the conn interface provided in the input parameters, but how to go to the logical function of business registration?\nInteraction with upper-level logic A complete protocol must introduce business logic control (except for very few special situations), so how does the custom protocol in the Hertz framework realize this part of the ability? In fact, in the process of custom server initialization, the framework has naturally handed over this part of the capabilities to the custom protocol server.\ntype ServerFactory interface { New(core Core) (server protocol.Server, err error) } // Core is the core interface that promises to be provided for the protocol layer extensions type Core interface { // IsRunning Check whether engine is running or not  IsRunning() bool // A RequestContext pool ready for protocol server impl  GetCtxPool() *sync.Pool // Business logic entrance  // After pre-read works, protocol server may call this method  // to introduce the middlewares and handlers  ServeHTTP(c context.Context, ctx *app.RequestContext) // GetTracer for tracing requirement  GetTracer() tracer.Controller } A custom server only needs to implement a protocol server generation factory according to the above interface. The Core in the parameters actually includes the introduction of upper-layer logic interaction and the specific implementation of other core application layer interfaces. When initializing a custom server, normally you only need to save the Core to the server. When you need to transfer to the business logic, you can guide the process to the application layer processing logic (Route, Middleware, Logic Handler) through the Core. When the business logic is executed and returned, further packets can be written back based on the business data.\ntype myServer struct{ suite.Core xxx } func (s *myServer) Serve (c context.Context, conn network.Conn) error{ // protocol parsing \t... Core.ServeHTTP(c, ctx) // write data back \t... } So far, a custom protocol layer server has been developed.\nRegistration of custom protocol server into Hertz After completing the development of the server generation factory according to the above interface, it is very easy to load it into Hertz. Hertz’s core engine naturally provides an interface for registering a custom protocol server:\nfunc (engine *Engine) AddProtocol(protocol string, factory suite.ServerFactory) { engine.protocolSuite.Add(protocol, factory) } It is only necessary to register the user’s custom server generation factory with the engine according to the parameters specified by the interface. But it is worth noting that the protocol (string) registered here actually corresponds to the protocol negotiation key in ALPN (Application-Layer Protocol Negotiation), so if you want to access a custom protocol server through ALPN , directly specify the key as the corresponding key during ALPN negotiation. Currently, Hertz integrates an HTTP1 protocol server by default (the corresponding key is “http/1.1”). If you need to customize the HTTP1 protocol processing logic, you can directly specify the key as “http/1.1” within AddProtocol to overwrite.\nExample package main import ( \"bytes\" \"context\" \"github.com/cloudwego/hertz/pkg/app\" \"github.com/cloudwego/hertz/pkg/app/server\" \"github.com/cloudwego/hertz/pkg/common/errors\" \"github.com/cloudwego/hertz/pkg/common/hlog\" \"github.com/cloudwego/hertz/pkg/network\" \"github.com/cloudwego/hertz/pkg/protocol\" \"github.com/cloudwego/hertz/pkg/protocol/suite\" ) type myServer struct { suite.Core } func (m myServer) Serve(c context.Context, conn network.Conn) error { firstThreeBytes, _ := conn.Peek(3) if !bytes.Equal(firstThreeBytes, []byte(\"GET\")) { return errors.NewPublic(\"not a GET method\") } ctx := m.GetCtxPool().Get().(*app.RequestContext) defer func() { m.GetCtxPool().Put(ctx) conn.Skip(conn.Len()) conn.Flush() }() ctx.Request.SetMethod(\"GET\") ctx.Request.SetRequestURI(\"/test\") m.ServeHTTP(c, ctx) conn.WriteBinary([]byte(\"HTTP/1.1 200 OK\\n\" + \"Server: hertz\\n\" + \"Date: Sun, 29 May 2022 10:49:33 GMT\\n\" + \"Content-Type: text/plain; charset=utf-8\\n\" + \"Content-Length: 2\\n\\nok\\n\")) return nil } type serverFactory struct { } func (s *serverFactory) New(core suite.Core) (server protocol.Server, err error) { return \u0026myServer{ core, }, nil } func main() { h := server.New() h.GET(\"/test\", func(c context.Context, ctx *app.RequestContext) { hlog.Info(\"in handler\") }) h.AddProtocol(\"http/1.1\", \u0026serverFactory{}) h.Spin() } ","categories":"","description":"","excerpt":"Overview Thanks to the layered design of Hertz, in addition to the …","ref":"/docs/hertz/tutorials/framework-exten/advanced-exten/protocol/","tags":"","title":"Protocol extension"},{"body":"案例介绍   近些年电商行业高速发展，森马电商线上业务激增，面临着高并发、高性能的业务场景需求。森马通过使用 Kitex 接入 Istio，极大地提高了对高并发需求的处理能力。\n本文将从四个方面为大家讲解 Kitex 在森马电商场景下的落地实践：\n 森马电商订单流转中心——天枢所面临的业务挑战； 项目的技术选型过程； 项目上线性能压测对比； CloudWeGo 团队的技术支持。  森马电商订单流转中心——天枢 业务增长 第一部分给大家介绍订单流转中心——天枢。天枢的主要功能是对接各大电商平台，把订单、商品、退单等信息统一处理后流转到下游系统，是下游系统和平台对接的中间枢纽。 目前森马电商在运营的电商平台几十家，如：天猫、抖店、京东、拼多多等，由于每个平台的接口和对接的方式不统一，我们专门开发了这套系统，去统一对接电商平台，然后把数据处理成统一的格式发到下游系统，如：OMS 和 WMS。 该系统在电商活动，如 6·18，双十一等订单峰值流量下发挥了重要作用。\n从 2015 年至 2021 年，森马的双十一业务量增长非常迅速。2015 年双十一的业绩有 3 亿+，而去年的双十一业绩为 20 亿+，2021 年商品交易总额（GMV） 更是突破百亿。 随着业务的增长，对订单系统的性能和稳定性要求越来越高。而且随着系统的规模增长，集群内的 Pod 数量和 Service 不断增加，对系统底层架构有很大的考验。 目前从旧系统迁移的平台有：有赞、抖音、拼多多、快手等，集群内的 Pod 数已经超过 200 个，后续会接入京东、唯品会、天猫等平台后，Pod 数会成倍的增长，更需要一个成熟的系统架构作为支撑。\n面临的问题 随着直播行业的兴起，我们请了一些网红主播和流量明星来直播带货。直播期间，订单量经常会出现几秒内突然爆发的情况，订单推送到系统后，如果系统处理较慢，订单就不能及时流入下游系统， 下游系统的 OMS 不知道已经产生如此大的订单量，就会出现不能同步的情况，即超卖现象。在电商行业，超卖是很严重的问题，如果用户下单后不能及时发货，不仅需要大量的人力去跟客户解释道歉， 也要以优惠券等形式赔偿用户遭受的损失，甚至会接到大量投诉，严重影响我们在电商平台的信誉，电商平台也会对我们进行处罚。我们经历过当 GMV 超过千万时，订单系统延迟超过半个小时的情况，对我们造成了极大的影响。 因此，当遇到如双十一，6·18 大促等活动时，特别是在直播时订单量短时间内暴增的情况下，我们原有的系统架构已经无法支撑，不能及时处理订单数据。这影响了我们发货及库存同步，间接地产生了不同类型的资损。\n技术挑战 我们在技术上面临的挑战主要有以下三个方面：\n 高并发。在电商业务场景下，不管是面向用户，比如秒杀，还是面向业务，比如订单处理，如果实现不了高并发，系统就很难做大，很难适应业务的增长。 高性能。除了用高并发来实现业务的快速处理外，性能也是一个挑战。例如在当前疫情状态下，各行各业都在降本增效，解决不了性能问题，就会不断地增加服务器资源，大大增加企业成本。 技术保障。我们电商行业的公司，大多资源和精力都在销售端，运营端，技术方面投入相对薄弱。因此在技术选型上需要从可靠、安全、支持等维度去考量。  项目的技术选型 如何选择 在开发语言的选择方面，开发语言没有好坏之分，只有这个语言在相关场景下合适不合适的问题。我们从性能、多线程、编译、效率等方面综合考虑，选择了 Golang。\n在微服务框架的选择方面，团队分别用 Google 开源的 gRPC 和字节跳动开源的 CloudWeGo-Kitex 做了技术评估和性能压测。经过专业测试同学的压力测试，最终选择了 CloudWeGo-Kitex 作为我们的微服务框架。\n选择 Kitex 的原因主要有两点。第一是 Kitex 背后有强大的技术团队提供及时有效的技术支持。第二是经过压力测试，Kitex 的性能优于其他微服务框架。\n关于微服务 使用微服务框架，一定会涉及到选择第三方开源的服务注册中心，那么是选择常用的开源注册中心（Zookeeper、Eureka、Nacos、Consul 和 ETCD）， 还是直接选择云原生的服务网格（Istio）？那么我从流量转发、服务注册和服务发现维度介绍一下微服务集群的两种形式。\n第一种是 Kubernetes Native，Kubernetes 集群中的每个节点都部署了一个 Kube-proxy 组件，该组件与 Kubernetes API Server 进行通信，观测服务和节点中的变化，进行负载均衡的转发。 这种开源注册中心默认使用 TCP 协议，由于 K8s 负载均衡不支持 RPC 协议（HTTP2），因而需要额外的第三方服务注册中心支持。\n第二种是基于 Istio 的服务网格，它并不需要额外的注册中心组件支持。Istio 接管了 K8s 的网络，通过 Sidecar Proxy 的方式将 Kubernetes 中的流量控制从服务层中抽离出来， Istio 基于 Enovy 的 xDS 协议扩展了其控制平面，每个 Pod 中放入原有的 Kube-proxy 路由转发功能。Istio 具备了流量管理、策略控制、可观察性等特点，将“应用程序”与“网络”解耦，因此不需要额外使用第三方注册中心。\n 那么这两种服务注册与发现的流程是怎样的呢？ 下图中左侧就是常用的服务注册中心使用流程。目标服务先把实例注册到服务注册中心，客户端从服务注册中心拿到目标实例的数据，根据负载均衡策略去选择一个服务实例，完成整个请求。 右侧是使用了基于 Istio 的服务网格。大概流程是 Client 访问目标服务的时候，流量先进入 Service 的 Proxy，被 Proxy 拦截，Proxy 会从服务发现（Pilot）拿到服务与服务实例的映射关系， 同时会拿到负载均衡的策略，去选择 Service 一个实例。总体来看，这两种流程大致相同，但实现方式有所差别，各有所长。\n天枢系统基本架构 像抖音、快手、拼多多和有赞等这样成熟的平台在产生订单时，都会将订单以消息推送的形式发送到服务网格中。我们先后通过 Ingress Gateway 网格入口管理程序、VirtualService 把订单转发到网格的不同服务中， 内部再通过不同服务之间进行调用。其中，Kitex 作为微服务的 RPC 框架，服务发现和服务注册均是基于云原生的服务网格 Istio。\nKitex 接入 Istio 那么 Kitex 接入 Istio 是怎么实现的呢？如下图所示，服务端注册服务之后，在创建客户端的时候，客户端的 Server-host 要写实际集群中的内网地址，例如：server-douyin.default.svc.cluster.local，如上文所说，不用再搭配第三方的服务注册中心。\n由于 Kitex 使用 gRPC 协议，在创建客户端的时候需指定使用 gRPC 协议：\n在 Istio 中怎么部署我们的客户端或者服务端呢？有以下两种方式：\n 为命名空间开启自动注入：kubectl label namespace default istio-injection=enabled。注入之后会产生两个重要的容器，第一个是 Istio-proxy，负责流量拦截和流量代理，比如做流量转发；第二个是 Server-douyin，是负责开发的应用容器。  把 Go 代码打包的镜像部署到集群中： 例如我们创建了一个 Deployment，名为 Server-douyin，另外作为服务端需要创建相应的 Service。  压测对比 我们将 Kitex 和 gRPC 在以下相同服务器硬件资源和网络环境下进行了压测对比：\n 压测工具：JMeter； 阿里云 ECS （8 vCPU，16 GiB，5 台）； 集群：Kubernetes 1.20.11； 服务网格：Istio v1.10.5.39。  通过对比发现，在指定时间相同的情况下，Kitex 在单位时间内处理订单数量更多。在指定订单数量的情况下，Kitex 对于处理相同数量的订单所需时间更短，且订单量越大，这种性能差别越明显。总体来看，Kitex 在处理大批订单时优势还是非常突出的。\nKitex 产生性能优势的原因 CloudWeGo 团队来森马做技术支持时讲到对自研网络库 Netpoll 做了一些性能优化，比如：\n 连接利用率； 调度延迟优化； 优化 I/O 调用； 序列化/反序列化优化； …….  更多资料可以查看 CloudWeGo 官网或参考官网博客\nCloudWeGo 团队的技术支持 我们选择 Kitex 之后，CloudWeGo 技术团队给予了足够的技术支持，包括现场支持和远程协助。这也让我们对使用 Kitex 有了信心，不管遇到什么样的技术难题，都会有强大的技术团队来协助解决。\n后续规划 Thrift 和 Protobuf 如何选择 我们在项目初期选择 gRPC 协议 Protobuf 是因为选择了 Istio 服务网格，而选择 Istio 服务网格主要是因为它有多流量转发和服务治理等功能，例如在电商场景下， 不同平台的推送消息都可以通过 VirtualService 转发到不同的服务，相当方便。但是目前每个 Pod 中放入原有的 Kube-proxy 路由转发功能，会增加响应延迟。由于 Sidecar 拦截流量时跳数更多，会消耗更多的资源。\n而对于 Thrift，它是 Kitex 默认支持的协议，字节官方对它做了很多性能上的优化，如：使用 SIMD 优化 Thrift 编码，减少函数调用，减少内存操作等，还开源了高性能 Thrift 编解码器 Frugal， Frugal 具有无需生成代码、高性能（在多核场景下，Frugal 的性能可以达到传统编解码方式的 5 倍）和稳定性等特点，进一步提升了性能和开发效率。\n因此，我们目前也在考虑在下一次系统版本的架构中改用 Thrift 协议。\n服务、合作共赢 我们开发的电商相关产品不仅可以为自己电商品牌所使用，产品成熟后还可以服务于其他相似的电商公司。后续我们也希望能够和 Kitex 官方有更深的技术合作，为社区带来更大价值。\n","categories":"","description":"","excerpt":"案例介绍   近些年电商行业高速发展，森马电商线上业务激增，面临着高并发、高性能的业务场景需求。森马通过使用 Kitex 接入 Istio， …","ref":"/cooperation/semir/","tags":"","title":"Kitex 在森马电商场景的落地实践"},{"body":"In microservices, link tracing is a very important capability, which plays an important role in quickly locating problems, analyzing business bottlenecks, and restoring the link status of a request. Hertz provides the capability of link tracking and also supports user-defined link tracking.\nHertz abstracts trace as the following interface：\n// Tracer is executed at the start and finish of an HTTP. type Tracer interface { Start(ctx context.Context, c *app.RequestContext) context.Context Finish(ctx context.Context, c *app.RequestContext) } Use the server.WithTracer() configuration to add a tracer, you can add multiple tracers.\nHertz will execute the Start method of all tracers before the request starts (before reading the packet), and execute the Finish method of all tracers after the request ends (after writing back the data). Care should be taken when implementing this：\n When the Start method is executed, it just starts accepting packets, and at this time requestContext is an “empty” requestContext, so we can’t get information about this request. If you want to get some information (such as the traceID in the header, etc.) after unpacking, you can use the middleware capability to inject the traceID into the span. Changes to the context within the middleware are invalid.  There is traceInfo in the requestContext memory, which has the following information\ntype HTTPStats interface { Record(event stats.Event, status stats.Status, info string) // Recording events  GetEvent(event stats.Event) Event // Get events  SendSize() int // Get SendSize  RecvSize() int // Get RecvSize  Error() error // Get Error  Panicked() (bool, interface{}) // Get Panic  Level() stats.Level // Get the current trace level  SetLevel(level stats.Level) // Set the trace level to not report when the event level is higher than the trace level  ... } Events include：\nHTTPStart = newEvent(httpStart, LevelBase) // Request start HTTPFinish = newEvent(httpFinish, LevelBase) // Request end  ServerHandleStart = newEvent(serverHandleStart, LevelDetailed) // Business handler start ServerHandleFinish = newEvent(serverHandleFinish, LevelDetailed) // Business handler end ReadHeaderStart = newEvent(readHeaderStart, LevelDetailed) // Read header start ReadHeaderFinish = newEvent(readHeaderFinish, LevelDetailed) // Read header end ReadBodyStart = newEvent(readBodyStart, LevelDetailed) // Read body start ReadBodyFinish = newEvent(readBodyFinish, LevelDetailed) // Read body end WriteStart = newEvent(writeStart, LevelDetailed) // Write response start WriteFinish = newEvent(writeFinish, LevelDetailed) // Write response end The above information is available at Finish\nAt the same time, if you don’t want to log this information, you don’t have to register any tracer, and the framework stops logging this information.\nAn extension for opentracing is provided in hertz-contrib, and a demo for calling from http to rpc is also available in hertz-examples.\nRelated Repository： https://github.com/hertz-contrib/tracer\n","categories":"","description":"","excerpt":"In microservices, link tracing is a very important capability, which …","ref":"/docs/hertz/tutorials/service-governance/tracing/","tags":"","title":"Tracing"},{"body":"Hertz complies with the Semantic Version 2.0.0 release version.\n Master version number: Upgrade this version number if the API provided by Hertz becomes incompatible Secondary version number: Upgrade this version number when Hertz provides new features while maintaining backward compatibility Revision number: Upgrade this version number when Hertz’s code provides minor features or backward-compatible optimizations and issue fixes  ","categories":"","description":"","excerpt":"Hertz complies with the Semantic Version 2.0.0 release version. …","ref":"/docs/hertz/reference/version/","tags":"","title":"Version Descriptions"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/volo/volo-thrift/","tags":"","title":"Volo-Thrift"},{"body":" SUPPORT Vulnerability Management   # Vulnerability Response   CloudWeGo 社区非常重视社区版本的安全性，CloudWeGo 安全委员会负责接收、调查和披露 CloudWeGo 社区相关的安全漏洞。我们鼓励漏洞研究人员和行业组织主动将 CloudWeGo 社区的疑似安全漏洞报告给 CloudWeGo 社区安全委员会。我们会快速的响应、分析和解决上报的安全问题或安全漏洞。\n# Supported Version  漏洞响应流程主要支持 CloudWeGo 社区各个子项目的最新版本，如果您还没有升级请尽快升级。\n# Vulnerability Handling Process 每个一个安全漏洞都会有一个指定的人员进行跟踪和处理，协调员是 CloudWeGo 安全委员会的成员，他将负责跟踪和推动漏洞的修复和披露。漏洞端到端的处理流程如下图。\n在这里我们主要介绍流程中漏洞上报、漏洞评估和漏洞披露这三部分内容。\n# Vulnerability Report 如果您认为 CloudWeGo 产品存在一个疑似安全漏洞，我们希望您将漏洞上报给 CloudWeGo 社区，并与我们配合以负责任的方式修复和披露该问题。\n# Reporting Methods 您可以通过 email 将 CloudWeGo 产品的潜在安全漏洞发送到 CloudWeGo 安全团队邮箱（security@cloudwego.io）。 # Reporting Methods 为了便于快速的确认和验证疑似漏洞，请在漏洞上报邮件中包含但不限于以下内容：\n 基本信息：包括漏洞影响的模块、漏洞的触发条件和成功利用后对系统的影响等。\n 技术细节：包括系统配置、定位方法、Exploit 的描述、POC、问题重现方法和步骤等。\n 修复方案建议。\n 上报者的组织和联系方式。\n 上报者可能的漏洞披露计划。\n  # Email Response Time 我们将在48小时内响应通过邮箱上报的疑似安全漏洞，并向上报者反馈漏洞处理的进展。\n# Vulnerability Severity Assessment  业界普遍使用 CVSS 标准评估漏洞的严重性，CloudWeGo 在使用 CVSSv3 进行漏洞评估时，需要设定漏洞攻击场景，基于在该攻击场景下的实际影响进行评估。漏洞严重等级评估是指针对漏洞利用难易程度，以及利用后对机密性、完整性、可用性的影响进行评估，并生成一个评分值。\n# Assessment Criteria CloudWeGo 社区采用 CVSS v3对漏洞进行评估，CVSS V3 由通过对以下向量来评估一个漏洞的影响：\n 攻击向量（AV）-表示攻击的“远程性”以及如何利用此漏洞。\n 攻击复杂性（AC）-讲述攻击执行的难度以及成功进行攻击需要哪些因素。\n 用户交互（UI）-确定攻击是否需要用户参与。\n 所需的权限（PR）-记录成功进行攻击所需的用户身份验证级别。\n 范围（S）-确定攻击者是否可以影响具有不同权限级别的组件。\n 机密性（C）-衡量信息泄露给非授权方后导致的影响程度。\n 完整性（I）-衡量信息被篡改后导致的影响程度。\n 可用性（A）-衡量用户在需要访问数据或服务时受影响的程度。\n  # Assessment Principles  评估漏洞的严重等级，不是评估风险。\n 评估时必须基于攻击场景，且保证在该场景下，攻击者成功攻击后能对系统造成机密性、完整性、可用性影响。\n 当安全漏洞有多个攻击场景时，应以造成最大的影响，即 CVSS 评分最高的攻击场景为依据。\n 被嵌入调用的库存在漏洞，要根据该库在产品中的使用方式，确定漏洞的攻击场景后进行评估。\n 安全缺陷不能被触发或不影响 CIA(机密性/完整性/可用性)，CVSS 评分为0分。\n  # Assessment Steps 对漏洞进行评估时，可根据下述步骤进行操作：\n 设定可能的攻击场景，基于攻击场景评分。\n 确定漏洞组件（Vulnerable Component）和受影响组件（Impact Component）。\n 选择基础评估指标的值：通过对可利用指标（攻击向量/攻击复杂度/所需权限/用户交互/范围）和受影响指标（机密性/完整性/可用性）给出漏洞影响评估。\n  # Severity Classification   严重等级（Severity Rating） CVSS评分（Score）   致命（Critical） 9.0 - 10.0   高（High） 7.0 - 8.9   中（Medium） 4.0 - 6.9   低（Low） 0.1 - 3.9   无（None） 0.0    # Vulnerability Disclosure  为了保护 CloudWeGo 用户的安全，在进行调查、修复和发布安全公告之前，CloudWeGo 社区不会公开披露、讨论或确认 CloudWeGo 产品的安全问题。安全漏洞修复后 CloudWeGo 社区会发布安全公告，安全公告内容包括该漏洞的技术细节、CVE 编号、CVSS 安全评分、严重性等级以及受到该漏洞影响的版本和修复版本等信息。   ","categories":"","description":"","excerpt":" SUPPORT Vulnerability Management   # Vulnerability Response …","ref":"/security/vulnerability-reporting/","tags":"","title":"vulnerability-reporting"},{"body":"Volo-gRPC 是一个 RPC 框架，既然是 RPC，底层就需要两大功能：\n Serialization 序列化 Transport 传输  IDL 全称是 Interface Definition Language，接口定义语言。\nWhy IDL 如果我们要进行 RPC，就需要知道对方的接口是什么，需要传什么参数，同时也需要知道返回值是什么样的，就好比两个人之间交流，需要保证在说的是同一个语言、同一件事。 这时候，就需要通过 IDL 来约定双方的协议，就像在写代码的时候需要调用某个函数，我们需要知道函数签名一样。\nProtobuf IDL 是一套跨语言的全栈式 RPC 解决方案，具体的语法可以看参考 protocol-buffers/docs/proto3。\n编写 IDL 为了创建一个 gRPC 项目，我们需要先编写一个 protobuf IDL。\n在你的工作目录下，我们先执行以下命令：\n$ mkdir volo-example $ cd volo-example $ mkdir idl $ vim idl/volo_example.proto 随后，我们输入以下内容：\nsyntax = \"proto3\";package volo.example;message Item { int64 id = 1; string title = 2; string content = 3; map\u003cstring, string\u003e extra = 10;}message GetItemRequest { int64 id = 1;}message GetItemResponse { Item item = 1;}service ItemService { rpc GetItem(GetItemRequest) returns (GetItemResponse);}保存退出后，我们执行以下命令：\n$ volo init --includes=idl volo-example idl/volo_example.proto 这里我们使用init命令，后面跟了我们项目的名字，意思是需要生成模板代码。在末尾，需要指定一个 IDL 表示 server 使用的 IDL。\n如果只需要增加一个 IDL（如 client 的 IDL）而不需要生成模板的话，如：\n$ volo idl add idl/volo_example.proto | 插播一个广告，volo 工具还支持从 git 下载 IDL 并生成代码哦，如：\n$ volo idl add -g git@github.com:org/repo.git -r main /path/to/your/idl.proto | 感兴趣可以直接输入 volo 看详细用法~ 接下来回到正题~\n这时候，我们整个目录的结构如下：\n. ├── Cargo.toml ├── idl │ └── volo_example.proto ├── rust-toolchain.toml ├── src │ ├── bin │ │ └── server.rs │ └── lib.rs └── volo-gen ├── Cargo.toml ├── build.rs ├── src │ └── lib.rs └── volo.yml 然后，我们打开 src/lib.rs，在 impl 块中加入方法的实现，最终的代码应该是这样的：\n#![feature(generic_associated_types)]#![feature(type_alias_impl_trait)]pubstruct S;#[volo::async_trait]implvolo_gen::volo::example::ItemServiceforS{// 这部分是我们需要增加的代码 asyncfn get_item(\u0026self,_req: volo_grpc::Request\u003cvolo_gen::volo::example::GetItemRequest\u003e,)-\u003e core::result::Result\u003cvolo_grpc::Response\u003cvolo_gen::volo::example::GetItemResponse\u003e,volo_grpc::Status\u003e{Ok(volo_grpc::Response::new(Default::default()))}}然后执行：\n$ cargo update $ cargo build 这时候，就会发现 OUT_DIR 目录下多出来一个 volo_gen.rs 的文件了。\n然后执行以下命令，即可把我们的 server 端跑起来：\n$ cargo run --bin server 至此，我们已经能把我们的 server 跑起来啦！\n","categories":"","description":"","excerpt":"Volo-gRPC 是一个 RPC 框架，既然是 RPC，底层就需要两大功能：\n Serialization 序列化 Transport 传 …","ref":"/zh/docs/volo/volo-grpc/getting-started/part_2/","tags":"","title":"Part 2. 创建一个 gRPC Server"},{"body":"Volo-Thrift 是一个 RPC 框架，既然是 RPC，底层就需要两大功能：\n Serialization 序列化 Transport 传输  IDL 全称是 Interface Definition Language，接口定义语言。\nWhy IDL 如果我们要进行 RPC，就需要知道对方的接口是什么，需要传什么参数，同时也需要知道返回值是什么样的，就好比两个人之间交流，需要保证在说的是同一个语言、同一件事。 这时候，就需要通过 IDL 来约定双方的协议，就像在写代码的时候需要调用某个函数，我们需要知道函数签名一样。\nThrift IDL 是一套跨语言的全栈式 RPC 解决方案，具体的语法可以看参考 thrift-missing-guide 或官方 Thrift interface description language。\n编写 IDL 为了创建一个 Thrift 项目，我们需要先编写一个 Thrift IDL。\n在你的工作目录下，我们先执行以下命令：\n$ mkdir volo-example $ cd volo-example $ mkdir idl $ vim idl/volo_example.thrift 随后，我们输入以下内容：\nnamespacersvolo.examplestructItem{1:requiredi64id,2:requiredstringtitle,3:requiredstringcontent,10:optionalmap\u003cstring,string\u003eextra,}structGetItemRequest{1:requiredi64id,}structGetItemResponse{1:requiredItemitem,}serviceItemService{GetItemResponseGetItem (1:GetItemRequestreq),}保存退出后，我们执行以下命令：\n$ volo init volo-example idl/volo_example.thrift 这里我们使用init命令，后面跟了我们项目的名字，意思是需要生成模板代码。在末尾，需要指定一个 IDL 表示 server 使用的 IDL。\n如果只需要增加一个 IDL（如 client 的 IDL）而不需要生成模板的话，如：\n$ volo idl add idl/volo_example.thrift | 插播一个广告，volo 工具还支持从 git 下载 IDL 并生成代码哦，如：\n$ volo idl add -g git@github.com:org/repo.git -r main /path/to/your/idl.thrift | 感兴趣可以直接输入 volo 看详细用法~ 接下来回到正题~\n这时候，我们整个目录的结构如下：\n. ├── Cargo.toml ├── idl │ └── volo_example.thrift ├── rust-toolchain.toml ├── src │ ├── bin │ │ └── server.rs │ └── lib.rs └── volo-gen ├── Cargo.toml ├── build.rs ├── src │ └── lib.rs └── volo.yml 然后，我们打开 src/lib.rs，在 impl 块中加入方法的实现，最终的代码应该是这样的：\n#![feature(generic_associated_types)]#![feature(type_alias_impl_trait)]pubstruct S;#[volo::async_trait]implvolo_gen::volo::example::ItemServiceforS{// 这部分是我们需要增加的代码 asyncfn get_item(\u0026self,_req: volo_gen::volo::example::GetItemRequest,)-\u003e core::result::Result\u003cvolo_gen::volo::example::GetItemResponse,pilota::AnyhowError\u003e{Ok(Default::default())}}然后执行：\n$ cargo update $ cargo build 这时候，就会发现 OUT_DIR 目录下多出来一个 volo_gen.rs 的文件了。\n然后执行以下命令，即可把我们的 server 端跑起来：\n$ cargo run --bin server 至此，我们已经能把我们的 server 跑起来啦！\n","categories":"","description":"","excerpt":"Volo-Thrift 是一个 RPC 框架，既然是 RPC，底层就需要两大功能：\n Serialization 序列化 Transport …","ref":"/zh/docs/volo/volo-thrift/getting-started/part_2/","tags":"","title":"Part 2. 创建一个 Thrift Server"},{"body":"概述 得益于 Hertz 的分层设计，除了 Hertz 框架默认自带的 HTTP1/HTTP2（即将开源）协议 server，框架的使用者能够非常容易的按照自身的需求增加/定制符合自身业务场景需求的协议处理逻辑。\n简单来说实现了以下接口的 server 即可作为自定义扩展 server 加入到 Hertz 当中来：\ntype Server interface { Serve(c context.Context, conn network.Conn) error } 协议层扩展三要素 协议层 server 初始化 前言里面提到的接口其实就是网络层将数据准备好之后的一个标准回调，即当有新的请求建立连接之后，进入到我们的协议层的处理逻辑。 在这个逻辑中我们可以自定义诸如协议解析方式，引入业务 Handler 执行，数据写回等协议层标准行为。这也是我们的自定义 server 的核心逻辑所在。\ntype myServer struct{ xxx xxx } func (s *myServer)Serve(c context.Context, conn network.Conn) error{ // 解析协议 \t... // 转到业务注册的逻辑函数（路由、中间件、Handler） \t... // 将数据写回 \t... } 定义一个协议处理逻辑就这么简单，不过解析协议、将数据写回这两个步骤通过入参中提供的 conn 接口能够轻易达成，但转到业务注册的逻辑函数这一步是如何办到的呢？\n与上层逻辑交互 一个完整的协议一定少不了引入业务逻辑控制（极少数特殊场景除外），在 Hertz 框架中自定义的协议是如何实现这部分能力的呢？其实，在自定义 server 初始化的过程中，框架已经天然的将这部分能力交给自定义协议 server 了。\ntype ServerFactory interface { New(core Core) (server protocol.Server, err error) } // Core is the core interface that promises to be provided for the protocol layer extensions type Core interface { // IsRunning Check whether engine is running or not  IsRunning() bool // A RequestContext pool ready for protocol server impl  GetCtxPool() *sync.Pool // Business logic entrance  // After pre-read works, protocol server may call this method  // to introduce the middlewares and handlers  ServeHTTP(c context.Context, ctx *app.RequestContext) // GetTracer for tracing requirement  GetTracer() tracer.Controller } 自定义 server 只需要按照以上接口实现一个协议 server 生成工厂即可，入参里面的 Core，其实就是包含了引入上层逻辑交互以及其他核心应用层接口的具体实现，在初始化自定义 server 的时候， 正常情况只需要将 Core 保存到 server 中，当需要转到业务逻辑时，通过 Core 即可将流程引导到应用层处理逻辑（路由、中间件、逻辑 Handler），当业务逻辑执行完毕返回后，即可根据业务数据进行进一步的数据包写回。\ntype myServer struct{ suite.Core xxx } func (s *myServer)Serve(c context.Context, conn network.Conn) error{ // 解析协议 \t... Core.ServeHTTP(c, ctx) // 将数据写回 \t... } 至此，一个自定义的协议层 server 就开发完毕了。\n注册自定义协议 server 到 Hertz 中 按照上述接口完成 server 生成工厂的开发后，将其加载到 Hertz 当中来就非常的容易了，我们在 Hertz 的核心引擎上面天然提供了一个注册自定义协议 server 的接口:\nfunc (engine *Engine) AddProtocol(protocol string, factory suite.ServerFactory) { engine.protocolSuite.Add(protocol, factory) } 只需要按照接口指定的参数将我们的自定义 server 生成工厂注册到 engine 上即可。值得注意的一点是，这里注册的 protocol（string）其实和 ALPN 中的协议协商 key 也是一一对应的， 所以，如果是想通过 ALPN 的方式接入自定义的协议 server，直接将 key 指定为对应的 ALPN 协商时的 key 即可。当前 Hertz 默认集成了一个 HTTP1 的协议 server（对应的 key 为\"http/1.1\"）， 如果有自定义 HTTP1 协议处理逻辑的需求，在 AddProtocol 时直接将 key 指定为\"http/1.1\"即可完成覆盖。\n例子 package main import ( \"bytes\" \"context\" \"github.com/cloudwego/hertz/pkg/app\" \"github.com/cloudwego/hertz/pkg/app/server\" \"github.com/cloudwego/hertz/pkg/common/errors\" \"github.com/cloudwego/hertz/pkg/common/hlog\" \"github.com/cloudwego/hertz/pkg/network\" \"github.com/cloudwego/hertz/pkg/protocol\" \"github.com/cloudwego/hertz/pkg/protocol/suite\" ) type myServer struct { suite.Core } func (m myServer) Serve(c context.Context, conn network.Conn) error { firstThreeBytes, _ := conn.Peek(3) if !bytes.Equal(firstThreeBytes, []byte(\"GET\")) { return errors.NewPublic(\"not a GET method\") } ctx := m.GetCtxPool().Get().(*app.RequestContext) defer func() { m.GetCtxPool().Put(ctx) conn.Skip(conn.Len()) conn.Flush() }() ctx.Request.SetMethod(\"GET\") ctx.Request.SetRequestURI(\"/test\") m.ServeHTTP(c, ctx) conn.WriteBinary([]byte(\"HTTP/1.1 200 OK\\n\" + \"Server: hertz\\n\" + \"Date: Sun, 29 May 2022 10:49:33 GMT\\n\" + \"Content-Type: text/plain; charset=utf-8\\n\" + \"Content-Length: 2\\n\\nok\\n\")) return nil } type serverFactory struct { } func (s *serverFactory) New(core suite.Core) (server protocol.Server, err error) { return \u0026myServer{ core, }, nil } func main() { h := server.New() h.GET(\"/test\", func(c context.Context, ctx *app.RequestContext) { hlog.Info(\"in handler\") }) h.AddProtocol(\"http/1.1\", \u0026serverFactory{}) h.Spin() } ","categories":"","description":"","excerpt":"概述 得益于 Hertz 的分层设计，除了 Hertz 框架默认自带的 HTTP1/HTTP2（即将开源）协议 server，框架的使用者能 …","ref":"/zh/docs/hertz/tutorials/framework-exten/advanced-exten/protocol/","tags":"","title":"协议扩展"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/blog/releases/","tags":"","title":"发布"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/hertz/tutorials/basic-feature/","tags":"","title":"基本特性"},{"body":"异常类型 Kitex 框架定义在 github.com/cloudwego/kitex/pkg/kerrors 下\nInternal Exception ErrInternalException, 框架内部发生的错误，具体包括以下几种：\n ErrNotSupported, \"operation not supported\"，进行了尚不支持的操作 ErrNoResolver, \"no resolver available\"，没有可用的 resolver ErrNoDestService, \"no dest service\"，没有指定目标 service ErrNoDestAddress, \"no dest address\"，没有指定目标地址 ErrNoConnection, \"no connection available\"，当前没有可用连接 ErrNoIvkRequest, \"invoker request not set\"，invoker 模式下调用时为设置 request  service discovery error ErrServiceDiscovery, 服务发现错误，具体错误见报错信息\nget connection error ErrGetConnection, 获取连接错误，具体错误见报错信息\nloadbalance error ErrLoadbalance, 负载均衡错误\nno more instances to retry ErrNoMoreInstance, 没有可供重试的示例，上一次调用的错误见报错信息\nrpc timeout ErrRPCTimeout, RPC 调用超时，具体错误见报错信息\nrequest forbidden ErrACL, 调用被拒绝，具体错误见报错信息\nforbidden by circuitbreaker ErrCircuitBreak, 发生熔断后请求被拒绝，通常包含两种错误：\n ErrServiceCircuitBreak, \"service circuitbreak\"，发生服务级别熔断后请求被拒绝 ErrInstanceCircuitBreak, \"instance circuitbreak\"，发生实例级别熔断后请求被拒绝  remote or network error ErrRemoteOrNetwork, 远端服务发生错误，或者出现网络错误。具体错误见报错信息\n当带有[remote] 字样时，代表此错误为远端返回\nrequest over limit ErrOverlimit, 过载保护错误。通常包括以下两种错误：\n ErrConnOverLimit, \"too many connections\"，连接过载，建立的连接超过限制 ErrQPSOverLimit, \"request too frequent\"，请求过载，请求数超过限制  panic ErrPanic, 服务发生 panic 。\n当带有[happened in biz handler] 字样时，代表 panic 发生在服务端 handler 中，此时错误信息中会带上堆栈。\nbiz error ErrBiz, 服务端 handler 返回的错误。\nretry error ErrRetry, 重试时发生错误，具体错误见报错信息。\nTHRIFT 错误码 该类别对应 Thrift 框架原生的 Application Exception 错误，通常，这些错误会被 Kitex 框架包装成 remote or network error 。\n   错误码 名称 含义     0 UnknownApplicationException 未知错误   1 UnknownMethod 未知方法   2 InValidMessageTypeException 无效的消息类型   3 WrongMethodName 错误的方法名字   4 BadSequenceID 错误的包序号   5 MissingResult 返回结果缺失   6 InternalError 内部错误   7 ProtocolError 协议错误    异常判断 判断是否是 Kitex 的错误 可以通过 kerrors 包提供的 IsKitexError 直接进行判断\nimport \"github.com/cloudwego/kitex/pkg/kerrors\" ... isKitexErr := kerrors.IsKitexError(kerrors.ErrInternalException) // 返回 true 判断具体的错误类型 可以通过 errors.Is 进行判断，其中详细错误可以通过详细错误判断，如：\nimport \"errors\" import \"github.com/cloudwego/kitex/client\" import \"github.com/cloudwego/kitex/pkg/kerrors\" ... _, err := echo.NewClient(\"echo\", client.WithResolver(nil)) // 返回 kerrors.ErrNoResolver ... isKitexErr := errors.Is(err, kerrors.ErrNoResolver) // 返回 true 也可以通过基本错误进行判断，如：\nimport \"errors\" import \"github.com/cloudwego/kitex/client\" import \"github.com/cloudwego/kitex/pkg/kerrors\" ... _, err := echo.NewClient(\"echo\", client.WithResolver(nil)) // 返回 kerrors.ErrNoResolver ... isKitexErr := errors.Is(err, kerrors.ErrInternalException) // 返回 true 特别的，timeout 错误可以通过 kerrors 包提供的 IsTimeoutError 进行判断\n获取更详细的错误信息 kerrors 中所有的具体错误类型都是 kerrors 包下的 DetailedError，故而可以通过 errors.As 获取到实际的 DetailedError，如：\nimport \"errors\" import \"github.com/cloudwego/kitex/client\" import \"github.com/cloudwego/kitex/pkg/kerrors\" ... _, err := echo.NewClient(\"echo\", client.WithResolver(nil)) // 返回 kerrors.ErrNoResolver ... var de *kerrors.DetailedError ok := errors.As(err, \u0026ke) // 返回 true if de.ErrorType() == kerrors.ErrInternalException {} // 返回 true DetailedError 提供了下述方法用于获取更详细的信息：\n ErrorType() error ，用于获取基本错误类型 Stack() string ，用于获取堆栈信息（目前仅 ErrPanic 会带上）  ","categories":"","description":"","excerpt":"异常类型 Kitex 框架定义在 github.com/cloudwego/kitex/pkg/kerrors 下\nInternal …","ref":"/zh/docs/kitex/reference/exception/","tags":"","title":"异常说明"},{"body":"准备 Golang 开发环境  如果您之前未搭建 Golang 开发环境，可以参考 Golang 安装。 推荐使用最新版本的 Golang，或保证现有 Golang 版本 \u003e= 1.15。小于 1.15 版本，可以自行尝试使用但不保障兼容性和稳定性。 确保打开 go mod 支持 (Golang \u003e= 1.15时，默认开启)。   目前，Hertz 支持 Linux、macOS、Windows 系统\n 快速上手 在完成环境准备后，本章节将帮助你快速上手 Hertz。\n安装命令行工具 hz 首先，我们需要安装使用本示例所需要的命令行工具 hz：\n 确保 GOPATH 环境变量已经被正确地定义（例如 export GOPATH=~/go）并且将$GOPATH/bin添加到 PATH 环境变量之中(例如 export PATH=$GOPATH/bin:$PATH)；请勿将 GOPATH 设置为当前用户没有读写权限的目录 安装 hz：go install github.com/cloudwego/hertz/cmd/hz@latest  更多 hz 使用方法可参考: hz\n确定代码放置位置  若将代码放置于$GOPATH/src下，需在$GOPATH/src下创建额外目录，进入该目录后再获取代码：  $ mkdir -p $(go env GOPATH)/src/github.com/cloudwego $ cd $(go env GOPATH)/src/github.com/cloudwego 若将代码放置于 GOPATH 之外，可直接获取  生成/编写示例代码  在当前目录下创建 hertz_demo 文件夹，进入该目录中 生成代码 hz new 整理 \u0026 拉取依赖  $ go mod tidy 如果当前使用的是 Windows 环境，可以编写如下的示例代码：\n 在当前目录下创建 hertz_demo 文件夹，进入该目录中 创建 main.go 文件 在 main.go 文件中添加以下代码  package main import ( \"context\" \"github.com/cloudwego/hertz/pkg/app\" \"github.com/cloudwego/hertz/pkg/app/server\" \"github.com/cloudwego/hertz/pkg/common/utils\" \"github.com/cloudwego/hertz/pkg/protocol/consts\" ) func main() { h := server.Default() h.GET(\"/ping\", func(c context.Context, ctx *app.RequestContext) { ctx.JSON(consts.StatusOK, utils.H{\"ping\": \"pong\"}) }) h.Spin() } 生成 go.mod 文件  $ go mod init hertz_demo 整理 \u0026 拉取依赖  $ go mod tidy 运行示例代码 完成以上操作后，我们可以直接编译并启动 Server\n$ go build -o hertz_demo \u0026\u0026 ./hertz_demo 如果成功启动，你将看到以下信息\n2022/05/17 21:47:09.626332 engine.go:567: [Debug] HERTZ: Method=GET absolutePath=/ping --\u003e handlerName=main.main.func1 (num=2 handlers) 2022/05/17 21:47:09.629874 transport.go:84: [Info] HERTZ: HTTP server listening on address=[::]:8888 接下来，我们可以对接口进行测试\n$ curl http://127.0.0.1:8888/ping 如果不出意外，我们可以看到类似如下输出\n$ {\"message\":\"pong\"} 到现在，我们已经成功启动了 Hertz Server，并完成了一次调用。更多 API 示例请参考 API 示例\n目录结构 关于项目目录结构组织，这里有一个目录结构可供参考，具体可以根据业务的实际情况进行组织\n更多示例 参考：hertz-examples\n","categories":"","description":"","excerpt":"准备 Golang 开发环境  如果您之前未搭建 Golang 开发环境，可以参考 Golang 安装。 推荐使用最新版本的 Golang， …","ref":"/zh/docs/hertz/getting-started/","tags":"","title":"快速开始"},{"body":"准备 Golang 开发环境  如果您之前未搭建 Golang 开发环境， 可以参考 Golang 安装 推荐使用最新版本的 Golang，我们保证最新三个正式版本的兼容性(现在 \u003e= v1.16)。 确保打开 go mod 支持 (Golang \u003e= 1.15时，默认开启) kitex 暂时没有针对 Windows 做支持，如果本地开发环境是 Windows 建议使用 WSL2  快速上手 在完成环境准备后，本章节将帮助你快速上手 Kitex\n安装代码生成工具 首先，我们需要安装使用本示例所需要的命令行代码生成工具：\n 确保 GOPATH 环境变量已经被正确地定义（例如 export GOPATH=~/go）并且将$GOPATH/bin添加到 PATH 环境变量之中（例如 export PATH=$GOPATH/bin:$PATH）；请勿将 GOPATH 设置为当前用户没有读写权限的目录 安装 kitex：go install github.com/cloudwego/kitex/tool/cmd/kitex@latest 安装 thriftgo：go install github.com/cloudwego/thriftgo@latest  安装成功后，执行 kitex --version 和 thriftgo --version 应该能够看到具体版本号的输出（版本号有差异，以 x.x.x 示例）：\n$ kitex --version vx.x.x $ thriftgo --version thriftgo x.x.x 如果在安装阶段发生问题，可能主要是由于对 Golang 的不当使用造成，请依照报错信息进行检索  确定代码放置位置  若将代码放置于 $GOPATH/src 下，需在 $GOPATH/src 下创建额外目录，进入该目录后再获取代码：  mkdir -p $(go env GOPATH)/src/github.com/cloudwego cd $(go env GOPATH)/src/github.com/cloudwego 若将代码放置于 GOPATH 之外，可直接获取  获取示例代码  你可以直接点击 此处 下载示例仓库 也可以克隆该示例仓库到本地 git clone https://github.com/cloudwego/kitex-examples.git  运行示例代码 方式一：直接启动   进入示例仓库的 hello 目录\ncd kitex-examples/hello\n  运行 server\ngo run .\n  运行 client\n另起一个终端后，go run ./client\n  方式二：使用 Docker 快速启动   进入示例仓库目录\ncd kitex-examples\n  编译项目\ndocker build -t kitex-examples .\n  运行 server\ndocker run --network host kitex-examples ./hello-server\n  运行 client\n另起一个终端后，docker run --network host kitex-examples ./hello-client\n  恭喜你，你现在成功通过 Kitex 发起了 RPC 调用。\n增加一个新的方法 打开 hello.thrift，你会看到如下内容：\nnamespacegoapistructRequest{1:stringmessage}structResponse{1:stringmessage}serviceHello{Responseecho(1:Requestreq)}现在让我们为新方法分别定义一个新的请求和响应，AddRequest 和 AddResponse，并在 service Hello 中增加 add 方法：\nnamespacegoapistructRequest{1:stringmessage}structResponse{1:stringmessage}structAddRequest{1:i64first2:i64second}structAddResponse{1:i64sum}serviceHello{Responseecho(1:Requestreq)AddResponseadd(1:AddRequestreq)}完成之后 hello.thrift 的内容应该和上面一样。\n重新生成代码 运行如下命令后，kitex 工具将根据 hello.thrift 更新代码文件。\nkitex -service a.b.c hello.thrift # 若当前目录不在 $GOPATH/src 下，需要加上 -module 参数，一般为 go.mod 下的名字 kitex -module \"your_module_name\" -service a.b.c hello.thrift 执行完上述命令后，kitex 工具将更新下述文件\n 更新 ./handler.go，在里面增加一个 Add 方法的基本实现 更新 ./kitex_gen，里面有框架运行所必须的代码文件  更新服务端处理逻辑 上述步骤完成后，./handler.go 中会自动补全一个 Add 方法的基本实现，类似如下代码：\n// Add implements the HelloImpl interface. func (s *HelloImpl) Add(ctx context.Context, req *api.AddRequest) (resp *api.AddResponse, err error) { // TODO: Your code here...  return } 让我们在里面增加我们所需要的逻辑，类似如下代码：\n// Add implements the HelloImpl interface. func (s *HelloImpl) Add(ctx context.Context, req *api.AddRequest) (resp *api.AddResponse, err error) { // TODO: Your code here...  resp = \u0026api.AddResponse{Sum: req.First + req.Second} return } 增加客户端调用 服务端已经有了 Add 方法的处理，现在让我们在客户端增加对 Add 方法的调用。\n在 ./client/main.go 中你会看到类似如下的 for 循环：\nfor { req := \u0026api.Request{Message: \"my request\"} resp, err := client.Echo(context.Background(), req) if err != nil { log.Fatal(err) } log.Println(resp) time.Sleep(time.Second) } 现在让我们在里面增加 Add 方法的调用：\nfor { req := \u0026api.Request{Message: \"my request\"} resp, err := client.Echo(context.Background(), req) if err != nil { log.Fatal(err) } log.Println(resp) time.Sleep(time.Second) addReq := \u0026api.AddRequest{First: 512, Second: 512} addResp, err := client.Add(context.Background(), addReq) if err != nil { log.Fatal(err) } log.Println(addResp) time.Sleep(time.Second) } 重新运行示例代码 关闭之前运行的客户端和服务端之后\n 运行 server  go run .\n运行 client  另起一个终端后，go run ./client\n现在，你应该能看到客户端在调用 Add 方法了。\n基础教程 关于 Kitex Kitex 是一个 RPC 框架，既然是 RPC，底层就需要两大功能：\n Serialization 序列化 Transport 传输  Kitex 框架及命令行工具，默认支持 thrift 和 proto3 两种 IDL，对应的 Kitex 支持 thrift 和 protobuf 两种序列化协议。传输上 Kitex 使用扩展的 thrift 作为底层的传输协议（注：thrift 既是 IDL 格式，同时也是序列化协议和传输协议）。IDL 全称是 Interface Definition Language，接口定义语言。\n为什么要使用 IDL 如果我们要进行 RPC，就需要知道对方的接口是什么，需要传什么参数，同时也需要知道返回值是什么样的，就好比两个人之间交流，需要保证在说的是同一个语言、同一件事。这时候，就需要通过 IDL 来约定双方的协议，就像在写代码的时候需要调用某个函数，我们需要知道函数签名一样。\nThrift IDL 语法可参考：Thrift interface description language。\nproto3 语法可参考：Language Guide(proto3)。\n创建项目目录 在开始后续的步骤之前，先让我们创建一个项目目录用于后续的教程。\n$ mkdir example\n然后让我们进入项目目录\n$ cd example\nKitex 命令行工具 Kitex 自带了一个同名的命令行工具 kitex，用来帮助大家很方便地生成代码，新项目的生成以及之后我们会学到的 server、client 代码的生成都是通过 kitex 工具进行。\n安装 可以使用以下命令来安装或者更新 kitex：\n$ go install github.com/cloudwego/kitex/tool/cmd/kitex\n完成后，可以通过执行 kitex 来检测是否安装成功。\n$ kitex\n如果出现如下输出，则安装成功。\n$ kitex\nNo IDL file found.\n如果出现 command not found 错误，可能是因为没有把 $GOPATH/bin 加入到 $PATH 中，详见环境准备一章。\n使用 kitex 的具体使用请参考代码生成工具\n编写 IDL 首先我们需要编写一个 IDL，这里以 thrift IDL 为例。\n首先创建一个名为 echo.thrift 的 thrift IDL 文件。\n然后在里面定义我们的服务\nnamespacegoapistructRequest{1:stringmessage}structResponse{1:stringmessage}serviceEcho{Responseecho(1:Requestreq)}生成 echo 服务代码 有了 IDL 以后我们便可以通过 kitex 工具生成项目代码了，执行如下命令：\n$ kitex -module example -service example echo.thrift\n上述命令中，-module 表示生成的该项目的 go module 名，-service 表明我们要生成一个服务端项目，后面紧跟的 example 为该服务的名字。最后一个参数则为该服务的 IDL 文件。\n生成后的项目结构如下：\n. |-- build.sh |-- echo.thrift |-- handler.go |-- kitex_gen | `-- api | |-- echo | | |-- client.go | | |-- echo.go | | |-- invoker.go | | `-- server.go | |-- echo.go | `-- k-echo.go |-- main.go `-- script |-- bootstrap.sh `-- settings.py 获取最新的 Kitex 框架 由于 kitex 要求使用 go mod 进行依赖管理，所以我们要升级 kitex 框架会很容易，只需要执行以下命令即可：\n$ go get github.com/cloudwego/kitex@latest $ go mod tidy 如果遇到类似如下报错：\ngithub.com/apache/thrift/lib/go/thrift: ambiguous import: found package github.com/apache/thrift/lib/go/thrift in multiple modules\n先执行一遍下述命令，再继续操作：\ngo mod edit -droprequire=github.com/apache/thrift/lib/go/thrift go mod edit -replace=github.com/apache/thrift=github.com/apache/thrift@v0.13.0 编写 echo 服务逻辑 我们需要编写的服务端逻辑都在 handler.go 这个文件中，现在这个文件应该如下所示：\npackage main import ( \"context\" \"example/kitex_gen/api\" ) // EchoImpl implements the last service interface defined in the IDL. type EchoImpl struct{} // Echo implements the EchoImpl interface. func (s *EchoImpl) Echo(ctx context.Context, req *api.Request) (resp *api.Response, err error) { // TODO: Your code here...  return } 这里的 Echo 函数就对应了我们之前在 IDL 中定义的 echo 方法。\n现在让我们修改一下服务端逻辑，让 Echo 服务名副其实。\n修改 Echo 函数为下述代码：\nfunc (s *EchoImpl) Echo(ctx context.Context, req *api.Request) (resp *api.Response, err error) { return \u0026api.Response{Message: req.Message}, nil } 编译运行 kitex 工具已经帮我们生成好了编译和运行所需的脚本：\n编译：\n$ sh build.sh\n执行上述命令后，会生成一个 output 目录，里面含有我们的编译产物。\n运行：\n$ sh output/bootstrap.sh\n执行上述命令后，Echo 服务就开始运行啦！\n编写客户端 有了服务端后，接下来就让我们编写一个客户端用于调用刚刚运行起来的服务端。\n首先，同样的，先创建一个目录用于存放我们的客户端代码：\n$ mkdir client\n进入目录：\n$ cd client\n创建一个 main.go 文件，然后就开始编写客户端代码了。\n创建 client 首先让我们创建一个调用所需的 client：\nimport \"example/kitex_gen/api/echo\" import \"github.com/cloudwego/kitex/client\" ... c, err := echo.NewClient(\"example\", client.WithHostPorts(\"0.0.0.0:8888\")) if err != nil { log.Fatal(err) } 上述代码中，echo.NewClient 用于创建 client，其第一个参数为调用的 服务名，第二个参数为 options，用于传入参数，此处的 client.WithHostPorts 用于指定服务端的地址，更多参数可参考基本特性一节。\n发起调用 接下来让我们编写用于发起调用的代码：\nimport \"example/kitex_gen/api\" ... req := \u0026api.Request{Message: \"my request\"} resp, err := c.Echo(context.Background(), req, callopt.WithRPCTimeout(3*time.Second)) if err != nil { log.Fatal(err) } log.Println(resp) 上述代码中，我们首先创建了一个请求 req , 然后通过 c.Echo 发起了调用。\n其第一个参数为 context.Context，通过通常用其传递信息或者控制本次调用的一些行为，你可以在后续章节中找到如何使用它。\n其第二个参数为本次调用的请求。\n其第三个参数为本次调用的 options ，Kitex 提供了一种 callopt 机制，顾名思义——调用参数 ，有别于创建 client 时传入的参数，这里传入的参数仅对此次生效。 此处的 callopt.WithRPCTimeout 用于指定此次调用的超时（通常不需要指定，此处仅作演示之用）同样的，你可以在基本特性一节中找到更多的参数。\n发起调用 在编写完一个简单的客户端后，我们终于可以发起调用了。\n你可以通过下述命令来完成这一步骤：\n$ go run main.go\n如果不出意外，你可以看到类似如下输出：\n2021/05/20 16:51:35 Response({Message:my request})\n恭喜你！至此你成功编写了一个 Kitex 的服务端和客户端，并完成了一次调用！\n","categories":"","description":"","excerpt":"准备 Golang 开发环境  如果您之前未搭建 Golang 开发环境， 可以参考 Golang 安装 推荐使用最新版本的 Golang， …","ref":"/zh/docs/kitex/getting-started/","tags":"","title":"快速开始"},{"body":" 本教程通过一些简单的示例帮助您开始使用 Netpoll，包括如何使用 Server、Client 和 nocopy API。\n 1. 使用 Server 这里 是一个简单的 server 例子，接下来我们会解释它是如何构建的。\n1.1 创建 Listener 首先我们需要一个 Listener，它可以是 net.Listener 或者 netpoll.Listener，两者都可以，依据你的代码情况自由选择。 创建 Listener 的过程如下：\npackage main import \"net\" func main() { listener, err := net.Listen(network, address) if err != nil { panic(\"create net listener failed\") } ... } 或者\npackage main import \"github.com/cloudwego/netpoll\" func main() { listener, err := netpoll.CreateListener(network, address) if err != nil { panic(\"create netpoll listener failed\") } ... } 1.2 创建 EventLoop EventLoop 是一个事件驱动的调度器，一个真正的 NIO Server，负责连接管理、事件调度等。\n参数说明:\n OnRequest 是用户应该自己实现来处理业务逻辑的接口。 注释 详细描述了它的行为。 Option 用于自定义 EventLoop 创建时的配置，下面的例子展示了它的用法。更多详情请参考 options 。  创建过程如下：\npackage main import ( \"time\" \"github.com/cloudwego/netpoll\" ) var eventLoop netpoll.EventLoop func main() { ... eventLoop, _ = netpoll.NewEventLoop( handle, netpoll.WithOnPrepare(prepare), netpoll.WithReadTimeout(time.Second), ) ... } 1.3 运行 Server EventLoop 通过绑定 Listener 来提供服务，如下所示。Serve 方法为阻塞式调用，直到发生 panic 等错误，或者由用户主动调用 Shutdown 时触发退出。\npackage main import ( \"github.com/cloudwego/netpoll\" ) var eventLoop netpoll.EventLoop func main() { ... // start listen loop ... \teventLoop.Serve(listener) } 1.4 关闭 Server EventLoop 提供了 Shutdown 功能，用于优雅地停止服务器。用法如下：\npackage main import ( \"context\" \"time\" \"github.com/cloudwego/netpoll\" ) var eventLoop netpoll.EventLoop func main() { // stop server ... \tctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() eventLoop.Shutdown(ctx) } 2. 使用 Dialer Netpoll 也支持在 Client 端使用，提供了 Dialer，类似于 net.Dialer。同样的，这里 展示了一个简单的 Client 端示例，接下来我们详细介绍一下：\n2.1 快速方式 与 Net 类似，Netpoll 提供了几个用于直接建立连接的公共方法，可以直接调用。 如：\nDialConnection(network, address string, timeout time.Duration) (connection Connection, err error) DialTCP(ctx context.Context, network string, laddr, raddr *TCPAddr) (*TCPConnection, error) DialUnix(network string, laddr, raddr *UnixAddr) (*UnixConnection, error) 2.2 创建 Dialer Netpoll 还定义了Dialer 接口。 用法如下：（通常推荐使用上一节的快速方式）\npackage main import ( \"github.com/cloudwego/netpoll\" ) func main() { // Dial a connection with Dialer. \tdialer := netpoll.NewDialer() conn, err := dialer.DialConnection(network, address, timeout) if err != nil { panic(\"dial netpoll connection failed\") } ... } 3. 使用 Nocopy API Connection 提供了 Nocopy API —— Reader 和 Writer，以避免频繁复制。下面介绍一下它们的简单用法。\npackage main type Connection interface { // Recommended nocopy APIs \tReader() Reader Writer() Writer ... // see code comments for more details } 3.1 简单用法 Nocopy API 设计为两步操作。\n使用 Reader 时，通过 Next、Peek、ReadString 等方法读取数据后，还需要主动调用 Release 方法释放 buffer（Nocopy 读取 buffer 的原地址，所以您必须主动再次确认 buffer 已经不再使用）。\n同样，使用 Writer 时，首先需要分配一个 []byte 来写入数据，然后调用 Flush 确认所有数据都已经写入。Writer 还提供了丰富的 API 来分配 buffer，例如 Malloc、WriteString 等。\n下面是一些简单的读写数据的例子。 更多详情请参考 说明 。\npackage main import ( \"github.com/cloudwego/netpoll\" ) func main() { var conn netpoll.Connection var reader, writer = conn.Reader(), conn.Writer() // reading \tbuf, _ := reader.Next(n) ... parse the read data ... reader.Release() // writing \tvar write_data []byte ... make the write data ... alloc, _ := writer.Malloc(len(write_data)) copy(alloc, write_data) // write data \twriter.Flush() } 3.2 高阶用法 如果你想使用单个连接来发送（或接收）多组数据（如连接多路复用），那么你将面临数据打包和分包。在 net 上，这种工作一般都是通过复制来完成的。一个例子如下：\npackage main import ( \"net\" ) func main() { var conn net.Conn var buf = make([]byte, 8192) // reading \tfor { n, _ := conn.Read(buf) ... unpacking \u0026 handling ... var i int for i = 0; i \u003c= n-pkgsize; i += pkgsize { pkg := append([]byte{}, buf[i:i+pkgsize]...) go func() { ... handling pkg ... } } buf = append(buf[:0], buf[i:n]...) } // writing \tvar write_datas \u003c-chan []byte ... packing write ... for { pkg := \u003c-write_datas conn.Write(pkg) } } 但是，Netpoll 不需要这样做，nocopy APIs 支持对 buffer 进行原地址操作（原地址组包和分包），并通过引用计数实现资源的自动回收和重用。\n示例如下（使用方法 Reader.Slice 和 Writer.Append）：\npackage main import ( \"github.com/cloudwego/netpoll\" ) func main() { var conn netpoll.Connection // reading \treader := conn.Reader() for { ... unpacking \u0026 handling ... pkg, _ := reader.Slice(pkgsize) go func() { ... handling pkg ... pkg.Release() } } // writing \tvar write_datas \u003c-chan netpoll.Writer ... packing write ... writer := conn.Writer() for { select { case pkg := \u003c-write_datas: writer.Append(pkg) default: if writer.MallocLen() \u003e 0 { writer.Flush() } } } } ","categories":"","description":"","excerpt":" 本教程通过一些简单的示例帮助您开始使用 Netpoll，包括如何使用 Server、Client 和 nocopy API。\n 1. …","ref":"/zh/docs/netpoll/getting-started/","tags":"","title":"快速开始"},{"body":"Hertz 提供对日志的扩展，接口定义在 pkg/common/hlog 中。\n接口定义 Hertz 在 pkg/common/hlog 里定义了 Logger、CtxLogger、FormatLogger 几个接口实现不同的打日志方式，并定义了一个 Control 接口实现 logger 的控制。 用户注入自己的 logger 实现时需要实现上面的所有接口( FullLogger )。Hertz提供了一个 FullLogger 默认实现。\n// FullLogger is the combination of Logger, FormatLogger, CtxLogger and Control. type FullLogger interface { Logger FormatLogger CtxLogger Control } 注意，由于默认 logger 底层使用标准库的 log.Logger 实现，其在日志里输出的调用位置依赖于设置的调用深度（call depth），因此封装 hlog 提供的实现可能会导致日志内容里文件名和行数不准确。\n注入自己的 logger 实现 Hertz 提供 SetLogger 接口用于注入用户自定义的 logger 实现，也可以使用 SetOutput 接口重定向默认的 logger 输出，随后的中间件以及框架的其他部分可以使用 hlog 中的全局方法来输出日志。 默认使用 hertz 默认实现的 logger。\n","categories":"","description":"","excerpt":"Hertz 提供对日志的扩展，接口定义在 pkg/common/hlog 中。\n接口定义 Hertz 在 pkg/common/hlog 里 …","ref":"/zh/docs/hertz/tutorials/framework-exten/log/","tags":"","title":"日志扩展"},{"body":"案例介绍   近些年电商行业高速发展，森马电商线上业务激增，面临着高并发、高性能的业务场景需求。森马通过使用 Kitex 接入 Istio，极大地提高了对高并发需求的处理能力。\n本文将从四个方面为大家讲解 Kitex 在森马电商场景下的落地实践：\n 森马电商订单流转中心——天枢所面临的业务挑战； 项目的技术选型过程； 项目上线性能压测对比； CloudWeGo 团队的技术支持。  森马电商订单流转中心——天枢 业务增长 第一部分给大家介绍订单流转中心——天枢。天枢的主要功能是对接各大电商平台，把订单、商品、退单等信息统一处理后流转到下游系统，是下游系统和平台对接的中间枢纽。 目前森马电商在运营的电商平台几十家，如：天猫、抖店、京东、拼多多等，由于每个平台的接口和对接的方式不统一，我们专门开发了这套系统，去统一对接电商平台，然后把数据处理成统一的格式发到下游系统，如：OMS 和 WMS。 该系统在电商活动，如 6·18，双十一等订单峰值流量下发挥了重要作用。\n从 2015 年至 2021 年，森马的双十一业务量增长非常迅速。2015 年双十一的业绩有 3 亿+，而去年的双十一业绩为 20 亿+，2021 年商品交易总额（GMV） 更是突破百亿。 随着业务的增长，对订单系统的性能和稳定性要求越来越高。而且随着系统的规模增长，集群内的 Pod 数量和 Service 不断增加，对系统底层架构有很大的考验。 目前从旧系统迁移的平台有：有赞、抖音、拼多多、快手等，集群内的 Pod 数已经超过 200 个，后续会接入京东、唯品会、天猫等平台后，Pod 数会成倍的增长，更需要一个成熟的系统架构作为支撑。\n面临的问题 随着直播行业的兴起，我们请了一些网红主播和流量明星来直播带货。直播期间，订单量经常会出现几秒内突然爆发的情况，订单推送到系统后，如果系统处理较慢，订单就不能及时流入下游系统， 下游系统的 OMS 不知道已经产生如此大的订单量，就会出现不能同步的情况，即超卖现象。在电商行业，超卖是很严重的问题，如果用户下单后不能及时发货，不仅需要大量的人力去跟客户解释道歉， 也要以优惠券等形式赔偿用户遭受的损失，甚至会接到大量投诉，严重影响我们在电商平台的信誉，电商平台也会对我们进行处罚。我们经历过当 GMV 超过千万时，订单系统延迟超过半个小时的情况，对我们造成了极大的影响。 因此，当遇到如双十一，6·18 大促等活动时，特别是在直播时订单量短时间内暴增的情况下，我们原有的系统架构已经无法支撑，不能及时处理订单数据。这影响了我们发货及库存同步，间接地产生了不同类型的资损。\n技术挑战 我们在技术上面临的挑战主要有以下三个方面：\n 高并发。在电商业务场景下，不管是面向用户，比如秒杀，还是面向业务，比如订单处理，如果实现不了高并发，系统就很难做大，很难适应业务的增长。 高性能。除了用高并发来实现业务的快速处理外，性能也是一个挑战。例如在当前疫情状态下，各行各业都在降本增效，解决不了性能问题，就会不断地增加服务器资源，大大增加企业成本。 技术保障。我们电商行业的公司，大多资源和精力都在销售端，运营端，技术方面投入相对薄弱。因此在技术选型上需要从可靠、安全、支持等维度去考量。  项目的技术选型 如何选择 在开发语言的选择方面，开发语言没有好坏之分，只有这个语言在相关场景下合适不合适的问题。我们从性能、多线程、编译、效率等方面综合考虑，选择了 Golang。\n在微服务框架的选择方面，团队分别用 Google 开源的 gRPC 和字节跳动开源的 CloudWeGo-Kitex 做了技术评估和性能压测。经过专业测试同学的压力测试，最终选择了 CloudWeGo-Kitex 作为我们的微服务框架。\n选择 Kitex 的原因主要有两点。第一是 Kitex 背后有强大的技术团队提供及时有效的技术支持。第二是经过压力测试，Kitex 的性能优于其他微服务框架。\n关于微服务 使用微服务框架，一定会涉及到选择第三方开源的服务注册中心，那么是选择常用的开源注册中心（Zookeeper、Eureka、Nacos、Consul 和 ETCD）， 还是直接选择云原生的服务网格（Istio）？那么我从流量转发、服务注册和服务发现维度介绍一下微服务集群的两种形式。\n第一种是 Kubernetes Native，Kubernetes 集群中的每个节点都部署了一个 Kube-proxy 组件，该组件与 Kubernetes API Server 进行通信，观测服务和节点中的变化，进行负载均衡的转发。 这种开源注册中心默认使用 TCP 协议，由于 K8s 负载均衡不支持 RPC 协议（HTTP2），因而需要额外的第三方服务注册中心支持。\n第二种是基于 Istio 的服务网格，它并不需要额外的注册中心组件支持。Istio 接管了 K8s 的网络，通过 Sidecar Proxy 的方式将 Kubernetes 中的流量控制从服务层中抽离出来， Istio 基于 Enovy 的 xDS 协议扩展了其控制平面，每个 Pod 中放入原有的 Kube-proxy 路由转发功能。Istio 具备了流量管理、策略控制、可观察性等特点，将“应用程序”与“网络”解耦，因此不需要额外使用第三方注册中心。\n 那么这两种服务注册与发现的流程是怎样的呢？ 下图中左侧就是常用的服务注册中心使用流程。目标服务先把实例注册到服务注册中心，客户端从服务注册中心拿到目标实例的数据，根据负载均衡策略去选择一个服务实例，完成整个请求。 右侧是使用了基于 Istio 的服务网格。大概流程是 Client 访问目标服务的时候，流量先进入 Service 的 Proxy，被 Proxy 拦截，Proxy 会从服务发现（Pilot）拿到服务与服务实例的映射关系， 同时会拿到负载均衡的策略，去选择 Service 一个实例。总体来看，这两种流程大致相同，但实现方式有所差别，各有所长。\n天枢系统基本架构 像抖音、快手、拼多多和有赞等这样成熟的平台在产生订单时，都会将订单以消息推送的形式发送到服务网格中。我们先后通过 Ingress Gateway 网格入口管理程序、VirtualService 把订单转发到网格的不同服务中， 内部再通过不同服务之间进行调用。其中，Kitex 作为微服务的 RPC 框架，服务发现和服务注册均是基于云原生的服务网格 Istio。\nKitex 接入 Istio 那么 Kitex 接入 Istio 是怎么实现的呢？如下图所示，服务端注册服务之后，在创建客户端的时候，客户端的 Server-host 要写实际集群中的内网地址，例如：server-douyin.default.svc.cluster.local，如上文所说，不用再搭配第三方的服务注册中心。\n由于 Kitex 使用 gRPC 协议，在创建客户端的时候需指定使用 gRPC 协议：\n在 Istio 中怎么部署我们的客户端或者服务端呢？有以下两种方式：\n 为命名空间开启自动注入：kubectl label namespace default istio-injection=enabled。注入之后会产生两个重要的容器，第一个是 Istio-proxy，负责流量拦截和流量代理，比如做流量转发；第二个是 Server-douyin，是负责开发的应用容器。  把 Go 代码打包的镜像部署到集群中： 例如我们创建了一个 Deployment，名为 Server-douyin，另外作为服务端需要创建相应的 Service。  压测对比 我们将 Kitex 和 gRPC 在以下相同服务器硬件资源和网络环境下进行了压测对比：\n 压测工具：JMeter； 阿里云 ECS （8 vCPU，16 GiB，5 台）； 集群：Kubernetes 1.20.11； 服务网格：Istio v1.10.5.39。  通过对比发现，在指定时间相同的情况下，Kitex 在单位时间内处理订单数量更多。在指定订单数量的情况下，Kitex 对于处理相同数量的订单所需时间更短，且订单量越大，这种性能差别越明显。总体来看，Kitex 在处理大批订单时优势还是非常突出的。\nKitex 产生性能优势的原因 CloudWeGo 团队来森马做技术支持时讲到对自研网络库 Netpoll 做了一些性能优化，比如：\n 连接利用率； 调度延迟优化； 优化 I/O 调用； 序列化/反序列化优化； …….  更多资料可以查看 CloudWeGo 官网或参考官网博客\nCloudWeGo 团队的技术支持 我们选择 Kitex 之后，CloudWeGo 技术团队给予了足够的技术支持，包括现场支持和远程协助。这也让我们对使用 Kitex 有了信心，不管遇到什么样的技术难题，都会有强大的技术团队来协助解决。\n后续规划 Thrift 和 Protobuf 如何选择 我们在项目初期选择 gRPC 协议 Protobuf 是因为选择了 Istio 服务网格，而选择 Istio 服务网格主要是因为它有多流量转发和服务治理等功能，例如在电商场景下， 不同平台的推送消息都可以通过 VirtualService 转发到不同的服务，相当方便。但是目前每个 Pod 中放入原有的 Kube-proxy 路由转发功能，会增加响应延迟。由于 Sidecar 拦截流量时跳数更多，会消耗更多的资源。\n而对于 Thrift，它是 Kitex 默认支持的协议，字节官方对它做了很多性能上的优化，如：使用 SIMD 优化 Thrift 编码，减少函数调用，减少内存操作等，还开源了高性能 Thrift 编解码器 Frugal， Frugal 具有无需生成代码、高性能（在多核场景下，Frugal 的性能可以达到传统编解码方式的 5 倍）和稳定性等特点，进一步提升了性能和开发效率。\n因此，我们目前也在考虑在下一次系统版本的架构中改用 Thrift 协议。\n服务、合作共赢 我们开发的电商相关产品不仅可以为自己电商品牌所使用，产品成熟后还可以服务于其他相似的电商公司。后续我们也希望能够和 Kitex 官方有更深的技术合作，为社区带来更大价值。\n","categories":"","description":"","excerpt":"案例介绍   近些年电商行业高速发展，森马电商线上业务激增，面临着高并发、高性能的业务场景需求。森马通过使用 Kitex 接入 Istio， …","ref":"/zh/cooperation/semir/","tags":"","title":"Kitex 在森马电商场景的落地实践"},{"body":" SUPPORT 漏洞管理   # 漏洞响应   CloudWeGo 社区非常重视社区版本的安全性，CloudWeGo 安全委员会负责接收、调查和披露 CloudWeGo 社区相关的安全漏洞。我们鼓励漏洞研究人员和行业组织主动将 CloudWeGo 社区的疑似安全漏洞报告给 CloudWeGo 社区安全委员会。我们会快速的响应、分析和解决上报的安全问题或安全漏洞。\n# 支持版本 漏洞响应流程主要支持 CloudWeGo 社区各个子项目的最新版本，如果您还没有升级请尽快升级。\n# 漏洞处理流程 每个一个安全漏洞都会有一个指定的人员进行跟踪和处理，协调员是 CloudWeGo 安全委员会的成员，他将负责跟踪和推动漏洞的修复和披露。漏洞端到端的处理流程如下图。\n在这里我们主要介绍流程中漏洞上报、漏洞评估和漏洞披露这三部分内容。\n# 漏洞上报 如果您认为 CloudWeGo 产品存在一个疑似安全漏洞，我们希望您将漏洞上报给 CloudWeGo 社区，并与我们配合以负责任的方式修复和披露该问题。\n# 漏洞上报方式 您可以通过 email 将 CloudWeGo 产品的潜在安全漏洞发送到 CloudWeGo 安全团队邮箱（security@cloudwego.io）。 # 漏洞上报内容 为了便于快速的确认和验证疑似漏洞，请在漏洞上报邮件中包含但不限于以下内容：\n 基本信息：包括漏洞影响的模块、漏洞的触发条件和成功利用后对系统的影响等。\n 技术细节：包括系统配置、定位方法、Exploit 的描述、POC、问题重现方法和步骤等。\n 修复方案建议。\n 上报者的组织和联系方式。\n 上报者可能的漏洞披露计划。\n  # 邮件响应时间 我们将在48小时内响应通过邮箱上报的疑似安全漏洞，并向上报者反馈漏洞处理的进展。\n# 漏洞严重性评估  业界普遍使用 CVSS 标准评估漏洞的严重性，CloudWeGo 在使用 CVSSv3 进行漏洞评估时，需要设定漏洞攻击场景，基于在该攻击场景下的实际影响进行评估。漏洞严重等级评估是指针对漏洞利用难易程度，以及利用后对机密性、完整性、可用性的影响进行评估，并生成一个评分值。\n# 评估标准 CloudWeGo 社区采用 CVSS v3 对漏洞进行评估，CVSS V3 由通过对以下向量来评估一个漏洞的影响：\n 攻击向量（AV）-表示攻击的“远程性”以及如何利用此漏洞。\n 攻击复杂性（AC）-讲述攻击执行的难度以及成功进行攻击需要哪些因素。\n 用户交互（UI）-确定攻击是否需要用户参与。\n 所需的权限（PR）-记录成功进行攻击所需的用户身份验证级别。\n 范围（S）-确定攻击者是否可以影响具有不同权限级别的组件。\n 机密性（C）-衡量信息泄露给非授权方后导致的影响程度。\n 完整性（I）-衡量信息被篡改后导致的影响程度。\n 可用性（A）-衡量用户在需要访问数据或服务时受影响的程度。\n  # 评估原则  评估漏洞的严重等级，不是评估风险。\n 评估时必须基于攻击场景，且保证在该场景下，攻击者成功攻击后能对系统造成机密性、完整性、可用性影响。\n 当安全漏洞有多个攻击场景时，应以造成最大的影响，即 CVSS 评分最高的攻击场景为依据。\n 被嵌入调用的库存在漏洞，要根据该库在产品中的使用方式，确定漏洞的攻击场景后进行评估。\n 安全缺陷不能被触发或不影响 CIA(机密性/完整性/可用性)，CVSS 评分为0分。\n  # 评估步骤 对漏洞进行评估时，可根据下述步骤进行操作：\n 设定可能的攻击场景，基于攻击场景评分。\n 确定漏洞组件（Vulnerable Component）和受影响组件（Impact Component）。\n 选择基础评估指标的值：通过对可利用指标（攻击向量/攻击复杂度/所需权限/用户交互/范围）和受影响指标（机密性/完整性/可用性）给出漏洞影响评估。\n  # 严重等级划分   严重等级（Severity Rating） CVSS 评分（Score）   致命（Critical） 9.0 - 10.0   高（High） 7.0 - 8.9   中（Medium） 4.0 - 6.9   低（Low） 0.1 - 3.9   无（None） 0.0    # 漏洞披露  为了保护 CloudWeGo 用户的安全，在进行调查、修复和发布安全公告之前，CloudWeGo 社区不会公开披露、讨论或确认 CloudWeGo 产品的安全问题。安全漏洞修复后 CloudWeGo 社区会发布安全公告，安全公告内容包括该漏洞的技术细节、CVE编号、CVSS安全评分、严重性等级以及受到该漏洞影响的版本和修复版本等信息。   ","categories":"","description":"","excerpt":" SUPPORT 漏洞管理   # 漏洞响应   CloudWeGo 社区非常重视社区版本的安全性，CloudWeGo 安全委员会负责接收、 …","ref":"/zh/security/vulnerability-reporting/","tags":"","title":"漏洞管理"},{"body":"Hertz 遵从 语义化版本 2.0.0 发布版本。\n 主版本号：Hertz 提供的 API 出现不兼容的情况时，升级该版本号 次版本号：Hertz 提供新的功能特性同时保持向下兼容时，升级该版本号 修订号：Hertz 的代码提供小的特性或向下兼容的优化和问题修复时，升级该版本号  ","categories":"","description":"","excerpt":"Hertz 遵从 语义化版本 2.0.0 发布版本。\n 主版本号：Hertz 提供的 API 出现不兼容的情况时，升级该版本号 次版本 …","ref":"/zh/docs/hertz/reference/version/","tags":"","title":"版本说明"},{"body":"服务发现 Discover trait 提供了自定义服务发现的能力，其支持自定义静态或可订阅的服务发现能力。\nTrait 定义\n/// [`Instance`] contains information of an instance from the target service. #[derive(Debug, Clone, PartialEq, Eq)]pubstruct Instance{pubaddress: Address,pubweight: u32,pubtags: HashMap\u003cCow\u003c'static,str\u003e,Cow\u003c'static,str\u003e\u003e,}/// Change indicates the change of the service discover. /// /// Change contains the difference between the current discovery result and the previous one. /// It is designed for providing detail information when dispatching an event for service /// discovery result change. /// /// Since the loadbalancer may rely on caching the result of discover to improve performance, /// the discover implementation should dispatch an event when result changes. #[derive(Debug, Clone)]pubstruct Change\u003cK\u003e{/// `key` should be the same as the output of `WatchableDiscover::key`, /// which is often used by cache. pubkey: K,puball: Vec\u003cArc\u003cInstance\u003e\u003e,pubadded: Vec\u003cArc\u003cInstance\u003e\u003e,pubupdated: Vec\u003cArc\u003cInstance\u003e\u003e,pubremoved: Vec\u003cArc\u003cInstance\u003e\u003e,}/// [`Discover`] is the most basic trait for Discover. pubtraitDiscover: Send +Sync+'static{/// `Key` identifies a set of instances, such as the cluster name. type Key: Hash+PartialEq+Eq+Send+Sync+Clone+'static;/// `Error` is the discovery error. type Error: std::error::Error+Send+Sync;/// `DiscFut` is a Future object which returns a discovery result. type DiscFut\u003c'future\u003e: Future\u003cOutput=Result\u003cVec\u003cArc\u003cInstance\u003e\u003e,Self::Error\u003e\u003e+Send+'future;/// `discover` allows to request an endpoint and return a discover future. fn discover(\u0026self,endpoint: \u0026Endpoint)-\u003e Self::DiscFut\u003c'_\u003e;/// `key` should return a key suitable for cache. fn key(\u0026self,endpoint: \u0026Endpoint)-\u003e Self::Key;/// `watch` should return a [`async_broadcast::Receiver`] which can be used to subscribe /// [`Change`]. fn watch(\u0026self)-\u003e Option\u003cReceiver\u003cChange\u003cSelf::Key\u003e\u003e\u003e;}示例\npubstruct StaticDiscover{instances: Vec\u003cArc\u003cInstance\u003e\u003e,}implDiscoverforStaticDiscover{type Key=();type Error=Infallible;type DiscFut\u003c'a\u003e=implFuture\u003cOutput=Result\u003cVec\u003cArc\u003cInstance\u003e\u003e,Self::Error\u003e\u003e+'a;fn discover(\u0026self,_: \u0026Endpoint)-\u003e Self::DiscFut\u003c'_\u003e{async{Ok(self.instances.clone())}}fn key(\u0026self,_: \u0026Endpoint)-\u003e Self::Key{}fn watch(\u0026self)-\u003e Option\u003casync_broadcast::Receiver\u003cChange\u003cSelf::Key\u003e\u003e\u003e{None}}负载均衡 Volo 提供基于 LoadBalance trait 自定义负载均衡策略的能力：\n/// [`LoadBalance`] promise the feature of the load balance policy. pubtraitLoadBalance\u003cD\u003e: Send +Sync+'staticwhereD: Discover,{/// `InstanceIter` is an iterator of [`crate::discovery::Instance`]. type InstanceIter\u003c'iter\u003e: Iterator\u003cItem=Address\u003e+Send+'iter;/// `Error` is the error of the `get_picker` result. type Error: std::error::Error+Send+Sync;/// `GetFut` is the return type of `get_picker`. type GetFut\u003c'future,'iter\u003e: Future\u003cOutput=Result\u003cSelf::InstanceIter\u003c'iter\u003e,Self::Error\u003e\u003e+Send;// remove +'future temporarily, see https://github.com/rust-lang/rust/issues/100013 /// `get_picker` allows to get an instance iterator of a specified endpoint from self or /// service discovery. fn get_picker\u003c'future,'iter\u003e(\u0026'iterself,endpoint: \u0026'futureEndpoint,discover: \u0026'futureD,)-\u003e Self::GetFut\u003c'future,'iter\u003e;/// `reblance` is the callback method be used in service discovering subscription. fn rebalance(\u0026self,changes: Change\u003cD::Key\u003e);}示例\npubstruct InstancePicker{instances: Vec\u003cArc\u003cInstance\u003e\u003e,index: usize }implIteratorforInstancePicker{type Item=Address;fn next(\u0026mutself)-\u003e Option\u003cSelf::Item\u003e{leti=self.instances.get(self.index);self.index+=1;i.map(|i|i.clone().address.clone())}}#[derive(Clone)]pubstruct RoundRobin\u003cK\u003ewhereK: Hash+PartialEq+Eq+Send+Sync+'static,{router: DashMap\u003cK,Arc\u003cVec\u003cArc\u003cInstance\u003e\u003e\u003e\u003e,}impl\u003cD\u003eLoadBalance\u003cD\u003eforRoundRobin\u003cD::Key\u003ewhereD: Discover,{type InstanceIter\u003c'iter\u003e=InstancePicker;type Error=D::Error;type GetFut\u003c'future,'iter\u003e=implFuture\u003cOutput=Result\u003cSelf::InstanceIter\u003c'iter\u003e,Self::Error\u003e\u003e+Send;fn get_picker\u003c'future,'iter\u003e(\u0026'iterself,endpoint: \u0026'futureEndpoint,discover: \u0026'futureD,)-\u003e Self::GetFut\u003c'future,'iter\u003e{async{letkey=discover.key(endpoint);letlist=matchself.router.entry(key){Entry::Occupied(e)=\u003ee.get().clone(),Entry::Vacant(e)=\u003e{letinstances=Arc::new(discover.discover(endpoint).await?);e.insert(instances).value().clone()}};Ok(InstancePicker{instances: list.to_vec(),index: 0})}}fn rebalance(\u0026self,changes: Change\u003cD::Key\u003e){ifletEntry::Occupied(entry)=self.router.entry(changes.key.clone()){entry.replace_entry(Arc::new(changes.all));}}}","categories":"","description":"","excerpt":"服务发现 Discover trait 提供了自定义服务发现的能力，其支持自定义静态或可订阅的服务发现能力。\nTrait 定义\n/// …","ref":"/zh/docs/volo/guide/discovery_lb/","tags":"","title":"自定义服务发现与负载均衡"},{"body":"在微服务中，链路追踪是一项很重要的能力，在快速定位问题，分析业务瓶颈，还原一次请求的链路情况等方面发挥重要作用。Hertz 提供了链路追踪的能力，也支持用户自定义链路跟踪。\nHertz 将 trace 抽象为以下接口：\n// Tracer is executed at the start and finish of an HTTP. type Tracer interface { Start(ctx context.Context, c *app.RequestContext) context.Context Finish(ctx context.Context, c *app.RequestContext) } 使用 server.WithTracer() 配置添加 tracer，可以添加多个 tracer。\nHertz 会在请求开始之前(读包之前)执行所有 tracer 的 Start 方法，在请求结束之后(写回数据之后)执行所有 tracer 的Finish 方法。这种实现时需要注意：\n Start 方法执行时，刚开始接受包，这个时候 requestContext 是一个“空”的 requestContext，并不能拿到这次请求的相关信息。如果想在解包后中拿到一些信息(如在 header 中的 traceID 等)再进行操作时，可以使用中间件能力将 traceID 注入到 span 中。 在中间件内对 context 的修改是无效的。  在 requestContext 内存有 traceInfo，其有以下信息\ntype HTTPStats interface { Record(event stats.Event, status stats.Status, info string) // 记录事件  GetEvent(event stats.Event) Event // 获取事件  SendSize() int // 获取 SendSize  RecvSize() int // 获取 RecvSize  Error() error // 获取 Error  Panicked() (bool, interface{}) // 获取 Panic  Level() stats.Level // 获取当前 trace 等级  SetLevel(level stats.Level) // 设置 trace 等级，当事件等级高于 trace 等级时不上报  ... } 事件包括：\nHTTPStart = newEvent(httpStart, LevelBase) // 请求开始 HTTPFinish = newEvent(httpFinish, LevelBase) // 请求结束  ServerHandleStart = newEvent(serverHandleStart, LevelDetailed) // 业务 handler 开始 ServerHandleFinish = newEvent(serverHandleFinish, LevelDetailed) // 业务 handler 结束 ReadHeaderStart = newEvent(readHeaderStart, LevelDetailed) // 读取 header 开始 ReadHeaderFinish = newEvent(readHeaderFinish, LevelDetailed) // 读取 header 结束 ReadBodyStart = newEvent(readBodyStart, LevelDetailed) // 读取 body 开始 ReadBodyFinish = newEvent(readBodyFinish, LevelDetailed) // 读取 body 结束 WriteStart = newEvent(writeStart, LevelDetailed) // 写 response 开始 WriteFinish = newEvent(writeFinish, LevelDetailed) // 写 response 结束 在 Finish 时可以获取到上述信息。\n同时，如果不希望记录这些信息，可以不注册任何 tracer ，则框架停止对这些信息的记录。\nhertz-contrib 中提供了 opentracing 的扩展方式，也在 hertz-examples 提供了可以从 http 到 rpc 调用的 demo。 仓库： https://github.com/hertz-contrib/tracer\n","categories":"","description":"","excerpt":"在微服务中，链路追踪是一项很重要的能力，在快速定位问题，分析业务瓶颈，还原一次请求的链路情况等方面发挥重要作用。Hertz 提供了链路追踪的 …","ref":"/zh/docs/hertz/tutorials/service-governance/tracing/","tags":"","title":"链路追踪"},{"body":"The service discovery extensions currently supported in the open source version of Hertz are stored in the registry. You are welcomed to join us in contributing and maintaining for this project.\nUsage The implementation of the Nacos registry is used as an example for reference. You can adjust the relevant parameters by yourself in the production environment.\nService Registration  Use server.WithRegistry to set up registration extensions and registration information.  import ( //... \t\"github.com/cloudwego/hertz/pkg/app\" \"github.com/cloudwego/hertz/pkg/app/server\" \"github.com/cloudwego/hertz/pkg/app/server/registry\" \"github.com/cloudwego/hertz/pkg/common/utils\" //... ) func main(){ // ....  r := nacos_demo.NewNacosRegistry(cli) h := server.Default( server.WithHostPorts(addr), server.WithRegistry(r, \u0026registry.Info{ ServiceName: \"hertz.test.demo\", Addr: utils.NewNetAddr(\"tcp\", addr), Weight: 10, Tags: nil, })) h.GET(\"/ping\", func(c context.Context, ctx *app.RequestContext) { ctx.JSON(consts.StatusOK, utils.H{\"ping\": \"pong1\"}) }) h.Spin() } Service Discovery  Use the sd.Discovery built-in middleware to support incoming custom service discovery extensions as well as load balance extensions. When using service discovery, replace Host with the service name and use config.WithSD to confirm that this request uses service registration.  import ( \"github.com/cloudwego/hertz/pkg/app/client\" \"github.com/cloudwego/hertz/pkg/app/middlewares/client/sd\" \"github.com/cloudwego/hertz/pkg/common/config\" \"github.com/cloudwego/hertz/pkg/common/hlog\" ) func main(){ cli, err := client.NewClient() if err != nil { panic(err) } r := nacos_demo.NewNacosResolver() cli.Use(sd.Discovery(r)) for i := 0; i \u003c 10; i++ { status, body, err := cli.Get(context.Background(), nil, \"http://hertz.test.demo/ping\", config.WithSD(true)) if err != nil { hlog.Fatal(err) } hlog.Infof(\"code=%d,body=%s\", status, string(body)) } } ","categories":"","description":"","excerpt":"The service discovery extensions currently supported in the open …","ref":"/docs/hertz/tutorials/service-governance/service_discovery/","tags":"","title":"Service Registration and Service Discovery"},{"body":"Service Registration Extension Hertz supports custom registration extensions, you can extend it to integrate other registries, which are defined under pkg/app/server/registry.\nInterface and Info Definition  Interface Definition  // Registry is extension interface of service registry. type Registry interface { Register(info *Info) error Deregister(info *Info) error }  Info Definition  // Info is used for registry. // The fields are just suggested, which is used depends on design. type Info struct { ServiceName string Addr net.Addr Weight int // extend other infos with Tags. \tTags map[string]string } Work with Hertz  Specify your own registration extensions and custom registration information via server.WithRegistry.  h := server.Default( server.WithHostPorts(addr), server.WithRegistry(r, \u0026registry.Info{ ServiceName: \"hertz.test.demo\", Addr: utils.NewNetAddr(\"tcp\", addr), Weight: 10, Tags: nil, })) Service Discovery Extension Interface Definition Hertz supports custom discovery extensions, you can extend it to integrate other registries, which are defined under pkg/app/server/discovery.\ntype Resolver interface { // Target should return a description for the given target that is suitable for being a key for cache. \tTarget(ctx context.Context, target *TargetInfo) string // Resolve returns a list of instances for the given description of a target. \tResolve(ctx context.Context, desc string) (Result, error) // Name returns the name of the resolver. \tName() string } type TargetInfo struct { Host string Tags map[string]string } type Result struct { CacheKey string // Unique key for cache  Instances []Instance // Service discovery result } Resolver is defined as follows:\n Resolve: Core function of Resolver, get the service discovery Result we need from the target key. Target: The unique target to be used by Resolve is resolved from the peer TargetInfo provided by Hertz, and this target is used as the unique key for the cache. Name: This is used to specify a unique name for the Resolver, and is used by Hertz to cache and reuse the Resolver.  Work with Hertz Specify custom service discovery extensions by using the Discovery middleware provided by Hertz.\ncli, err := client.NewClient() if err != nil { panic(err) } r := nacos_demo.NewNacosResolver() cli.Use(sd.Discovery(r)) Note  We improve performance by reusing Resolver in a way that requires the Resolver method implementation to be concurrency-safe.  Load Balancing Extension Hertz provides a WeightedRandom load balancing implementation by default, and also supports a custom load balancing implementation defined under pkg/app/client/loadbalance.\nInterface Definition // Loadbalancer picks instance for the given service discovery result.  type Loadbalancer interface { // Pick is used to select an instance according to discovery result  Pick(discovery.Result) discovery.Instance // Rebalance is used to refresh the cache of load balance's information  Rebalance(discovery.Result) // Delete is used to delete the cache of load balance's information when it is expired  Delete(string) // Name returns the name of the Loadbalancer.  Name() string } Work with Hertz By using the Discovery middleware provided by Hertz, custom service discovery extensions can be specified along with custom load balancing extensions using sd.WithLoadBalanceOptions.\ncli, err := client.NewClient() if err != nil { panic(err) } r := nacos_demo.NewNacosResolver() cli.Use(sd.Discovery(r),sd.WithLoadBalanceOptions(***,***)) ","categories":"","description":"","excerpt":"Service Registration Extension Hertz supports custom registration …","ref":"/docs/hertz/tutorials/framework-exten/service_discovery/","tags":"","title":"Service Registration and Service Discovery Extensions"},{"body":"目前在 Hertz 的开源版本支持的服务发现扩展都存放在registry 中，欢迎大家参与项目贡献与维护。\n使用方式 下面以 Nacos 注册中心为例，仅供参考，生产环境下可自行调整相关参数。\n服务注册  使用 server.WithRegistry 设置注册扩展以及注册信息。  import ( //... \t\"github.com/cloudwego/hertz/pkg/app\" \"github.com/cloudwego/hertz/pkg/app/server\" \"github.com/cloudwego/hertz/pkg/app/server/registry\" \"github.com/cloudwego/hertz/pkg/common/utils\" //... ) func main(){ // ....  r := nacos_demo.NewNacosRegistry(cli) h := server.Default( server.WithHostPorts(addr), server.WithRegistry(r, \u0026registry.Info{ ServiceName: \"hertz.test.demo\", Addr: utils.NewNetAddr(\"tcp\", addr), Weight: 10, Tags: nil, })) h.GET(\"/ping\", func(c context.Context, ctx *app.RequestContext) { ctx.JSON(consts.StatusOK, utils.H{\"ping\": \"pong1\"}) }) h.Spin() } 服务发现  使用内置的 sd.Discovery 中间件，支持传入自定义的服务发现扩展以及负载均衡扩展。 使用服务发现时需要将 Host 替换为服务名，并使用 config.WithSD 确定本次请求使用服务注册。  import ( \"github.com/cloudwego/hertz/pkg/app/client\" \"github.com/cloudwego/hertz/pkg/app/middlewares/client/sd\" \"github.com/cloudwego/hertz/pkg/common/config\" \"github.com/cloudwego/hertz/pkg/common/hlog\" ) func main(){ cli, err := client.NewClient() if err != nil { panic(err) } r := nacos_demo.NewNacosResolver() cli.Use(sd.Discovery(r)) for i := 0; i \u003c 10; i++ { status, body, err := cli.Get(context.Background(), nil, \"http://hertz.test.demo/ping\", config.WithSD(true)) if err != nil { hlog.Fatal(err) } hlog.Infof(\"code=%d,body=%s\", status, string(body)) } } ","categories":"","description":"","excerpt":"目前在 Hertz 的开源版本支持的服务发现扩展都存放在registry 中，欢迎大家参与项目贡献与维护。\n使用方式 下面以 Nacos 注 …","ref":"/zh/docs/hertz/tutorials/service-governance/service_discovery/","tags":"","title":"服务注册与发现"},{"body":"服务注册扩展 Hertz 支持自定义注册模块，使用者可自行扩展集成其他注册中心，该扩展定义在 pkg/app/server/registry 下。\n接口定义与Info定义  接口定义  // Registry is extension interface of service registry. type Registry interface { Register(info *Info) error Deregister(info *Info) error }  Info定义  // Info is used for registry. // The fields are just suggested, which is used depends on design. type Info struct { ServiceName string Addr net.Addr Weight int // extend other infos with Tags. \tTags map[string]string } 集成到Hertz  通过 server.WithRegistry 指定自己的注册模块和自定义的注册信息。  h := server.Default( server.WithHostPorts(addr), server.WithRegistry(r, \u0026registry.Info{ ServiceName: \"hertz.test.demo\", Addr: utils.NewNetAddr(\"tcp\", addr), Weight: 10, Tags: nil, })) 服务发现扩展 接口定义 Hertz 支持自定义发现模块，使用者可自行扩展集成其他注册中心，该扩展定义在 pkg/app/client/discovery 下。\ntype Resolver interface { // Target should return a description for the given target that is suitable for being a key for cache. \tTarget(ctx context.Context, target *TargetInfo) string // Resolve returns a list of instances for the given description of a target. \tResolve(ctx context.Context, desc string) (Result, error) // Name returns the name of the resolver. \tName() string } type TargetInfo struct { Host string Tags map[string]string } type Result struct { CacheKey string // 缓存的唯一 key  Instances []Instance // 服务发现结果 } Resolver 接口定义如下:\n Resolve：作为 Resolver 的核心方法， 从 target key 中获取我们需要的服务发现结果 Result。 Target：从 Hertz 提供的对端 TargetInfo 中解析出 Resolve 需要使用的唯一 target, 同时这个 target 将作为缓存的唯一 key。 Name：用于指定 Resolver 的唯一名称， 同时 Hertz 会用它来缓存和复用 Resolver。  集成到Hertz 通过使用 Hertz 提供的 Discovery 中间件，指定自定义的服务发现扩展。\ncli, err := client.NewClient() if err != nil { panic(err) } r := nacos_demo.NewNacosResolver() cli.Use(sd.Discovery(r)) 注意事项  我们通过复用 Resolver 的方式来提高性能， 要求 Resolver 的方法实现需要是并发安全的。  负载均衡扩展 Hertz 默认提供了 WeightedRandom 负载均衡实现,同时也支持自定义负载均衡实现，该扩展定义在 pkg/app/client/loadbalance 下\n接口定义 // Loadbalancer picks instance for the given service discovery result.  type Loadbalancer interface { // Pick is used to select an instance according to discovery result  Pick(discovery.Result) discovery.Instance // Rebalance is used to refresh the cache of load balance's information  Rebalance(discovery.Result) // Delete is used to delete the cache of load balance's information when it is expired  Delete(string) // Name returns the name of the Loadbalancer.  Name() string } 集成到Hertz 通过使用 Hertz 提供的 Discovery 中间件，指定自定义的服务发现扩展的同时也可以使用 sd.WithLoadBalanceOptions 指定自定义负载均衡扩展。\ncli, err := client.NewClient() if err != nil { panic(err) } r := nacos_demo.NewNacosResolver() cli.Use(sd.Discovery(r),sd.WithLoadBalanceOptions(***,***)) ","categories":"","description":"","excerpt":"服务注册扩展 Hertz 支持自定义注册模块，使用者可自行扩展集成其他注册中心，该扩展定义在 pkg/app/server/registry …","ref":"/zh/docs/hertz/tutorials/framework-exten/service_discovery/","tags":"","title":"服务注册与发现扩展"},{"body":"An RPC protocol generally includes a transport protocol in the application layer and a message protocol that tells how to access the payload. Transport protocols come with rich mechanisms that let you deal with additional metadata, which can be helpful for service governance. And Kitex allows you to read or write metadata through a protocol based on MetaHandler. The capability of carrying metadata enables us to track requests in their entirety as it travels across services of a distributed system, and thus makes transport protocol indispensable in Microservices.\nKitex already supports TTHeader and HTTP2. Available options for transport protocol are TTHeader、GRPC、Framed、TTHeaderFramed、PurePayload.\nSome clarifications:\n Kitex supports Protobuf in two ways: Kitex Protobuf and gRPC. We include gRPC as a transport protocol to make it easy to distinguish. Internally, Kitex will identify the protocol based on whether gRPC was configured. Framed is not technically a transport protocol. It was just there for marking the extra 4 bytes header in Payload Size. But the message protocol does not enforce the need for Framed Header. For instance, PurePayload doesn’t have any Header. Therefore, we also include Framed as an option for the transport protocol. Framed and TTHeader could be used together, which leads to TTHeaderFramed.  Here are the available combination options of transport protocols and message protocols in Kitex:\n Thrift: TTHeader(recommend), Framed, TTHeaderFramed KitexProtobuf: TTHeader(recommend), Framed, TTHeaderFramed gRPC: HTTP2  If you want to use custom implementations for the message or transport protocol, you can find help here Extension of Codec.\nConfiguration You can configure the transport protocol when initializing the client:\n// client option client.WithTransportProtocol(transport.XXX) Kitex Server supports protocol detection for all supported protocols and doesn’t require explicit configuration.\nUsage Thrift + TTHeader // client side var opts []client.Option opts = append(opts, client.WithTransportProtocol(transport.TTHeader)) // use TTHeader meta handler. \u003e= v0.3.4 ClientTTHeaderHandler is added by default, don't need to do setup opts = append(opts, client.WithMetaHandler(transmeta.ClientTTHeaderHandler)) cli, err := xxxservice.NewClient(targetService, opts...) // server side no need to config transport protocol var opts []server.Option // use TTHeader meta handler. \u003e= v0.3.4 ServerTTHeaderHandler is added by default, don't need to do setup opts = append(opts, server.WithMetaHandler(transmeta.ServerTTHeaderHandler)) cli, err := xxxservice.NewServer(handler, opts...) gRPC // client side var opts []client.Option opts = append(opts, client.WithTransportProtocol(transport.GRPC)) // use HTTP2 meta handler. \u003e= v0.3.4 ClientHTTP2Handler is added by default, don't need to do setup opts = append(opts, client.WithMetaHandler(transmeta.ClientHTTP2Handler)) cli, err := xxxservice.NewClient(targetService, opts...) // server side no need to config transport protocol var opts []server.Option // use HTTP2 meta handler. \u003e= v0.3.4 ServerHTTP2Handler is added by default, don't need to do setup opts = append(opts, server.WithMetaHandler(transmeta.ServerHTTP2Handler)) cli, err := xxxservice.NewServer(handler, opts...) ","categories":"","description":"","excerpt":"An RPC protocol generally includes a transport protocol in the …","ref":"/docs/kitex/tutorials/basic-feature/transport_protocol/","tags":"","title":"Transport Protocol"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/hertz/tutorials/framework-exten/advanced-exten/","tags":"","title":"Advanced Extensions"},{"body":"Usage When a client makes an RPC call, it adds an additional Option that takes precedence over client Option and overrides some configurations:\nresp, err := client.Call(ctx, req, callopt.WithXXX....) Options WithHostPort func WithHostPort(hostport string) Option Specifying a specific HostPort directly during this call phase will overwrite the resolver result for direct access. More\nWithURL func WithURL(url string) Option Specifying a specified URL during this call phase to initiate the call. More\nWithTag func WithTag(key, val string) Option Set some meta information for this RPC call, add it in the form of key-value, for example, if you want to add fields such as cluster and idc to the meta information for service governance, you can write it like this:\nresp, err := client.Call(ctx, req,callopt.WithTag(\"cluster\", cluster),callopt.WithTag(\"idc\", idc)) WithRPCTimeout func WithRPCTimeout(d time.Duration) Option Set RPC timeout. More\nWithConnectTimeout func WithConnectTimeout(d time.Duration) Option Set connection timeout. More\nWithHTTPHost func WithHTTPHost(host string) Option When using HTTP connection, the Option specifies the Host address in the HTTP header.\n","categories":"","description":"","excerpt":"Usage When a client makes an RPC call, it adds an additional Option …","ref":"/docs/kitex/tutorials/options/call_options/","tags":"","title":"Call Option"},{"body":"用法 客户端发起 RPC 调用时，额外添加一些 Option，优先级比 Client Option 高，会覆盖某些配置：\nresp, err := client.Call(ctx, req, callopt.WithXXX....) Option 说明 IP 端口 - WithHostPort func WithHostPort(hostport string) Option 在本次调用阶段直接指定一个具体的 HostPort ，会覆盖掉 Resolver 的结果进行直接访问，详见直连访问-指定 IP 和 Port 进行调用。\n指定 URL - WithURL func WithURL(url string) Option 在本次调用阶段重新指定一个指定 URL 发起调用，详见直连访问-指定 URL 进行调用。\n添加标签 - WithTag func WithTag(key, val string) Option 为本次 RPC 调用设置一些 Tag 元信息，以 key-value 的形式添加，例如希望在元信息中加入 cluster、idc 等字段用来做服务治理，可以像下面这样写：\nresp, err := client.Call(ctx, req,callopt.WithTag(\"cluster\", cluster),callopt.WithTag(\"idc\", idc)) 超时设置 - WithRPCTimeout func WithRPCTimeout(d time.Duration) Option 指定本次 RPC 调用的超时时间，详见超时控制。\n超时设置 - WithConnectTimeout func WithConnectTimeout(d time.Duration) Option 为本次 RPC 调用设置连接超时时间，详见超时控制。\nHTTP Host 设置 - WithHTTPHost func WithHTTPHost(host string) Option 当使用 HTTP 连接的场景时，该 Option 会在 HTTP Header 中指定 Host 地址。\n","categories":"","description":"","excerpt":"用法 客户端发起 RPC 调用时，额外添加一些 Option，优先级比 Client Option 高，会覆盖某些配置：\nresp, err …","ref":"/zh/docs/kitex/tutorials/options/call_options/","tags":"","title":"Call Option"},{"body":"通常 RPC 协议中包含 RPC 消息协议和应用层传输协议，RPC 消息协议看做是传输消息的 Payload，传输协议额外传递一些元信息通常会用于服务治理，框架的 MetaHandler 也是和传输协议搭配使用。在微服务场景下，传输协议起到了重要的作用，如链路跟踪的透传信息通常由传输协议进行链路传递。\nKitex 目前支持两种传输协议：TTHeader、HTTP2，但实际提供配置的 Transport Protocol 是：TTHeader、GRPC、Framed、TTHeaderFramed、PurePayload。\n这里做一些说明：\n 因为 Kitex 对 Protobuf 的支持有 Kitex Protobuf 和 gRPC，为方便区分将 gRPC 作为传输协议的分类，框架会根据是否有配置 gRPC 决定使用哪个协议； Framed 严格意义上并不是传输协议，只是标记 Payload Size 额外增加的 4 字节头，但消息协议对是否有 Framed 头并不是强制的，PurePayload 即没有任何头部的，所以将 Framed 也作为传输协议的分类； Framed 和 TTHeader 也可以结合使用，所以有 TTHeaderFramed 。  消息协议可选的传输协议组合如下：\n Thrift: TTHeader(建议)、Framed、TTHeaderFramed KitexProtobuf: TTHeader(建议)、Framed、TTHeaderFramed gRPC: HTTP2  如果想自定义消息协议和传输协议参考：编解码(协议)扩展\n配置项 Client 初始化时通过 WithTransportProtocol 配置传输协议：\n// client option client.WithTransportProtocol(transport.XXX) Server 支持协议探测（在 Kitex 默认支持的协议内），无需配置传输协议。\n使用示例 Thrift + TTHeader // client side var opts []client.Option opts = append(opts, client.WithTransportProtocol(transport.TTHeader)) // use TTHeader meta handler. \u003e= v0.3.4 ClientTTHeaderHandler is added by default, don't need to do setup opts = append(opts, client.WithMetaHandler(transmeta.ClientTTHeaderHandler)) cli, err := xxxservice.NewClient(targetService, opts...) // server side no need to config transport protocol var opts []server.Option // use TTHeader meta handler. \u003e= v0.3.4 ServerTTHeaderHandler is added by default, don't need to do setup opts = append(opts, server.WithMetaHandler(transmeta.ServerTTHeaderHandler)) cli, err := xxxservice.NewServer(handler, opts...) gRPC // client side var opts []client.Option opts = append(opts, client.WithTransportProtocol(transport.GRPC)) // use HTTP2 meta handler. \u003e= v0.3.4 ClientHTTP2Handler is added by default, don't need to do setup opts = append(opts, client.WithMetaHandler(transmeta.ClientHTTP2Handler)) cli, err := xxxservice.NewClient(targetService, opts...) // server side no need to config transport protocol var opts []server.Option // use HTTP2 meta handler. \u003e= v0.3.4 ServerHTTP2Handler is added by default, don't need to do setup opts = append(opts, server.WithMetaHandler(transmeta.ServerHTTP2Handler)) cli, err := xxxservice.NewServer(handler, opts...) ","categories":"","description":"","excerpt":"通常 RPC 协议中包含 RPC 消息协议和应用层传输协议，RPC 消息协议看做是传输消息的 Payload，传输协议额外传递一些元信息通常 …","ref":"/zh/docs/kitex/tutorials/basic-feature/transport_protocol/","tags":"","title":"传输协议"},{"body":"JSON Web Token (JWT) is a lightweight authentication specification that allows us to use JWT to deliver secure and reliable information between users and servers. Essentially a token, it is a compact security method for passing between two sides of network communication. Hertz also provides an implementation of JWT, it uses gin implementation for reference.\nAs for usage, you may refer to hertz example\npackage main import ( \"context\" \"log\" \"time\" \"github.com/cloudwego/hertz/pkg/app\" \"github.com/cloudwego/hertz/pkg/app/server\" \"github.com/hertz-contrib/jwt\" ) type login struct { Username string `form:\"username,required\" json:\"username,required\"` //lint:ignore SA5008 ignoreCheck \tPassword string `form:\"password,required\" json:\"password,required\"` //lint:ignore SA5008 ignoreCheck } var identityKey = \"id\" func PingHandler(c context.Context, ctx *app.RequestContext) { ctx.JSON(200, map[string]string{ \"ping\": \"pong\", }) } // User demo type User struct { UserName string FirstName string LastName string } func main() { h := server.Default() // the jwt middleware \tauthMiddleware, err := jwt.New(\u0026jwt.HertzJWTMiddleware{ Realm: \"test zone\", Key: []byte(\"secret key\"), Timeout: time.Hour, MaxRefresh: time.Hour, IdentityKey: identityKey, PayloadFunc: func(data interface{}) jwt.MapClaims { if v, ok := data.(*User); ok { return jwt.MapClaims{ identityKey: v.UserName, } } return jwt.MapClaims{} }, IdentityHandler: func(ctx context.Context, c *app.RequestContext) interface{} { claims := jwt.ExtractClaims(ctx, c) return \u0026User{ UserName: claims[identityKey].(string), } }, Authenticator: func(ctx context.Context, c *app.RequestContext) (interface{}, error) { var loginVals login if err := c.BindAndValidate(\u0026loginVals); err != nil { return \"\", jwt.ErrMissingLoginValues } userID := loginVals.Username password := loginVals.Password if (userID == \"admin\" \u0026\u0026 password == \"admin\") || (userID == \"test\" \u0026\u0026 password == \"test\") { return \u0026User{ UserName: userID, LastName: \"Hertz\", FirstName: \"CloudWeGo\", }, nil } return nil, jwt.ErrFailedAuthentication }, Authorizator: func(data interface{}, ctx context.Context, c *app.RequestContext) bool { if v, ok := data.(*User); ok \u0026\u0026 v.UserName == \"admin\" { return true } return false }, Unauthorized: func(ctx context.Context, c *app.RequestContext, code int, message string) { c.JSON(code, map[string]interface{}{ \"code\": code, \"message\": message, }) }, // TokenLookup is a string in the form of \"\u003csource\u003e:\u003cname\u003e\" that is used \t// to extract token from the request. \t// Optional. Default value \"header:Authorization\". \t// Possible values: \t// - \"header:\u003cname\u003e\" \t// - \"query:\u003cname\u003e\" \t// - \"cookie:\u003cname\u003e\" \t// - \"param:\u003cname\u003e\" \tTokenLookup: \"header: Authorization, query: token, cookie: jwt\", // TokenLookup: \"query:token\", \t// TokenLookup: \"cookie:token\",  // TokenHeadName is a string in the header. Default value is \"Bearer\" \tTokenHeadName: \"Bearer\", // TimeFunc provides the current time. You can override it to use another time value. This is useful for testing or if your server uses a different time zone than your tokens. \tTimeFunc: time.Now, }) if err != nil { log.Fatal(\"JWT Error:\" + err.Error()) } // When you use jwt.New(), the function is already automatically called for checking, \t// which means you don't need to call it again. \terrInit := authMiddleware.MiddlewareInit() if errInit != nil { log.Fatal(\"authMiddleware.MiddlewareInit() Error:\" + errInit.Error()) } h.POST(\"/login\", authMiddleware.LoginHandler) h.NoRoute(authMiddleware.MiddlewareFunc(), func(ctx context.Context, c *app.RequestContext) { claims := jwt.ExtractClaims(ctx, c) log.Printf(\"NoRoute claims: %#v\\n\", claims) c.JSON(404, map[string]string{\"code\": \"PAGE_NOT_FOUND\", \"message\": \"Page not found\"}) }) auth := h.Group(\"/auth\") // Refresh time can be longer than token timeout \tauth.GET(\"/refresh_token\", authMiddleware.RefreshHandler) auth.Use(authMiddleware.MiddlewareFunc()) { auth.GET(\"/ping\", PingHandler) } h.Spin() } ","categories":"","description":"","excerpt":"JSON Web Token (JWT) is a lightweight authentication specification …","ref":"/docs/hertz/tutorials/basic-feature/middleware/jwt/","tags":"","title":"JWT"},{"body":"JSON Web Token（JWT）是一个轻量级的认证规范，这个规范允许我们使用 JWT 在用户和服务器之间传递安全可靠的信息。其本质是一个 token ，是一种紧凑的 URL 安全方法，用于在网络通信的双方之间传递。 Hertz 也提供了 jwt 的实现 ，参考了 gin 的实现 。\n使用方法可参考如下 example\npackage main import ( \"context\" \"log\" \"time\" \"github.com/cloudwego/hertz/pkg/app\" \"github.com/cloudwego/hertz/pkg/app/server\" \"github.com/hertz-contrib/jwt\" ) type login struct { Username string `form:\"username,required\" json:\"username,required\"` //lint:ignore SA5008 ignoreCheck \tPassword string `form:\"password,required\" json:\"password,required\"` //lint:ignore SA5008 ignoreCheck } var identityKey = \"id\" func PingHandler(c context.Context, ctx *app.RequestContext) { ctx.JSON(200, map[string]string{ \"ping\": \"pong\", }) } // User demo type User struct { UserName string FirstName string LastName string } func main() { h := server.Default() // the jwt middleware \tauthMiddleware, err := jwt.New(\u0026jwt.HertzJWTMiddleware{ Realm: \"test zone\", Key: []byte(\"secret key\"), Timeout: time.Hour, MaxRefresh: time.Hour, IdentityKey: identityKey, PayloadFunc: func(data interface{}) jwt.MapClaims { if v, ok := data.(*User); ok { return jwt.MapClaims{ identityKey: v.UserName, } } return jwt.MapClaims{} }, IdentityHandler: func(ctx context.Context, c *app.RequestContext) interface{} { claims := jwt.ExtractClaims(ctx, c) return \u0026User{ UserName: claims[identityKey].(string), } }, Authenticator: func(ctx context.Context, c *app.RequestContext) (interface{}, error) { var loginVals login if err := c.BindAndValidate(\u0026loginVals); err != nil { return \"\", jwt.ErrMissingLoginValues } userID := loginVals.Username password := loginVals.Password if (userID == \"admin\" \u0026\u0026 password == \"admin\") || (userID == \"test\" \u0026\u0026 password == \"test\") { return \u0026User{ UserName: userID, LastName: \"Hertz\", FirstName: \"CloudWeGo\", }, nil } return nil, jwt.ErrFailedAuthentication }, Authorizator: func(data interface{}, ctx context.Context, c *app.RequestContext) bool { if v, ok := data.(*User); ok \u0026\u0026 v.UserName == \"admin\" { return true } return false }, Unauthorized: func(ctx context.Context, c *app.RequestContext, code int, message string) { c.JSON(code, map[string]interface{}{ \"code\": code, \"message\": message, }) }, // TokenLookup is a string in the form of \"\u003csource\u003e:\u003cname\u003e\" that is used \t// to extract token from the request. \t// Optional. Default value \"header:Authorization\". \t// Possible values: \t// - \"header:\u003cname\u003e\" \t// - \"query:\u003cname\u003e\" \t// - \"cookie:\u003cname\u003e\" \t// - \"param:\u003cname\u003e\" \tTokenLookup: \"header: Authorization, query: token, cookie: jwt\", // TokenLookup: \"query:token\", \t// TokenLookup: \"cookie:token\",  // TokenHeadName is a string in the header. Default value is \"Bearer\" \tTokenHeadName: \"Bearer\", // TimeFunc provides the current time. You can override it to use another time value. This is useful for testing or if your server uses a different time zone than your tokens. \tTimeFunc: time.Now, }) if err != nil { log.Fatal(\"JWT Error:\" + err.Error()) } // When you use jwt.New(), the function is already automatically called for checking, \t// which means you don't need to call it again. \terrInit := authMiddleware.MiddlewareInit() if errInit != nil { log.Fatal(\"authMiddleware.MiddlewareInit() Error:\" + errInit.Error()) } h.POST(\"/login\", authMiddleware.LoginHandler) h.NoRoute(authMiddleware.MiddlewareFunc(), func(ctx context.Context, c *app.RequestContext) { claims := jwt.ExtractClaims(ctx, c) log.Printf(\"NoRoute claims: %#v\\n\", claims) c.JSON(404, map[string]string{\"code\": \"PAGE_NOT_FOUND\", \"message\": \"Page not found\"}) }) auth := h.Group(\"/auth\") // Refresh time can be longer than token timeout \tauth.GET(\"/refresh_token\", authMiddleware.RefreshHandler) auth.Use(authMiddleware.MiddlewareFunc()) { auth.GET(\"/ping\", PingHandler) } h.Spin() } ","categories":"","description":"","excerpt":"JSON Web Token（JWT）是一个轻量级的认证规范，这个规范允许我们使用 JWT 在用户和服务器之间传递安全可靠的信息。其本质是一 …","ref":"/zh/docs/hertz/tutorials/basic-feature/middleware/jwt/","tags":"","title":"JWT认证"},{"body":"Overview Validator is a thrift plugin that supports struct validation.\nConstraints are described by annotations in IDL, and the plugin will generate the IsValid() error method according to the structure given by the annotation, which is generated in the xxx-validator.go file.\nAnnotation is described in the form vt.{ContType} = \"Value\".\nScope: Every field in struct/union.\nValidation form: User initiative (provided, consistent for all parameters/results).\nIDL example:\nenum MapKey { A, B, C, D, E, F } struct Request { 1: required string Message (vt.max_size = \"30\", vt.prefix = \"Debug\") 2: i32 ID (vt.gt = \"1000\", vt.lt = \"10000\") 3: list\u003cdouble\u003e Values (vt.elem.gt = \"0.25\") 4: map\u003cMapKey, binary\u003e KeyValues (vt.key.defined_only = \"true\", vt.min_size = \"1\") } struct Response { 1: required string Message (vt.in = \"Debug\", vt.in = \"Info\", vt.in = \"Warn\", vt.in = \"Error\") } Usage Take the project “Kitex Hello” in Getting Started as an example, add annotations inhello.thrift. For example, the code bellow requires the length and prefix for Request.message:\nstruct Request { 1: string message (vt.max_size = \"8\", vt.prefix = \"kitex-\") } When generating kitex code, add --thrift-plugin validator to generate the validator code.\nkitex --thrift-plugin validator -service a.b.c hello.thrift The code will be generated in the directory bellow:\n├── kitex_gen └── api ├── hello │ ├── client.go │ ├── hello.go │ ├── invoker.go │ └── server.go ├── hello.go --\u003e ├── hello_validator.go ├── k-consts.go └── k-hello.go For struct Request, IsValid() is like this：\nfunc (p *Request) IsValid() error { if len(p.Message) \u003e int(8) { return fmt.Errorf(\"field Message max_len rule failed, current value: %d\", len(p.Message)) } _src := \"kitex-\" if !strings.HasPrefix(p.Message, _src) { return fmt.Errorf(\"field Message prefix rule failed, current value: %v\", p.Message) } return nil } So just simply call IsValid() to verify the struct：\n req := \u0026api.Request{ //.... } err := req.IsValid() if err != nil { //invalid .... } //valid ... Supported Validators The verification order is subject to the definition order,‘in’ and ‘not_in’ can be defined multiple times, whichever occurs first.\nnumber Including i8, i16, i32, i64, double.\n const, must be the specified value. lt, le, gt, ge, represents less than, less than or equal to, greater than, greater than or equal to. in, not_in, represents values that can be used and values that cannot be used, and can be specified multiple times, one value at a time. not_nil, the field cannot be empty (only valid if the field is optional).  structNumericDemo{1:doubleValue (vt.gt=\"1000.1\",vt.lt=\"10000.1\")2:i8Type (vt.in=\"1\",vt.in=\"2\",vt.in=\"4\")3:i64MagicNumber (vt.const=\"0x5f3759df\")}string/binary  const, must be the specified value. min_size, max_size. pattern, is used for regular matching. prefix, suffix, contains, not_contains. in, not_in, respectively indicate that the numerical value used and the numerical value cannot be specified at the same time. It can be specified for one use, and one can be specified. not_nil, the field cannot be empty (only valid if the field is optional).  structStringDemo{1:stringUninitialized (vt.const=\"烫烫烫\")2:stringName (vt.min_size=\"6\",vt.max_size=\"12\")3:stringSomeStuffs (vt.pattern=\"[0-9A-Za-z]+\")4:stringDebugInfo (vt.prefix=\"[Debug]\")5:stringPanicInfo (vt.contains=\"panic\")6:stringEditor (vt.in=\"vscode\",vt.in=\"vim\",vt.in=\"goland\")}bool  const, must be the specified value. not_nil, the field cannot be empty (only valid if the field is optional).  structBoolDemo{1:boolAMD (vt.const=\"true\")2:optionalboolNvidia (vt.not_nil=\"false\")}enum  const, must be the specified value. defined_only, must be in the value defined in the enum. not_nil, the field cannot be empty (only valid if the field is optional).  enumType{Number,String,List,Map}structEnumDemo{1:TypeAddressType (vt.const=\"String\")2:TypeValueType (vt.defined_only=\"true\")3:optionalTypeOptType (vt.not_nil=\"true\")}set/list  min_size, max_size. elem, element constraints.  structSetListDemo{1:list\u003cstring\u003ePersons (vt.min_size=\"5\",vt.max_size=\"10\")2:set\u003cdouble\u003eHealthPoints (vt.elem.gt=\"0\")}map  min_size, max_size. no_sparse, means it can’t be nil when value is a pointer. key, value.  structMapDemo{1:map\u003ci32,string\u003eIdName (vt.min_size=\"5\",vt.max_size=\"10\")2:map\u003ci32,DemoTestRequest\u003eRequests (vt.no_sparse=\"true\")3:map\u003ci32,double\u003eSome,(vt.key.gt=\"0\",vt.value.lt=\"1000\")}struct/union/exception  skip, means skipping recursive checks for struct/union/exception. (Defaults to false when used as a separate field, true by default when used as an element). not_nil, the field cannot be empty (only valid if the field is optional).  structOuterRequest{1:SomeStructStruct (vt.skip=\"true\")2:SomeUnionUnion (vt.skip=\"true\")3:SomeStructNotNilStruct (vt.not_nil=\"true\")}variable reference The prefix ‘$’ represents a reference to a variable, which can be used for cross-field verification:\n $x represents a variable named x, the variable name is [a-zA-Z0-9_], and its scope rule is **current structure**. $ represents the current field where the validator is located.  structExample{1:stringA (vt.max_size=\"$C\")2:stringB (vt.not_in=\"$A\")3:i32C}utility function The prefix ‘@’ indicates the built-in tool function to calculate the check value. Currently supported tool functions:\n sprintf(fmt, $1, $2…), used to output specific characters. len($x), output variable size (string length, number of list elements).  structExample{1:stringA2:list\u003cstring\u003eB (vt.max_size=\"@len($D)\")3:map\u003cstring,int)C4:stringD (vt.const=\"@sprintf(\\\"%s_%s\\\", $A, \\\"mysuffix\\\")\")}","categories":"","description":"","excerpt":"Overview Validator is a thrift plugin that supports struct validation. …","ref":"/docs/kitex/tutorials/code-gen/validator/","tags":"","title":"Thrift Validator"},{"body":"概述 Validator 是用于支持结构体校验能力的 thriftgo 插件。\n在 IDL 中通过注解来描述约束，插件会根据注解给对应的 struct 生成 IsValid() error  方法，生成在 xxx-validator.go 文件。\n注解采用 vt.{ConstraintType} = \"Value\" 这种形式描述。\n适用范围：struct/union 中的每个 field 。\n校验形式：用户主动校验。（可提供中间件，统一对所有参数/结果校验）\nIDL 示例：\nenum MapKey { A, B, C, D, E, F } struct Request { 1: required string Message (vt.max_size = \"30\", vt.prefix = \"Debug\") 2: i32 ID (vt.gt = \"1000\", vt.lt = \"10000\") 3: list\u003cdouble\u003e Values (vt.elem.gt = \"0.25\") 4: map\u003cMapKey, binary\u003e KeyValues (vt.key.defined_only = \"true\", vt.min_size = \"1\") } struct Response { 1: required string Message (vt.in = \"Debug\", vt.in = \"Info\", vt.in = \"Warn\", vt.in = \"Error\") } 使用 以快速开始里的 Kitex Hello 项目为例，进入示例仓库的 hello 目录，在 hello.thrift 中添加注解，例如我们对 Request 结构体的 message 字段进行约束，约束长度不超过8且要以 “kitex-” 前缀开头：\nstruct Request { 1: string message (vt.max_size = \"8\", vt.prefix = \"kitex-\") } 在生成Kitex代码时，加上 --thrift-plugin validator 参数，即可生成 validator 文件。\nkitex --thrift-plugin validator -service a.b.c hello.thrift 执行后，可以看见新生成的 Validator：\n├── kitex_gen └── api ├── hello │ ├── client.go │ ├── hello.go │ ├── invoker.go │ └── server.go ├── hello.go --\u003e ├── hello_validator.go ├── k-consts.go └── k-hello.go 其中对于 Request 结构体，新生成了 IsValid() 方法：\nfunc (p *Request) IsValid() error { if len(p.Message) \u003e int(8) { return fmt.Errorf(\"field Message max_len rule failed, current value: %d\", len(p.Message)) } _src := \"kitex-\" if !strings.HasPrefix(p.Message, _src) { return fmt.Errorf(\"field Message prefix rule failed, current value: %v\", p.Message) } return nil } 在后续的使用中，调用 IsValid() 方法对结构体进行校验即可：\n req := \u0026api.Request { //.... } err := req.IsValid() if err != nil { //invalid .... } //valid ... 支持的校验能力 校验顺序以定义顺序为准， ‘in’ 和 ‘not_in’ 这类可以定义多次的，以第一次出现的顺序为准。\n数字类型 包括 i8，i16，i32，i64，double。\n const，必须为指定值。 lt，le，gt，ge，分别表示小于，小于等于，大于，大于等于。 in，not_in，分别表示可以使用的值和不可以使用的值，可多次指定，一次指定一个值。 not_nil，该字段不能为空。（仅当字段为 optional 时合法）  structNumericDemo{1:doubleValue (vt.gt=\"1000.1\",vt.lt=\"10000.1\")2:i8Type (vt.in=\"1\",vt.in=\"2\",vt.in=\"4\")3:i64MagicNumber (vt.const=\"0x5f3759df\")}string/binary  const，必须为指定值。 min_size，max_size，最大长度，最小长度。 pattern，正则匹配。 prefix，suffix，contains，not_contains，限制前缀，限制后缀，必须包含，不能包含。 in，not_in，分别表示可以使用的值和不可以使用的值，二者不能同时使用，可多次指定，一次指定一个值。 not_nil，该字段不能为空。（仅当字段为 optional 时合法）  structStringDemo{1:stringUninitialized (vt.const=\"烫烫烫\")2:stringName (vt.min_size=\"6\",vt.max_size=\"12\")3:stringSomeStuffs (vt.pattern=\"[0-9A-Za-z]+\")4:stringDebugInfo (vt.prefix=\"[Debug]\")5:stringPanicInfo (vt.contains=\"panic\")6:stringEditor (vt.in=\"vscode\",vt.in=\"vim\",vt.in=\"goland\")}bool  const，必须为指定值。 not_nil，该字段不能为空。（仅当字段为 optional 时合法）  structBoolDemo{1:boolAMD (vt.const=\"true\")2:optionalboolNvidia (vt.not_nil=\"false\")}enum  const，必须为指定值。 defined_only，必须在 enum 中定义的值中。 not_nil，该字段不能为空。（仅当字段为 optional 时合法）  enumType{Number,String,List,Map}structEnumDemo{1:TypeAddressType (vt.const=\"String\")2:TypeValueType (vt.defined_only=\"true\")3:optionalTypeOptType (vt.not_nil=\"true\")}set/list  min_size，max_size，最小长度，最大长度。 elem，元素约束。  structSetListDemo{1:list\u003cstring\u003ePersons (vt.min_size=\"5\",vt.max_size=\"10\")2:set\u003cdouble\u003eHealthPoints (vt.elem.gt=\"0\")}map  min_size，max_size，最小键值对数，最大键值对数。 no_sparse，value 为指针时，不能为 nil 。 key，value，键约束，值约束。  structMapDemo{1:map\u003ci32,string\u003eIdName (vt.min_size=\"5\",vt.max_size=\"10\")2:map\u003ci32,DemoTestRequest\u003eRequests (vt.no_sparse=\"true\")3:map\u003ci32,double\u003eSome,(vt.key.gt=\"0\",vt.value.lt=\"1000\")}struct/union/exception  skip，跳过该 struct/union/exception 的递归校验。（作为单独字段时默认为 false，作为元素时默认为 true ） not_nil，该字段不能为空。  structOuterRequest{1:SomeStructStruct (vt.skip=\"true\")2:SomeUnionUnion (vt.skip=\"true\")3:SomeStructNotNilStruct (vt.not_nil=\"true\")}变量引用 前置符 $ 表示某个变量的引用，可用于跨字段校验：\n $x 代表名为 x 的变量，变量名为 \\[a-zA-Z0-9_]\\，其作用域规则为当前结构体。 $ 表示 validator 所处的当前字段。  structExample{1:stringA (vt.max_size=\"$C\")2:stringB (vt.not_in=\"$A\")3:i32C}工具函数 前置符 @ 表示内置的工具函数来计算校验值，目前支持的工具函数：\n sprintf(fmt, $1, $2...) 用于输出特定字符。 len($x) 输出变量大小。（字符串长度、list 元素个数）  structExample{1:stringA2:list\u003cstring\u003eB (vt.max_size=\"@len($D)\")3:map\u003cstring,int)C4:stringD (vt.const=\"@sprintf(\\\"%s_%s\\\", $A, \\\"mysuffix\\\")\")}","categories":"","description":"","excerpt":"概述 Validator 是用于支持结构体校验能力的 thriftgo 插件。\n在 IDL 中通过注解来描述约束， …","ref":"/zh/docs/kitex/tutorials/code-gen/validator/","tags":"","title":"Thrift Validator"},{"body":"TLS Hertz Server \u0026 Client currently only supports TLS for the standard network library, and the support for the Netpoll network library is still on the way. Usage Reference: Hertz Example and Hertz Config\nALPN ALPN can be switched on or off with a switch after TLS is enabled.(depending on whether all required protocol Servers are currently registered via Protocol Suite)\nWebSocket WebSocket is already in use in internal production environments, so please look forward to hearing from us. Please feel free to raise an issue if you need it.\nHTTP2 HTTP2 is already in use in internal production environments, so please look forward to hearing from us. Please feel free to raise an issue if you need it.\n","categories":"","description":"","excerpt":"TLS Hertz Server \u0026 Client currently only supports TLS for the standard …","ref":"/docs/hertz/tutorials/basic-feature/protocol/","tags":"","title":"Protocol"},{"body":"TLS Hertz Server \u0026 Client 目前只有 标准网络库 支持 TLS，Netpoll 网络库的支持还在路上。 使用参考： Hertz 示例 和 Hertz 配置\nALPN 开启 TLS 之后，可以通过开关控制 ALPN 是否开启（依赖当前是否通过 Protocol Suite 注册了所需要的所有协议 Servers）\nWebSocket 内部生产环境已在使用，如有需求可提 issue，敬请期待。\nHTTP2 内部生产环境已在使用，如有需求可提 issue，敬请期待。\n","categories":"","description":"","excerpt":"TLS Hertz Server \u0026 Client 目前只有 标准网络库 支持 TLS，Netpoll 网络库的支持还在路上。 使用参考： …","ref":"/zh/docs/hertz/tutorials/basic-feature/protocol/","tags":"","title":"协议"},{"body":"会议主题： CloudWeGo 社区会议 3.25\n参会人员： YangruiEmma, liu-song, baiyutang, yccpt, AshleeT, Authorixy, Dianjun Suo, bodhisatan, CoderPoet, Quan Hu, li-jin-gou, JZK-Keven, EastHorse, GuangmingLuo, Xiwen Li, joway, jasondeng1997, HeyJavaBean.\n会前必读： http://www.cloudwego.io/; https://github.com/cloudwego\n议程 1 ：新成员自我介绍 内容：社区新成员和首次参加社区会议的内部成员分别进行自我介绍，主要包含个人基本情况、历史贡献和个人未来规划。\n议程 2 ：Kitex 单测任务进展介绍   领取进度： 10/14。\n  如何认领任务： 在任务认领页面下方的评论中，留言你需要认领的项目，之后会分配给你。\n  提交 PR 注意事项 ：\na. 提交 PR 一定要关联 Issue (可以在 PR 描述里面进行 Issue 关联)。\nb. Kitex 单测任务的 PR 的描述前缀统一使用 Test，便于相关同学进行 review。\nc. 提交了 PR之后，可以将 PR 发送在群里，方便后续跟进。\n  提交 PR 时间要求： 认领之后半个月内提交 PR，便于后续的意见修改和调试。\n  议程 3：源码分析落地   参考案例： 具体可以参考 Go-zero 和 Kratos 开源社区。例如：对框架一些较好的设计进行解读，提供“扩展阅读”文档，目录可以涵盖“日志组件介绍”、“令牌桶限流”等文档内容。\n  后续规划：\na. 草拟源码分析目录大纲：① 目录结构和内容可以参考 CloudWeGo 官网目录；② 源码分析目录文档完成后，可以发在群里或者在 Github 上提交 Issue ，方便大家讨论修改；③ 认领单测任务的同学可以关注一下源码分析活动，助于更好地了解模块的功能。\nb. 宣传推广：后续会讨论宣传方案（例如征文比赛），也鼓励做出贡献的同学寻找渠道进行推广。\n  议程4：Q\u0026A Q：写 Retry 的单测时，Retry 的单测其实是要配合 Kitex 的 Client 一起使用的。但是如果要把单测写到 Retry 下面的话，就需要引一个 Client 才能去写，这样就导致 Client 单测下面可能也有 Retry ，会存在一个循环依赖的问题？\nA：确实存在循坏依赖的情况。对于这种情况，可以使用 mock，比如你需要用到 Client，那你可能要专门去 mock 一个 Client；除此之外，单测使用 xxx_test package，也可以解决循环依赖的问题。\n议程5：社区建议 欢迎大家将参与社区建设期间遇到的任何问题和想法发在群里，同社区成员一起沟通。内容不限于 Kitex、Netpoll 代码库、CloudWeGo 官网、宣传渠道等。\n","categories":"","description":"","excerpt":"会议主题： CloudWeGo 社区会议 3.25\n参会人员： YangruiEmma, liu-song, baiyutang, …","ref":"/zh/community/meeting_notes/2022-03-25/","tags":"","title":"CloudWeGo 社区会议 3.25"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kitex/tutorials/advanced-feature/","tags":"","title":"Advanced Feature"},{"body":"SDK Mode（invoker）provides a way to call Kitex server just like a SDK.\nmessage is used to start a call, you should use local and remote two net.Addr to initialize message. Which indicates local and remote address (used in logging and tracing). After initialization, you can use SetRequestBytes(buf []byte) error to setup request binary. Then call Call method of invoker to start a call. After call, you can use GetResponseBytes() ([]byte, error) of message to get response binary.\nimport ( ... \"github.com/cloudwego/kitex/sdk/message\" ... ) func main() { var reqPayload, respPayload []byte var local, remote net.Addr ... // init local/remote  ... ivk := echo.NewInvoker(new(EchoImpl)) msg := message.NewMessage(local, remote) // setup request payload  msg.SetRequestBytes(reqPayload) // start a call  err := ivk.Call(msg) if err != nil { ... } respPayload, err = msg.GetResponseBytes() if err != nil { ... } } ","categories":"","description":"","excerpt":"SDK Mode（invoker）provides a way to call Kitex server just like a SDK. …","ref":"/docs/kitex/tutorials/advanced-feature/invoker/","tags":"","title":"Server SDK Mode"},{"body":"Kitex supports opentelemetry tracer, opentracing tracer, and also customized tracer.\nopentelemetry Kitex’s opentelemetry extension provides support for tracing, metrics and logging.\nExample\nclient side:\nimport ( ... \"github.com/kitex-contrib/obs-opentelemetry/provider\" \"github.com/kitex-contrib/obs-opentelemetry/tracing\" ) func main(){ serviceName := \"echo-client\" p := provider.NewOpenTelemetryProvider( provider.WithServiceName(serviceName), provider.WithExportEndpoint(\"localhost:4317\"), provider.WithInsecure(), ) defer p.Shutdown(context.Background()) c, err := echo.NewClient( \"echo\", client.WithSuite(tracing.NewClientSuite()), // Please keep the same as provider.WithServiceName  client.WithClientBasicInfo(\u0026rpcinfo.EndpointBasicInfo{ServiceName: serviceName}), ) if err != nil { klog.Fatal(err) } } server side:\nimport ( ... \"github.com/kitex-contrib/obs-opentelemetry/provider\" \"github.com/kitex-contrib/obs-opentelemetry/tracing\" ) func main() { serviceName := \"echo\" p := provider.NewOpenTelemetryProvider( provider.WithServiceName(serviceName), provider.WithExportEndpoint(\"localhost:4317\"), provider.WithInsecure(), ) defer p.Shutdown(context.Background()) svr := echo.NewServer( new(EchoImpl), server.WithSuite(tracing.NewServerSuite()), // Please keep the same as provider.WithServiceName  server.WithServerBasicInfo(\u0026rpcinfo.EndpointBasicInfo{ServiceName: serviceName}), ) if err := svr.Run(); err != nil { klog.Fatalf(\"server stopped with error:\", err) } } For more information see obs-opentelemetry\nopentracing client side, use opentracing GlobalTracer by default\nimport ( \"github.com/cloudwego/kitex/client\" \"github.com/cloudwego/kitex-examples/kitex_gen/api/echo\" internal_opentracing \"github.com/kitex-contrib/tracer-opentracing\" ) ... tracer := internal_opentracing.NewDefaultClientSuite() client, err := echo.NewClient(\"echo\", client.WithSuite(tracer)) if err != nil { log.Fatal(err) } server side, use opentracing GlobalTracer by default\nimport ( \"github.com/cloudwego/kitex/server\" \"github.com/cloudwego/kitex-examples/kitex_gen/api/echo\" internal_opentracing \"github.com/kitex-contrib/tracer-opentracing\" ) ... tracer := internal_opentracing.NewDefaultServerSuite() svr, err := echo.NewServer(new(EchoImpl), server.WithSuite(tracer)) if err := svr.Run(); err != nil { log.Println(\"server stopped with error:\", err) } else { log.Println(\"server stopped\") } For more information see tracer-opentracing\nCustomize opentracing tracer and operation name client side:\nimport ( ... ko \"github.com/kitex-contrib/opentracing\" \"github.com/opentracing/opentracing-go\" \"github.com/cloudwego/kitex/pkg/endpoint\" \"github.com/cloudwego/kitex/pkg/rpcinfo\" ... ) ... myTracer := opentracing.GlobalTracer() operationNameFunc := func(ctx context.Context) string { endpoint := rpcinfo.GetRPCInfo(ctx).To() return endpoint.ServiceName() + \"::\" + endpoint.Method() } ... client, err := echo.NewClient(\"echo\", ko.ClientOption(myTracer, operationNameFunc)) if err != nil { log.Fatal(err) } server side:\nimport ( ... ko \"github.com/kitex-contrib/opentracing\" \"github.com/opentracing/opentracing-go\" \"github.com/cloudwego/kitex/pkg/endpoint\" \"github.com/cloudwego/kitex/pkg/rpcinfo\" ... ) ... myTracer := opentracing.GlobalTracer() operationNameFunc := func(ctx context.Context) string { endpoint := rpcinfo.GetRPCInfo(ctx).To() return endpoint.ServiceName() + \"::\" + endpoint.Method() } ... svr, err := echo.NewServer(ko.ClientOption(myTracer, operationNameFunc)) if err := svr.Run(); err != nil { log.Println(\"server stopped with error:\", err) } else { log.Println(\"server stopped\") } Customize tracer tracer interface:\ntype Tracer interface { Start(ctx context.Context) context.Context Finish(ctx context.Context) } Example\nclient side:\nimport \"github.com/cloudwego/kitex/client\" ... type myTracer struct {} func (m *myTracer) Start(ctx context.Context) context.Context { _, ctx = opentracing.StartSpanFromContextWithTracer(ctx, o.tracer, \"RPC call\") return ctx } func (m *myTracer) Finish(ctx context.Context) { span := opentracing.SpanFromContext(ctx) span.Finish() } ... client, err := echo.NewClient(\"echo\", client.WithTracer(\u0026myTracer{})) if err != nil { log.Fatal(err) } server side:\nimport \"github.com/cloudwego/kitex/server\" ... type myTracer struct {} func (m *myTracer) Start(ctx context.Context) context.Context { _, ctx = opentracing.StartSpanFromContextWithTracer(ctx, o.tracer, \"RPC handle\") return ctx } func (m *myTracer) Finish(ctx context.Context) { span := opentracing.SpanFromContext(ctx) span.Finish() } ... svr, err := echo.NewServer(server.WithTracer(\u0026myTracer{})) if err := svr.Run(); err != nil { log.Println(\"server stopped with error:\", err) } else { log.Println(\"server stopped\") } ","categories":"","description":"","excerpt":"Kitex supports opentelemetry tracer, opentracing tracer, and also …","ref":"/docs/kitex/tutorials/service-governance/tracing/","tags":"","title":"Tracing"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kitex/tutorials/","tags":"","title":"Tutorials"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kitex/tutorials/","tags":"","title":"指南"},{"body":"Kitex supports user-defined registration module. Users can extend and integrate other registration centers by themselves. This extension is defined under pkg/registry.\nExtension API and Definition of Info Struct  Extension API  // Registry is extension interface of service registry. type Registry interface { Register(info *Info) error Deregister(info *Info) error }  Definition of Info Struct Kitex defines some registration information. Users can also expand the registration information into tags as needed.  // Info is used for registry. // The fields are just suggested, which is used depends on design. type Info struct { // ServiceName will be set in kitex by default \tServiceName string // Addr will be set in kitex by default \tAddr net.Addr // PayloadCodec will be set in kitex by default, like thrift, protobuf \tPayloadCodec string Weight int StartTime time.Time WarmUp time.Duration // extend other infos with Tags. \tTags map[string]string } Integrate into Kitex Specify your own registration module and customized registration information through option. Note that registration requires service information, which is also specified through option.\n  Specify Server Info\noption: WithServerBasicInfo\nebi := \u0026rpcinfo.EndpointBasicInfo{ ServiceName: \"yourServiceName\", Tags: make(map[string]string), } ebi.Tags[idc] = \"xxx\" svr := xxxservice.NewServer(handler, server.WithServerBasicInfo(ebi))   Specify Custom Registion module\noption: WithRegistry\nsvr := xxxservice.NewServer(handler, server.WithServerBasicInfo(ebi), server.WithRegistry(yourRegistry))   Custom RegistryInfo\nKitex sets ServiceName, Addr and PayloadCodec by default. If other registration information is required, you need to inject it by yourself. option: WithRegistryInfo.\nsvr := xxxservice.NewServer(handler, server.WithRegistry(yourRegistry), server.WithRegistryInfo(yourRegistryInfo))   ","categories":"","description":"","excerpt":"Kitex supports user-defined registration module. Users can extend and …","ref":"/docs/kitex/tutorials/framework-exten/registry/","tags":"","title":"Extension of Service Registry"},{"body":"SDK化（invoker）允许用户将 Kitex server 当作一个本地 SDK 调用。\n调用通过 message 完成，初始化 message 需要 local 和 remote 两个 net.Addr ，分别表示本地地址和远端（客户端）地址（此处的地址主要用于日志监控），初始化后通过 SetRequestBytes(buf []byte) error 设置请求的二进制数据。最后调用 invoker 的 Call 方法即可完成调用。调用完成后可通过 message 的 GetResponseBytes() ([]byte, error) 获取响应的二进制数据。\nimport ( ... \"github.com/cloudwego/kitex/sdk/message\" ... ) func main() { var reqPayload, respPayload []byte var local, remote net.Addr ... // init local/remote  ... ivk := echo.NewInvoker(new(EchoImpl)) msg := message.NewMessage(local, remote) // 装载payload  msg.SetRequestBytes(reqPayload) // 发起调用  err := ivk.Call(msg) if err != nil { ... } respPayload, err = msg.GetResponseBytes() if err != nil { ... } } ","categories":"","description":"","excerpt":"SDK化（invoker）允许用户将 Kitex server 当作一个本地 SDK 调用。\n调用通过 message 完成， …","ref":"/zh/docs/kitex/tutorials/advanced-feature/invoker/","tags":"","title":"Server SDK化"},{"body":"Kitex 支持自定义注册模块，使用者可自行扩展集成其他注册中心，该扩展定义在 pkg/registry 下。\n扩展接口和 Info 定义  扩展接口  // Registry is extension interface of service registry. type Registry interface { Register(info *Info) error Deregister(info *Info) error }  Info 定义 Kitex 定义了部分注册信息，使用者也可以根据需要自行扩展注册信息到 Tags 中。  // Info is used for registry. // The fields are just suggested, which is used depends on design. type Info struct { // ServiceName will be set in kitex by default  ServiceName string // Addr will be set in kitex by default  Addr net.Addr // PayloadCodec will be set in kitex by default, like thrift, protobuf  PayloadCodec string Weight int StartTime time.Time WarmUp time.Duration // extend other infos with Tags.  Tags map[string]string } 集成到 Kitex 通过 option 指定自己的注册模块和自定义的注册信息。注意注册需要服务信息，服务信息也是通过 option 指定。\n  指定服务信息\noption: WithServerBasicInfo\nebi := \u0026rpcinfo.EndpointBasicInfo{ ServiceName: \"yourServiceName\", Tags: make(map[string]string), } ebi.Tags[idc] = \"xxx\" svr := xxxservice.NewServer(handler, server.WithServerBasicInfo(ebi))   指定自定义注册模块\noption: WithRegistry\nsvr := xxxservice.NewServer(handler, server.WithServerBasicInfo(ebi), server.WithRegistry(yourRegistry))   自定义 RegistryInfo\nKitex 默认赋值 ServiceName、Addr 和 PayloadCodec，若需要其他注册信息需要使用者自行注入。option: WithRegistryInfo\nsvr := xxxservice.NewServer(handler, server.WithRegistry(yourRegistry), server.WithRegistryInfo(yourRegistryInfo))   ","categories":"","description":"","excerpt":"Kitex 支持自定义注册模块，使用者可自行扩展集成其他注册中心，该扩展定义在 pkg/registry 下。\n扩展接口和 Info …","ref":"/zh/docs/kitex/tutorials/framework-exten/registry/","tags":"","title":"服务注册扩展"},{"body":"Kitex 提供了对 opentelemetry 和 opentracing 的支持，也支持用户自定义链路跟踪。\nopentelemetry Kitex 的 opentelemetry 扩展 提供了 tracing、metrics、logging 的支持\n示例:\nclient 侧\nimport ( ... \"github.com/kitex-contrib/obs-opentelemetry/provider\" \"github.com/kitex-contrib/obs-opentelemetry/tracing\" ) func main(){ serviceName := \"echo-client\" p := provider.NewOpenTelemetryProvider( provider.WithServiceName(serviceName), provider.WithExportEndpoint(\"localhost:4317\"), provider.WithInsecure(), ) defer p.Shutdown(context.Background()) c, err := echo.NewClient( \"echo\", client.WithSuite(tracing.NewClientSuite()), // Please keep the same as provider.WithServiceName  client.WithClientBasicInfo(\u0026rpcinfo.EndpointBasicInfo{ServiceName: serviceName}), ) if err != nil { klog.Fatal(err) } } server 侧\nimport ( ... \"github.com/kitex-contrib/obs-opentelemetry/provider\" \"github.com/kitex-contrib/obs-opentelemetry/tracing\" ) func main() { serviceName := \"echo\" p := provider.NewOpenTelemetryProvider( provider.WithServiceName(serviceName), provider.WithExportEndpoint(\"localhost:4317\"), provider.WithInsecure(), ) defer p.Shutdown(context.Background()) svr := echo.NewServer( new(EchoImpl), server.WithSuite(tracing.NewServerSuite()), // Please keep the same as provider.WithServiceName  server.WithServerBasicInfo(\u0026rpcinfo.EndpointBasicInfo{ServiceName: serviceName}), ) if err := svr.Run(); err != nil { klog.Fatalf(\"server stopped with error:\", err) } } 更多信息参考 obs-opentelemetry\nopentracing client 侧，默认使用 opentracing GlobalTracer\nimport ( \"github.com/cloudwego/kitex/client\" \"github.com/cloudwego/kitex-examples/kitex_gen/api/echo\" internal_opentracing \"github.com/kitex-contrib/tracer-opentracing\" ) ... tracer := internal_opentracing.NewDefaultClientSuite() client, err := echo.NewClient(\"echo\", client.WithSuite(tracer)) if err != nil { log.Fatal(err) } server 侧，默认使用 opentracing GlobalTracer\nimport ( \"github.com/cloudwego/kitex/server\" \"github.com/cloudwego/kitex-examples/kitex_gen/api/echo\" internal_opentracing \"github.com/kitex-contrib/tracer-opentracing\" ) ... tracer := internal_opentracing.NewDefaultServerSuite() svr, err := echo.NewServer(new(EchoImpl), server.WithSuite(tracer)) if err := svr.Run(); err != nil { log.Println(\"server stopped with error:\", err) } else { log.Println(\"server stopped\") } 更多信息参考 tracer-opentracing\n自定义 opentracing tracer 和 operation name client 侧\nimport ( ... ko \"github.com/kitex-contrib/opentracing\" \"github.com/opentracing/opentracing-go\" \"github.com/cloudwego/kitex/pkg/endpoint\" \"github.com/cloudwego/kitex/pkg/rpcinfo\" ... ) ... myTracer := opentracing.GlobalTracer() operationNameFunc := func(ctx context.Context) string { endpoint := rpcinfo.GetRPCInfo(ctx).To() return endpoint.ServiceName() + \"::\" + endpoint.Method() } ... client, err := echo.NewClient(\"echo\", ko.ClientOption(myTracer, operationNameFunc)) if err != nil { log.Fatal(err) } server 侧\nimport ( ... ko \"github.com/kitex-contrib/opentracing\" \"github.com/opentracing/opentracing-go\" \"github.com/cloudwego/kitex/pkg/endpoint\" \"github.com/cloudwego/kitex/pkg/rpcinfo\" ... ) ... myTracer := opentracing.GlobalTracer() operationNameFunc := func(ctx context.Context) string { endpoint := rpcinfo.GetRPCInfo(ctx).To() return endpoint.ServiceName() + \"::\" + endpoint.Method() } ... svr, err := echo.NewServer(ko.ClientOption(myTracer, operationNameFunc)) if err := svr.Run(); err != nil { log.Println(\"server stopped with error:\", err) } else { log.Println(\"server stopped\") } 自定义 tracer tracer 的定义如下：\ntype Tracer interface { Start(ctx context.Context) context.Context Finish(ctx context.Context) } 示例：\nclient 侧\nimport \"github.com/cloudwego/kitex/client\" ... type myTracer struct {} func (m *myTracer) Start(ctx context.Context) context.Context { _, ctx = opentracing.StartSpanFromContextWithTracer(ctx, o.tracer, \"RPC call\") return ctx } func (m *myTracer) Finish(ctx context.Context) { span := opentracing.SpanFromContext(ctx) span.Finish() } ... client, err := echo.NewClient(\"echo\", client.WithTracer(\u0026myTracer{})) if err != nil { log.Fatal(err) } server 侧\nimport \"github.com/cloudwego/kitex/server\" ... type myTracer struct {} func (m *myTracer) Start(ctx context.Context) context.Context { _, ctx = opentracing.StartSpanFromContextWithTracer(ctx, o.tracer, \"RPC handle\") return ctx } func (m *myTracer) Finish(ctx context.Context) { span := opentracing.SpanFromContext(ctx) span.Finish() } ... svr, err := echo.NewServer(server.WithTracer(\u0026myTracer{})) if err := svr.Run(); err != nil { log.Println(\"server stopped with error:\", err) } else { log.Println(\"server stopped\") } ","categories":"","description":"","excerpt":"Kitex 提供了对 opentelemetry 和 opentracing 的支持，也支持用户自定义链路跟踪。\nopentelemetry …","ref":"/zh/docs/kitex/tutorials/service-governance/tracing/","tags":"","title":"链路跟踪"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kitex/tutorials/advanced-feature/","tags":"","title":"高级特性"},{"body":"1. How to configure the number of pollers ? NumLoops represents the number of epoll created by [Netpoll][Netpoll], which has been automatically adjusted according to the number of P (runtime.GOMAXPROCS(0)) by default, and users generally don’t need to care.\nBut if your service has heavy I/O, you may need the following configuration:\npackage main import ( \"runtime\" \"github.com/cloudwego/netpoll\" ) func init() { netpoll.SetNumLoops(runtime.GOMAXPROCS(0)) } 2. How to configure poller’s connection loadbalance ? When there are multiple pollers in Netpoll, the connections in the service process will be loadbalanced to each poller.\nThe following strategies are supported now:\n Random  The new connection will be assigned to a randomly picked poller.   RoundRobin  The new connection will be assigned to the poller in order.    Netpoll uses RoundRobin by default, and users can change it in the following ways:\npackage main import ( \"github.com/cloudwego/netpoll\" ) func init() { netpoll.SetLoadBalance(netpoll.Random) // or \tnetpoll.SetLoadBalance(netpoll.RoundRobin) } 3. How to configure gopool ? Netpoll uses gopool as the goroutine pool by default to optimize the stack growth problem that generally occurs in RPC services.\nIn the project gopool, it explains how to change its configuration, so won’t repeat it here.\nOf course, if your project does not have a stack growth problem, it is best to close gopool as follows:\npackage main import ( \"github.com/cloudwego/netpoll\" ) func init() { netpoll.DisableGopool() } 4. How to prepare a new connection ? There are different ways to prepare a new connection on the client and server.\n On the server side, OnPrepare is defined to prepare for the new connection, and it also supports returning a context, which can be reused in subsequent business processing. WithOnPrepare provides this registration. When the server accepts a new connection, it will automatically execute the registered OnPrepare function to complete the preparation work. The example is as follows:  package main import ( \"context\" \"github.com/cloudwego/netpoll\" ) func main() { // register OnPrepare \tvar onPrepare netpoll.OnPrepare = prepare evl, _ := netpoll.NewEventLoop(handler, netpoll.WithOnPrepare(onPrepare)) ... } func prepare(connection netpoll.Connection) (ctx context.Context) { ... prepare connection ... return } On the client side, the connection preparation needs to be completed by the user. Generally speaking, the connection created by Dialer can be controlled by the user, which is different from passively accepting the connection on the server side. Therefore, the user not relying on the trigger, just prepare a new connection like this:  package main import ( \"context\" \"github.com/cloudwego/netpoll\" ) func main() { conn, err := netpoll.DialConnection(network, address, timeout) if err != nil { panic(\"dial netpoll connection failed\") } ... prepare here directly ... prepare(conn) ... } func prepare(connection netpoll.Connection) (ctx context.Context) { ... prepare connection ... return } 5. How to configure connection timeout ? Netpoll now supports two timeout configurations:\n Read Timeout  In order to maintain the same operating style as net.Conn, Connection.Reader is also designed to block reading. So provide Read Timeout. Read Timeout has no default value(wait infinitely), it can be configured via Connection or EventLoop.Option, for example:    package main import ( \"github.com/cloudwego/netpoll\" ) func main() { var conn netpoll.Connection // 1. setting by Connection \tconn.SetReadTimeout(timeout) // or  // 2. setting with Option \tnetpoll.NewEventLoop(handler, netpoll.WithReadTimeout(timeout)) ... } Idle Timeout  Idle Timeout utilizes the TCP KeepAlive mechanism to kick out dead connections and reduce maintenance overhead. When using Netpoll, there is generally no need to create and close connections frequently, and idle connections have little effect. When the connection is inactive for a long time, in order to prevent dead connection caused by suspended animation, hang of the opposite end, abnormal disconnection, etc., the connection will be actively closed after the Idle Timeout. The default minimum value of Idle Timeout is 10min, which can be configured through Connection API or EventLoop.Option, for example:    package main import ( \"github.com/cloudwego/netpoll\" ) func main() { var conn netpoll.Connection // 1. setting by Connection \tconn.SetIdleTimeout(timeout) // or  // 2. setting with Option \tnetpoll.NewEventLoop(handler, netpoll.WithIdleTimeout(timeout)) ... } 6. How to configure connection read event callback ? OnRequest refers to the callback triggered by Netpoll when a read event occurs on the connection. On the Server side, when creating the EventLoop, you can register an OnRequest, which will be triggered when each connection data arrives and perform business processing. On the Client side, there is no OnRequest by default, and it can be set via API when needed. E.g:\npackage main import ( \"context\" \"github.com/cloudwego/netpoll\" ) func main() { var onRequest netpoll.OnRequest = handler // 1. on server side \tevl, _ := netpoll.NewEventLoop(onRequest, opts...) ... // 2. on client side \tconn, _ := netpoll.DialConnection(network, address, timeout) conn.SetOnRequest(handler) ... } func handler(ctx context.Context, connection netpoll.Connection) (err error) { ... handling ... return nil } 7. How to configure the connection close callback ? CloseCallback refers to the callback triggered by Netpoll when the connection is closed, which is used to perform additional processing after the connection is closed. Netpoll is able to perceive the connection status. When the connection is closed by peer or cleaned up by self, it will actively trigger CloseCallback instead of returning an error on the next Read or Write(the way of net.Conn). Connection provides API for adding CloseCallback, callbacks that have been added cannot be removed, and multiple callbacks are supported.\npackage main import ( \"github.com/cloudwego/netpoll\" ) func main() { var conn netpoll.Connection // add close callback \tvar cb netpoll.CloseCallback = callback conn.AddCloseCallback(cb) ... } func callback(connection netpoll.Connection) error { return nil } ","categories":"","description":"","excerpt":"1. How to configure the number of pollers ? NumLoops represents the …","ref":"/docs/netpoll/common-usage/","tags":"","title":"Common Usage"},{"body":"案例介绍   飞书管理后台是飞书套件专为企业管理员提供的信息管理平台，在单体应用架构下，它面临了一系列的挑战。 它通过引入 Kitex 泛化调用对飞书管理后台进行平台化改造，使之变为业务网关，提供一套统一的标准和通用服务，让有管控诉求的套件业务方能快速实现能力集成，并且提供一致的体验。最终实现了飞书管理后台作为企业统一数字化管理平台的愿景。\n本文将从三个方面为大家讲解 Kitex 泛化调用在飞书管理后台平台化改造过程中的落地实践：\n 架构和挑战，即飞书管理后台单体架构面临的各种挑战； 平台化构想，即飞书管理后台平台化构想和架构升级； 平台化实现，包括微前端技术架构、泛化调用实践和功能扩展。  架构和挑战 飞书是真正的一站式企业沟通与协作平台，整合视频会议、即时消息、日历、云文档、邮箱、工作台等功能于一体，立志打造高效的办公方式，加速企业成长。 飞书管理后台（以下简称 Admin）是飞书套件专为企业管理员提供的信息管理平台，企业管理员可通过后台管理企业设置、组织架构、工作台和会议室等功能。下图是飞书管理后台的界面。\n平台改造背景 飞书采用的是 all-in-one 的套件模式，Admin 作为整个套件统一的管理后台，承接了包括组织管理、云文档、视频会议、邮箱、开放平台等 10 多个业务线的管控需求。 一直以来的开发模式是各业务方直接在 Admin 的代码仓库提交代码或者由 Admin 团队负责 Web 层逻辑的开发。下图是目前飞书管理后台中包括的一些功能， 可以看到功能种类还是非常多的，之前的开发模式是业务方直接在 Admin 的代码仓库中提交代码，或者由业务方给 Admin 团队提供一些需求，由我们来负责 Web 层逻辑的开发。 从飞书初创开始，Admin 就是以单体应用的模式开发的，随着后续飞书整体的演进，我们的团队越来越多，不同业务线的团队也会有一些管控需求要接入 Admin 平台，因此他们就直接在代码仓库中提交代码。\nAdmin 架构 下图是 Admin 旧架构图。上面是 Admin 前端，它其实就是 Node 层的单体，中间是 Admin 后端，它基于我们内部单体 HTTP 的 Web 服务，会通过 RPC 调用到其他业务线的微服务。\n面临的问题 在这个架构下我们会面临一些问题，第一个问题是业务迭代慢，因为所有业务线都只能在 Admin 的代码仓库里进行开发和发版，因此这些业务线完全依赖 Admin 的研发资源和迭代流程， Admin 的研发资源被过多的耗在各业务的迭代中，无法快速支持自身的业务规划，如组织架构、安全、KA等因为我们是 To B 的产品，因此发版节奏不会很快。 如果各个业务线有一些比较紧急的需求，也只能跟 Admin 的节奏，这就会造成发版节奏不一致，研发资源不匹配，导致Admin会成为业务迭代的瓶颈。 第二个问题是研发效率低，因为各个业务线需要在 Admin 的代码仓库里进行修改，因此需要了解我们仓库的设计模式，我们在为各个业务线提供服务时，也需要了解各业务的上下文，双方都需要花费大量时间沟通。 联调、Oncall的链路也很长，双方的责任也划不清楚，导致整体的研发效率偏低。第三个问题是工程质量差，多个团队共同维护一个代码仓库，代码质量参差不齐，设计规范也各不相同，底层的代码的修改还会相互影响，造成线上问题。\n面临的挑战 此外，我们还会面临很多挑战。首先面临的是多环境互通与隔离的问题，我们需要解决不同环境的网络隔离、版本异构问题；其次是接入业务复杂性，Admin需要集成十几个业务线，接入诉求不统一。 接口协议包含 HTTP 协议和 Thrift 协议，还有各种自定义插件需求和权限校验需求；最后还有安全保障，Admin 作为飞书套件的管理配置中心，关系到整个企业的数据安全。 安全一直是 Admin 最重要的需求，为了保障Admin 的接口数据安全，需要提供鉴权中间件、管理员权限验证、参数校验、风控、频控等功能，提升业务方的安全能力。\n平台化构想 第二部分给大家介绍飞书平台化构想，即如何进行飞书管理后台平台化构想和架构升级。\n首先要明确的是目标，主要目标是通过提供一套统一的标准和通用服务，让有管控诉求的套件业务方能快速实现能力集成，并且能给客户带来一致的体验。 因为我们各个业务线的管控需求都是集成在 Admin，我们不希望每个业务线提供的 Web 页面展示、功能和 UI 等差别较大，希望他们是相对统一的，最终实现Admin作为企业统一数字化管理平台的愿景。 关于在技术上需要达到的效果，我们希望业务方不要继续在 Admin 代码仓库中进行代码开发，而是直接提供我们的后端接口和前端页面动态接入，Admin 无需代码改造和服务发布即可无缝上线，Admin 从单体应用进化为业务网关，是包含 UI 交互在内的独立产品模块的集成。\n我们并不是要做一个搭建系统。目前很多平台型产品都提供 Low Code / No Code 的工具方便开发者快速搭建所需要的功能。 但是目前通过我们对客户诉求的调研，没有相关的需求（但是不代表未来也没有，这块我们会持续保持关注）。 我们需要做的是制定相关的标准，比如 UI、交互、API 等，业务按照标准去实现。我们也不是要做一个 API Gateway 或者 Service Mesh。 API Gateway 的核心是Exposes your services as managed APIs，将内部的服务以更加可控可管理的方式暴露出去，可以认为是后端服务的一个代理。 Service Mesh 可以看成是 API Gateway 的去中心化实现方式，用来解决单点、隔离、耦合等问题。我们需要解决的不仅仅是服务路由、协议转换、安全管控等问题，而是包含 UI 交互在内的独立产品模块的集成。\n旧的框架 这是我们旧的架构。它的前端架构是前后端分离的 Node 单体项目。后端架构（Golang 实现）采用 Hertz 框架对前端暴露 HTTP 接口，Handler 层通过 Kitex 调用依赖的各个业务线的微服务。\n框架介绍 下面介绍一下 CloudWeGo 现有的两款框架。首先是 Hertz 框架，Hertz [həːts] 是一个 Golang 微服务 HTTP 框架，在设计之初参考了其他开源框架 Fasthttp、Gin、Echo 的优势，并结合字节跳动内部的需求，使其具有高易用性、高性能、高扩展性等特点，目前在字节跳动内部已广泛使用。如今越来越多的微服务选择使用 Golang，如果对微服务性能有要求，又希望框架能够充分满足内部的可定制化需求，Hertz 会是一个不错的选择。Kitex [kaɪt’eks] 字节跳动内部的 Golang 微服务 RPC 框架，具有高性能、强可扩展的特点，在字节内部已广泛使用。如果对微服务性能有要求，又希望定制扩展融入自己的治理体系，可以考虑选择 Kitex。\n新的框架 下面介绍一下我们的新架构，它主要包括：\n Gaia 控制面。我们增加了 Gaia 平台（基于 Hertz 框架的 Web 服务）来作为我们整个 Admin 的控制系统，负责整体的发布和管控需求，包括接口的生命周期管理、微应用生命周期管理、监控告警、业务线接入、多环境发布等。 前端架构。前端采用微前端架构，各个业务方通过构建微应用接入 Admin 基座，使用统一封装好的组件库实现前端页面。 后端架构。后端使用字节通用 BAM 规范，通过泛化调用的方式打通 Admin 和各接入业务方服务，并抽象公共组件以插件的方式进行功能扩展。  Admin 架构 下图就是新的 Admin 架构图。左上方是微前端，它包含前端里面各个业务线的微应用。微前端通过 HTTP 接口和 Admin 网关进行交互，Admin 网关把业务逻辑都剥离到下一层， 而自身只负责公共组件、登录鉴权、协议代理和通用配置等通用需求，同时它会通过泛化调用来调用下游的业务服务。业务服务包括组织管理、云文档、视频会议和邮箱等微服务。 右侧是 Gaia 控制面，包括一些管控功能，如接口生命周期管理、监控大盘、微应用管理、工单系统等等。另外如果我们有一些独立的自定义功能，会通过插件的方式集成。\nGaia 平台功能 Gaia 平台主要包括以下功能：\n 业务线管理。业务线是实现以业务为维度进行接入 Admin 而提出的概念。通过业务线来聚合业务为维度的所有资源，相关资源包括微应用、菜单、接口、监控等。图中就是业务线管理的菜单页面。  接口生命周期管理。包括接口创建、更新、编排、发布、上线、下线、删除等。同时维护接口 IDL 文件。 微应用生命周期管理。包括微应用的申请、接入、微应用版本创建、发布、下线等。 控制大盘。包括业务整体维度和单接口维度的 SLA 大盘，以及错误告警管理。 插件管理。包括默认插件和自定义插件的配置管理。  平台化实现 微前端技术架构 第三部分具体介绍飞书平台化实现，包括微前端技术架构、泛化调用实践和功能扩展。\n下图是微前端的技术架构。这里涉及到三个概念，第一个是基座，即指微前端入口模块，负责组装各个模块；第二个是微应用，指独立的业务模块；第三个是微应用市场，负责管理微应用的创建，管理，版本发布等。 通过微应用市场下发的配置进行微应用组合，将基础能力下放到各个业务方。例如，现有一个新的业务线需要接入，那么它需要开发自己的微应用，打包测试并发布到我们的微应用市场，我们的基座就会从微应用市场接收到这个微应用，最后进行发布之后，就可以从 Web 看到对应模块的页面。\n泛化调用方案调研 接下来说一下后端实现的细节，即如何通过泛化调用实现整体依赖的剥离？首先讲一下这个问题的背景，Admin 的前端和后端是通过 JSON实现序列化传递的， 如果把 Admin 变成一个平台化的网关，不再维护业务逻辑，只处理通用逻辑，泛化调用是我们最好的选择。因为通过泛化调用，Admin 的网关就不需要写各种业务代码， 直接通过 RPC 接口就可以把前端传过来的 JSON 序列化数据、请求参数再传递到微服务，然后通过微服务的返回值把 JSON 序列化数据返回前端，跟前端进行交互。\n我们通过调研现有框架，如网关与微服务之间使用 gRPC、Thrift 等协议进行通信，都是通过代码生成实现的协议解析和协议传输，不能动态更新，都需要生成代码，再重新发布。 而我们内部旧的 Kite 框架（Thrift 协议）不支持泛化调用，而新框架 Kitex 是字节跳动内部的 Golang 微服务 RPC 框架，具有高性能、强可扩展的特点。 在我们使用 Kitex 的泛化调用功能之前曾调研了一些泛化调用的方案，也基于 Kitex 实现了泛化调用的类似功能。但是我们认为飞书内部实现泛化调用不如推动 Kitex 的研发人员， 让他们把泛化调用变为一个通用的功能，这样不仅仅是我们团队，公司内部其余团队以及 Kitex 开源后其他外部团队都可以使用这个功能。目前 Kitex 已经支持基于 Thrift 协议的泛化调用。\n非泛化调用 那么非泛化调用的实现方式和泛化调用的实现方式有什么不同呢？这张图就是非泛化调用的实现方式，无论 gRPC、Thrift 还是 Kitex 都是基于 IDL 生成协议代码， 服务端和客户端都需要依赖 IDL 生成静态代码，接口的迭代意味着服务端和客户端都需要升级代码重新发布。在 Admin 场景下意味着其他业务方的业务迭代， 需要我们引入代码依赖并发布服务，这并不符合我们平台化的需求。\nKitex 泛化调用 在 Kitex 泛化调用中，服务端无需做任何改造。客户端只有一份通用的协议处理代码，基于已有的 IDL 信息来动态生成协议字节流，IDL 信息可以动态更新，以维护最新的接口协议，无需生成代码。 在 Admin场景下，网关作为客户端，动态维护业务方接口的 IDL，通过泛化调用来实现 HTTP 接口到 RPC 接口的转换，不再依赖业务服务客户端代码，实现了网关和业务在代码层面的解耦。\n相关地址：https://github.com/cloudwego/kitex/tree/develop/pkg/generic/thrift\nHTTP 协议映射 Admin 网关是基于 Hertz 对外暴露 HTTP 协议的接口，Hertz 路由支持运行时新增，通过自定义 Middleware 和 HandlerFunc 可以实现接口运行时的增删改，这样可以实现解析修改后的 IDL 来进行接口调用。 这段代码就是初始化客户端的 Client，其实就是泛化调用的 Client，可以看到它会读业务方的 IDL，假如业务方的 IDL 有接口更新，我们可以通过这个进行业务更新，动态实现接口的上下线。 然后再构造 HTTP 类型的泛化调用 Client，每个业务方都会构造一个 Client 实例，比如有十几个业务线，就会生成十几个业务线微服务的实例。\n下面是泛化调用的路由，它其实是 HandlerFunc 的实现，通过这个方式可以动态注册路由，注册之后将 Hertz Request 转化成泛化调用 Request， 再通过前一步生成的泛化调用 Client 实现泛化调用，最后得到 HTTPResponse，再将它写回 Hertz Response中，这就是简单的泛化调用路由的实现。通过这个可以做很多业务拓展，比如错误码处理等等。\n功能扩展 具体给大家讲一下功能扩展。功能扩展的第一类就是 Kitex 提供的自定义注解，Kitex 内置了 API 注解来实现路由解析、参数传递等功能。 这里面有三个接口，第一个是 HTTPMapping，实现了参数传递、返回值等等自定义注解；第二个是 Route，实现了 Kitex 路由解析的功能； 第三个是 ValueMapping，是指将参数进行映射，比如目前 JSON 不支持 Int64，但 Go 可以支持 Int64，在使用 JSON 序列化的时候就要把参数类型定义为 String， 因此从 JSON 到 Go 就有一个转换的过程，这就可以通过映射来实现。我们通过自定义注解方式实现了框架未提供的功能，例如文件上传和下载、自定义参数注入、参数校验、自定义鉴权等。\n功能扩展的第二类就是接口编排，已经实现的单接口泛化调用，不能完全满足我们一些复杂场景的使用需求，例如：简单组装两个接口的结果，比如同时调用接口 A 和 B，再将两个接口进行组装； 接口有顺序依赖，一个接口结果是其他接口的参数，比如先调用 A，A 的返回值作为参数去调用 B，再将 B 的返回值作为整体接口的返回值。 Kitex 和 Hertz 还不能支持接口编排的功能，所以我们通过自定义 DSL 引擎来对简单接口进行编排，以便实现一些复杂场景的接口调用需求。\n成果 最后给大家介绍一下我们的演进成果，主要有以下三点：\n 业务迭代加速。Admin 不再关注其他业务线的需求，更加专注于自身的迭代需求。各个业务方发布完全隔离，使得他们不再依赖 Admin，加快了 Admin 整体的业务功能迭代速度。 研发效率提升。丰富的前后端组件和简单的接入方式，业务方不需要再花费时间熟悉我们的代码仓库，使得业务方接入更加便捷，研发效率大大提升。 工程质量提高。  其他团队不再向 Admin 仓库提交代码，仓库代码风格趋向统一； 去除了大量的业务逻辑，聚焦网关通用逻辑，提高了单测覆盖率； Bug 率显著下降，服务 SLA 明显提升。    未来规划 我们目前制定了一些未来的发展规划，主要有以下四点：\n 开放更多的组件，让接入的业务方聚焦在业务逻辑本身，例如组织管理里面的选人组件，之前需要各个业务方自己内部实现，之后我们会提供一套公共组件，业务方可以直接使用，包括消息中心、任务管理、安全风控、短信邮件等； 完善服务治理和运维能力，包括灰度、降级、限流、精细化大盘等； 建设通用的静态页面托管解决方案，为开发者提供便捷、稳定、高扩展性的静态页面托管服务； 对接集成测试平台，闭环路由管理生命周期，保障接口稳定性和安全性。  ","categories":"","description":"","excerpt":"案例介绍   飞书管理后台是飞书套件专为企业管理员提供的信息管理平台，在单体应用架构下，它面临了一系列的挑战。 它通过引入 Kitex 泛化 …","ref":"/cooperation/feishu/","tags":"","title":"引入 CloudWeGo 后飞书管理后台平台化改造的演进史"},{"body":"By default, Hertz integrates with and uses Sonic for serializing ctx.JSON interface and deserialization requests as defined in the binding package. Sonic is an ultra-high performance golang json library, also see Sonic README for details.\nThe following are requirements to enable Sonic:\n Go 1.15/1.16/1.17/1.18 Linux / darwin OS / Windows Amd64 CPU with AVX instruction set  Sonic automatically fallback to golang’s encoding/json library when the above requirements have not been satisfied.\nCompatibility With encoding/json Currently, Hertz uses the default configuration for Sonic (i.e.sonic.ConfigDefault), which behaves different from JSON encoding/json. Specifically, by default, Sonic are configured to:\n disable html escape: Sonic will not escape HTML’s special characters disable key-sort by default: Sonic will not sort json in lexicographical order  To find more about the compatibility with encoding/json, you may want to see sonic#Compatibility. You may change Sonic’s behavior (e.g. behaving exactly the same way as encoding/json) by calling ResetJSONMarshaler for render.\nrender.ResetJSONMarshaler(sonic.ConfigStd.Marshal) Bringing Your Own JSON Marshal Library If Sonic does not meet your needs, you may provide your own implementation by calling ResetJSONMarshal for render and ResetJSONUnmarshaler for binding.\nimport ( \"encoding/json\" \"github.com/bytedance/go-tagexpr/v2/binding\" \"github.com/cloudwego/hertz/pkg/app/server/render\" ) func main() { // Render  render.ResetJSONMarshal(json.Marshal) // Binding  binding.ResetJSONUnmarshaler(json.Unmarshal) } Common FAQs Compilation Error on Mac M1 Unsupported CPU, maybe it’s too old to run Sonic In most cases, this is because the go binary and/or build configuration is not consistent with ARM arch.\n Go binary is not built for ARM64. Please use a go binary compiled for ARM64. You may encounter issues on go1.16 as compiler incorrectly links x86 files due to official bugs. Therefore, go1.17 or above is highly recommended. GOARCH is set to amd64 i.e. GOARCH=amd64. You can either remove the flag or set its value to arm64. Running go binary compiled for x86 with a translator (e.g. Rosetta). This is not supported yet.  Build constraints exclude all Go files in xxx This is mostly because Sonic does not work on your go version. See sonic#Requirement for a list of supported go versions.\n","categories":"","description":"","excerpt":"By default, Hertz integrates with and uses Sonic for serializing …","ref":"/docs/hertz/reference/json/","tags":"","title":"JSON Marshal Library"},{"body":"Hertz 默认集成并使用 Sonic 用于序列化ctx.JSON接口，以及反序列化binding包中的请求。Sonic 是一款超高性能 golang json 库，详情参考 Sonic README 。\n开启 Sonic 需要满足以下条件：\n Go 1.15/1.16/1.17/1.18 Linux / darwin OS / Windows Amd64 CPU with AVX instruction set  当上述条件不能满足时，Sonic 会自动 fallback 到 golang 的 encoding/json 库。\n与 encoding/json 兼容性 当前 hertz 使用Sonic的默认配置（即sonic.ConfigDefault），行为与标准库 encoding/json 有所差异，详见 sonic#Compatibility\n具体来说，默认情况下，Sonic：\n 禁用 html escape：Sonic 不会转义 HTML中的特殊字符 禁用 key-sort：Sonic 不会按照键对JSON排序  你可以通过调用 render 包中的ResetJSONMarshaler函数来修改Sonic的行为，比如保持和标准库兼容。\nrender.ResetJSONMarshaler(sonic.ConfigStd.Marshal) 自定义 JSON Marshall 库 如果 Sonic 不能够满足您的需求，你可以使用以下方式自定义 json marshal 库的实现:\nimport ( \"encoding/json\" \"github.com/bytedance/go-tagexpr/v2/binding\" \"github.com/cloudwego/hertz/pkg/app/server/render\" ) func main() { // Render render.ResetJSONMarshal(json.Marshal) // Binding  binding.ResetJSONUnmarshaler(json.Unmarshal) } 常见问题 Mac M1 上编译报错 Unsupported CPU, maybe it’s too old to run Sonic 一般为是因为 Go 镜像版本或构建参数和ARM架构不一致。\n 安装了非 arm 版本的 go 镜像。请安装 arm 版本 Go 镜像（go1.16某些 arm 镜像存在 bug 会导致 link 错误的 x86 文件，推荐 go1.17 以上版本） 设置GOARCH参数为amd64（即GOARCH=amd64）。请去除该参数或设置为 `arm64`` 使用了转译器（如Rosetta）运行 x86 环境下编译出来的程序。目前不支持这种使用方式  Build constraints exclude all Go files in xxx 一般是 Go 版本导致的问题，sonic 目前支持的版本见 sonic#Requirement\n","categories":"","description":"","excerpt":"Hertz 默认集成并使用 Sonic 用于序列化ctx.JSON接口，以及反序列化binding包中的请求。Sonic …","ref":"/zh/docs/hertz/reference/json/","tags":"","title":"JSON Marshal 库"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/releases/netpoll/","tags":"","title":"Netpoll Release"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/blog/releases/netpoll/","tags":"","title":"Netpoll Release"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/hertz/tutorials/service-governance/","tags":"","title":"Service Governance"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/hertz/tutorials/","tags":"","title":"Tutorials"},{"body":"Kitex complies with Semantic Versioning 2.0.0 release version.\n  Major Version: when Kitex provides incompatible API\n  Minor Version: when Kitex provides new features in a backward compatible manner\n  Patch Version: when Kitex makes backward compatible changes includes small features or makes bug fixes\n  ","categories":"","description":"","excerpt":"Kitex complies with Semantic Versioning 2.0.0 release version.\n  Major …","ref":"/docs/kitex/reference/version/","tags":"","title":"Version Specification"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/volo/","tags":"","title":"Volo"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/volo/","tags":"","title":"Volo"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/volo/volo-grpc/","tags":"","title":"Volo-gRPC"},{"body":"1. 如何配置 poller 的数量 ？ NumLoops 表示 [Netpoll][Netpoll] 创建的 epoll 的数量，默认已经根据P的数量自动调整(runtime.GOMAXPROCS(0))，用户一般不需要关心。\n但是如果你的服务有大量的 I/O，你可能需要如下配置：\npackage main import ( \"runtime\" \"github.com/cloudwego/netpoll\" ) func init() { netpoll.SetNumLoops(runtime.GOMAXPROCS(0)) } 2. 如何配置 poller 的连接负载均衡 ？ 当 Netpoll 中有多个 poller 时，服务进程中的连接会负载均衡到每个 poller。\n现在支持以下策略：\n Random  新连接将分配给随机选择的轮询器。   RoundRobin  新连接将按顺序分配给轮询器。    Netpoll 默认使用 RoundRobin，用户可以通过以下方式更改：\npackage main import ( \"github.com/cloudwego/netpoll\" ) func init() { netpoll.SetLoadBalance(netpoll.Random) // or \tnetpoll.SetLoadBalance(netpoll.RoundRobin) } 3. 如何配置 gopool ？ Netpoll 默认使用 gopool 作为 goroutine 池来优化 栈扩张 问题（RPC 服务常见问题）。\ngopool 项目中已经详细解释了如何自定义配置，这里不再赘述。\n当然，如果你的项目没有 栈扩张 问题，建议最好关闭 gopool，关闭方式如下：\npackage main import ( \"github.com/cloudwego/netpoll\" ) func init() { netpoll.DisableGopool() } 4. 如何初始化新的连接 ？ Client 和 Server 端通过不同的方式初始化新连接。\n 在 Server 端，定义了 OnPrepare 来初始化新链接，同时支持返回一个 context，可以传递给后续的业务处理并复用。WithOnPrepare 提供方法注册。当 Server 接收新连接时，会自动执行注册的 OnPrepare 方法来完成准备工作。示例如下：  package main import ( \"context\" \"github.com/cloudwego/netpoll\" ) func main() { // register OnPrepare \tvar onPrepare netpoll.OnPrepare = prepare evl, _ := netpoll.NewEventLoop(handler, netpoll.WithOnPrepare(onPrepare)) ... } func prepare(connection netpoll.Connection) (ctx context.Context) { ... prepare connection ... return } 在 Client 端，连接初始化需要由用户自行完成。 一般来说，Dialer 创建的新连接是可以由用户自行控制的，这与 Server 端被动接收连接不同。因此，用户不需要依赖触发器，可以自行初始化，如下所示：  package main import ( \"context\" \"github.com/cloudwego/netpoll\" ) func main() { conn, err := netpoll.DialConnection(network, address, timeout) if err != nil { panic(\"dial netpoll connection failed\") } ... prepare here directly ... prepare(conn) ... } func prepare(connection netpoll.Connection) (ctx context.Context) { ... prepare connection ... return } 5. 如何配置连接超时 ？ Netpoll 现在支持两种类型的超时配置：\n 读超时（ReadTimeout）  为了保持与 net.Conn 相同的操作风格，Connection.Reader 也被设计为阻塞读取。 所以提供了读取超时（ReadTimeout）。 读超时（ReadTimeout）没有默认值（默认无限等待），可以通过 Connection 或 EventLoop.Option 进行配置，例如：    package main import ( \"github.com/cloudwego/netpoll\" ) func main() { var conn netpoll.Connection // 1. setting by Connection \tconn.SetReadTimeout(timeout) // or  // 2. setting with Option \tnetpoll.NewEventLoop(handler, netpoll.WithReadTimeout(timeout)) ... } 空闲超时（IdleTimeout）  空闲超时（IdleTimeout）利用 TCP KeepAlive 机制来踢出死连接并减少维护开销。使用 Netpoll 时，一般不需要频繁创建和关闭连接，所以通常来说，空闲连接影响不大。当连接长时间处于非活动状态时，为了防止出现假死、对端挂起、异常断开等造成的死连接，在空闲超时（IdleTimeout）后，netpoll 会主动关闭连接。 空闲超时（IdleTimeout）的默认配置为 10min，可以通过 Connection API 或 EventLoop.Option 进行配置，例如：    package main import ( \"github.com/cloudwego/netpoll\" ) func main() { var conn netpoll.Connection // 1. setting by Connection \tconn.SetIdleTimeout(timeout) // or  // 2. setting with Option \tnetpoll.NewEventLoop(handler, netpoll.WithIdleTimeout(timeout)) ... } 6. 如何配置连接的读事件回调 ？ OnRequest 是指连接上发生读事件时 Netpoll 触发的回调。在 Server 端，在创建 EventLoop 时，可以注册一个OnRequest，在每次连接数据到达时触发，进行业务处理。Client端默认没有 OnRequest，需要时可以通过 API 设置。例如：\npackage main import ( \"context\" \"github.com/cloudwego/netpoll\" ) func main() { var onRequest netpoll.OnRequest = handler // 1. on server side \tevl, _ := netpoll.NewEventLoop(onRequest, opts...) ... // 2. on client side \tconn, _ := netpoll.DialConnection(network, address, timeout) conn.SetOnRequest(handler) ... } func handler(ctx context.Context, connection netpoll.Connection) (err error) { ... handling ... return nil } 7. 如何配置连接的关闭回调 ？ CloseCallback 是指连接关闭时 Netpoll 触发的回调，用于在连接关闭后进行额外的处理。 Netpoll 能够感知连接状态。当连接被对端关闭或被自己清理时，会主动触发 CloseCallback，而不是由下一次调用 Read 或 Write 时返回错误（net.Conn 的方式）。 Connection 提供了添加 CloseCallback 的 API，已经添加的回调无法删除，支持多个回调。\npackage main import ( \"github.com/cloudwego/netpoll\" ) func main() { var conn netpoll.Connection // add close callback \tvar cb netpoll.CloseCallback = callback conn.AddCloseCallback(cb) ... } func callback(connection netpoll.Connection) error { return nil } ","categories":"","description":"","excerpt":"1. 如何配置 poller 的数量 ？ NumLoops 表示 [Netpoll][Netpoll] 创建的 epoll 的数量，默认已经 …","ref":"/zh/docs/netpoll/common-usage/","tags":"","title":"常见用法"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/hertz/tutorials/","tags":"","title":"指南"},{"body":"对于 RPC 框架来说，日志、监控和 trace 是很重要的组成部分，云原生环境下可观测性基本依赖这三件套。\nVolo 框架使用的是 tracing 库来记录 Volo 自己的日志，同时也鼓励用户使用 tracing 库来输出日志和 trace 信息，这样就可以直接复用 Rust 社区现有的相关生态了，比如 tracing-opentelemetry 等库。\n用户也可以通过编写自己的 Service 和 Layer 来给所有的请求加上日志信息或者监控打点信息，如：\npubstruct ClientLogLayer;impl\u003cS\u003eLayer\u003cS\u003eforClientLogLayer{type Service=LogService\u003cS\u003e;fn layer(self,inner: S)-\u003e Self::Service{LogService{inner,}}}#[derive(Clone)]pubstruct LogService\u003cS\u003e{inner: S,}impl\u003cCx,Req,S\u003eService\u003cCx,Req\u003eforLogService\u003cS\u003ewhereS: Service\u003cCx,Req\u003e+Send+'static,Cx: Context\u003cConfig=volo_thrift::context::Config\u003e+'static+Send,Req: Send +'static,{type Response=S::Response;type Error=S::Error;type Future\u003c'cx\u003e=implFuture\u003cOutput=Result\u003cSelf::Response,Self::Error\u003e\u003e+'cx;fn call\u003c'cx,'s\u003e(\u0026'smutself,cx: \u0026'cxmutCx,req: Req)-\u003e Self::Future\u003c'cx\u003ewhere's: 'cx,{asyncmove{lettick=quanta::Instant::now();letret=self.inner.call(cx,req).await;letelapsed=quanta::Instant::now().duration_since(tick);tracing::info!(rpc_type=\"rpcCall\",cost=elapsed.as_micros()asi64,);ret}}}监控打点信息也是类似。\n","categories":"","description":"","excerpt":"对于 RPC 框架来说，日志、监控和 trace 是很重要的组成部分，云原生环境下可观测性基本依赖这三件套。\nVolo …","ref":"/zh/docs/volo/guide/observability/","tags":"","title":"自定义日志/监控打点/trace"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/hertz/tutorials/service-governance/","tags":"","title":"治理特性"},{"body":"Kitex 遵从语义化版本 2.0.0 发布版本。\n  主版本号：Kitex 提供的 API 出现不兼容的情况时，升级该版本号\n  次版本号：Kitex 提供新的功能特性同时保持向下兼容时，升级该版本号\n  修订号：Kitex 的代码提供小的特性或向下兼容的优化和问题修复时，升级该版本号\n  ","categories":"","description":"","excerpt":"Kitex 遵从语义化版本 2.0.0 发布版本。\n  主版本号：Kitex 提供的 API 出现不兼容的情况时，升级该版本号\n  次版本 …","ref":"/zh/docs/kitex/reference/version/","tags":"","title":"版本说明"},{"body":"上一节中，我们编写完成了 server 端，现在让我们来编写我们的 client 端并调用我们的 server 端。\n首先，创建一个文件 src/bin/client.rs，输入以下内容：\nuselazy_static::lazy_static;usestd::net::SocketAddr;lazy_static!{staticrefCLIENT: volo_gen::volo::example::ItemServiceClient={letaddr: SocketAddr=\"127.0.0.1:8080\".parse().unwrap();volo_gen::volo::example::ItemServiceClientBuilder::new(\"volo-example\").target(addr).build()};}#[volo::main]asyncfn main(){letreq=volo_gen::volo::example::GetItemRequest{id: 1024};letresp=CLIENT.clone().get_item(req).await;matchresp{Ok(info)=\u003etracing::info!(\"{:?}\",info),Err(e)=\u003etracing::error!(\"{:?}\",e),}}然后，在 Cargo.toml 文件中加入所需的依赖，加入后的文件如下：\n[package] name = \"volo-example\" version = \"0.1.0\" edition = \"2021\" # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html [dependencies] anyhow = \"1\" async-trait = \"0.1\" lazy_static = \"1\" tokio = { version = \"1\", features = [\"full\"] } tracing = \"0.1\" prost = \"0.11\" pilota = \"*\" volo = \"*\" # we recommend to use the latest framework version for new features and bug fixes volo-grpc = \"*\" # we recommend to use the latest framework version for new features and bug fixes volo-gen = { path = \"./volo-gen\" } [profile.release] opt-level = 3 debug = true debug-assertions = false overflow-checks = false lto = true panic = 'unwind' incremental = false codegen-units = 1 rpath = false [workspace] members = [\"volo-gen\"] resolver = \"2\" 接着，新建一个 terminal，执行以下命令，把我们的 server 端跑起来：\n$ cargo run --bin server 最后，我们再回到当前目录，执行以下命令，即可看到执行成功（这时候没有日志输出，因为我们没有给 tracing 设置 subscriber，大家可以自行选择对应的库）：\n$ cargo run --bin client 大功告成！\n","categories":"","description":"","excerpt":"上一节中，我们编写完成了 server 端，现在让我们来编写我们的 client 端并调用我们的 server 端。\n首先， …","ref":"/zh/docs/volo/volo-grpc/getting-started/part_3/","tags":"","title":"Part 3. 编写 Client 端"},{"body":"上一节中，我们编写完成了 server 端，现在让我们来编写我们的 client 端并调用我们的 server 端。\n首先，创建一个文件 src/bin/client.rs，输入以下内容：\nuselazy_static::lazy_static;usestd::net::SocketAddr;lazy_static!{staticrefCLIENT: volo_gen::volo::example::ItemServiceClient={letaddr: SocketAddr=\"127.0.0.1:8080\".parse().unwrap();volo_gen::volo::example::ItemServiceClientBuilder::new(\"volo-example\").address(addr).build()};}#[volo::main]asyncfn main(){letreq=volo_gen::volo::example::GetItemRequest{id: 1024};letresp=CLIENT.clone().get_item(req).await;matchresp{Ok(info)=\u003etracing::info!(\"{:?}\",info),Err(e)=\u003etracing::error!(\"{:?}\",e),}}然后，在 Cargo.toml 文件中加入所需的依赖，加入后的文件如下：\n[package] name = \"volo-example\" version = \"0.1.0\" edition = \"2021\" # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html [dependencies] anyhow = \"1\" async-trait = \"0.1\" lazy_static = \"1\" tokio = { version = \"1\", features = [\"full\"] } tracing = \"0.1\" pilota = \"*\" volo = \"*\" # we recommend to use the latest framework version for new features and bug fixes volo-thrift = \"*\" # we recommend to use the latest framework version for new features and bug fixes volo-gen = { path = \"./volo-gen\" } [profile.release] opt-level = 3 debug = true debug-assertions = false overflow-checks = false lto = true panic = 'unwind' incremental = false codegen-units = 1 rpath = false [workspace] members = [\"volo-gen\"] resolver = \"2\" 接着，新建一个 terminal，执行以下命令，把我们的 server 端跑起来：\n$ cargo run --bin server 最后，我们再回到当前目录，执行以下命令，即可看到执行成功（这时候没有日志输出，因为我们没有给 tracing 设置 subscriber，大家可以自行选择对应的库）：\n$ cargo run --bin client 大功告成！\n","categories":"","description":"","excerpt":"上一节中，我们编写完成了 server 端，现在让我们来编写我们的 client 端并调用我们的 server 端。\n首先， …","ref":"/zh/docs/volo/volo-thrift/getting-started/part_3/","tags":"","title":"Part 3. 编写 Client 端"},{"body":"案例介绍   飞书管理后台是飞书套件专为企业管理员提供的信息管理平台，在单体应用架构下，它面临了一系列的挑战。 它通过引入 Kitex 泛化调用对飞书管理后台进行平台化改造，使之变为业务网关，提供一套统一的标准和通用服务，让有管控诉求的套件业务方能快速实现能力集成，并且提供一致的体验。最终实现了飞书管理后台作为企业统一数字化管理平台的愿景。\n本文将从三个方面为大家讲解 Kitex 泛化调用在飞书管理后台平台化改造过程中的落地实践：\n 架构和挑战，即飞书管理后台单体架构面临的各种挑战； 平台化构想，即飞书管理后台平台化构想和架构升级； 平台化实现，包括微前端技术架构、泛化调用实践和功能扩展。  架构和挑战 飞书是真正的一站式企业沟通与协作平台，整合视频会议、即时消息、日历、云文档、邮箱、工作台等功能于一体，立志打造高效的办公方式，加速企业成长。 飞书管理后台（以下简称 Admin）是飞书套件专为企业管理员提供的信息管理平台，企业管理员可通过后台管理企业设置、组织架构、工作台和会议室等功能。下图是飞书管理后台的界面。\n平台改造背景 飞书采用的是 all-in-one 的套件模式，Admin 作为整个套件统一的管理后台，承接了包括组织管理、云文档、视频会议、邮箱、开放平台等 10 多个业务线的管控需求。 一直以来的开发模式是各业务方直接在 Admin 的代码仓库提交代码或者由 Admin 团队负责 Web 层逻辑的开发。下图是目前飞书管理后台中包括的一些功能， 可以看到功能种类还是非常多的，之前的开发模式是业务方直接在 Admin 的代码仓库中提交代码，或者由业务方给 Admin 团队提供一些需求，由我们来负责 Web 层逻辑的开发。 从飞书初创开始，Admin 就是以单体应用的模式开发的，随着后续飞书整体的演进，我们的团队越来越多，不同业务线的团队也会有一些管控需求要接入 Admin 平台，因此他们就直接在代码仓库中提交代码。\nAdmin 架构 下图是 Admin 旧架构图。上面是 Admin 前端，它其实就是 Node 层的单体，中间是 Admin 后端，它基于我们内部单体 HTTP 的 Web 服务，会通过 RPC 调用到其他业务线的微服务。\n面临的问题 在这个架构下我们会面临一些问题，第一个问题是业务迭代慢，因为所有业务线都只能在 Admin 的代码仓库里进行开发和发版，因此这些业务线完全依赖 Admin 的研发资源和迭代流程， Admin 的研发资源被过多的耗在各业务的迭代中，无法快速支持自身的业务规划，如组织架构、安全、KA等因为我们是 To B 的产品，因此发版节奏不会很快。 如果各个业务线有一些比较紧急的需求，也只能跟 Admin 的节奏，这就会造成发版节奏不一致，研发资源不匹配，导致Admin会成为业务迭代的瓶颈。 第二个问题是研发效率低，因为各个业务线需要在 Admin 的代码仓库里进行修改，因此需要了解我们仓库的设计模式，我们在为各个业务线提供服务时，也需要了解各业务的上下文，双方都需要花费大量时间沟通。 联调、Oncall的链路也很长，双方的责任也划不清楚，导致整体的研发效率偏低。第三个问题是工程质量差，多个团队共同维护一个代码仓库，代码质量参差不齐，设计规范也各不相同，底层的代码的修改还会相互影响，造成线上问题。\n面临的挑战 此外，我们还会面临很多挑战。首先面临的是多环境互通与隔离的问题，我们需要解决不同环境的网络隔离、版本异构问题；其次是接入业务复杂性，Admin需要集成十几个业务线，接入诉求不统一。 接口协议包含 HTTP 协议和 Thrift 协议，还有各种自定义插件需求和权限校验需求；最后还有安全保障，Admin 作为飞书套件的管理配置中心，关系到整个企业的数据安全。 安全一直是 Admin 最重要的需求，为了保障Admin 的接口数据安全，需要提供鉴权中间件、管理员权限验证、参数校验、风控、频控等功能，提升业务方的安全能力。\n平台化构想 第二部分给大家介绍飞书平台化构想，即如何进行飞书管理后台平台化构想和架构升级。\n首先要明确的是目标，主要目标是通过提供一套统一的标准和通用服务，让有管控诉求的套件业务方能快速实现能力集成，并且能给客户带来一致的体验。 因为我们各个业务线的管控需求都是集成在 Admin，我们不希望每个业务线提供的 Web 页面展示、功能和 UI 等差别较大，希望他们是相对统一的，最终实现Admin作为企业统一数字化管理平台的愿景。 关于在技术上需要达到的效果，我们希望业务方不要继续在 Admin 代码仓库中进行代码开发，而是直接提供我们的后端接口和前端页面动态接入，Admin 无需代码改造和服务发布即可无缝上线，Admin 从单体应用进化为业务网关，是包含 UI 交互在内的独立产品模块的集成。\n我们并不是要做一个搭建系统。目前很多平台型产品都提供 Low Code / No Code 的工具方便开发者快速搭建所需要的功能。 但是目前通过我们对客户诉求的调研，没有相关的需求（但是不代表未来也没有，这块我们会持续保持关注）。 我们需要做的是制定相关的标准，比如 UI、交互、API 等，业务按照标准去实现。我们也不是要做一个 API Gateway 或者 Service Mesh。 API Gateway 的核心是Exposes your services as managed APIs，将内部的服务以更加可控可管理的方式暴露出去，可以认为是后端服务的一个代理。 Service Mesh 可以看成是 API Gateway 的去中心化实现方式，用来解决单点、隔离、耦合等问题。我们需要解决的不仅仅是服务路由、协议转换、安全管控等问题，而是包含 UI 交互在内的独立产品模块的集成。\n旧的框架 这是我们旧的架构。它的前端架构是前后端分离的 Node 单体项目。后端架构（Golang 实现）采用 Hertz 框架对前端暴露 HTTP 接口，Handler 层通过 Kitex 调用依赖的各个业务线的微服务。\n框架介绍 下面介绍一下 CloudWeGo 现有的两款框架。首先是 Hertz 框架，Hertz [həːts] 是一个 Golang 微服务 HTTP 框架，在设计之初参考了其他开源框架 Fasthttp、Gin、Echo 的优势，并结合字节跳动内部的需求，使其具有高易用性、高性能、高扩展性等特点，目前在字节跳动内部已广泛使用。如今越来越多的微服务选择使用 Golang，如果对微服务性能有要求，又希望框架能够充分满足内部的可定制化需求，Hertz 会是一个不错的选择。Kitex [kaɪt’eks] 字节跳动内部的 Golang 微服务 RPC 框架，具有高性能、强可扩展的特点，在字节内部已广泛使用。如果对微服务性能有要求，又希望定制扩展融入自己的治理体系，可以考虑选择 Kitex。\n新的框架 下面介绍一下我们的新架构，它主要包括：\n Gaia 控制面。我们增加了 Gaia 平台（基于 Hertz 框架的 Web 服务）来作为我们整个 Admin 的控制系统，负责整体的发布和管控需求，包括接口的生命周期管理、微应用生命周期管理、监控告警、业务线接入、多环境发布等。 前端架构。前端采用微前端架构，各个业务方通过构建微应用接入 Admin 基座，使用统一封装好的组件库实现前端页面。 后端架构。后端使用字节通用 BAM 规范，通过泛化调用的方式打通 Admin 和各接入业务方服务，并抽象公共组件以插件的方式进行功能扩展。  Admin 架构 下图就是新的 Admin 架构图。左上方是微前端，它包含前端里面各个业务线的微应用。微前端通过 HTTP 接口和 Admin 网关进行交互，Admin 网关把业务逻辑都剥离到下一层， 而自身只负责公共组件、登录鉴权、协议代理和通用配置等通用需求，同时它会通过泛化调用来调用下游的业务服务。业务服务包括组织管理、云文档、视频会议和邮箱等微服务。 右侧是 Gaia 控制面，包括一些管控功能，如接口生命周期管理、监控大盘、微应用管理、工单系统等等。另外如果我们有一些独立的自定义功能，会通过插件的方式集成。\nGaia 平台功能 Gaia 平台主要包括以下功能：\n 业务线管理。业务线是实现以业务为维度进行接入 Admin 而提出的概念。通过业务线来聚合业务为维度的所有资源，相关资源包括微应用、菜单、接口、监控等。图中就是业务线管理的菜单页面。  接口生命周期管理。包括接口创建、更新、编排、发布、上线、下线、删除等。同时维护接口 IDL 文件。 微应用生命周期管理。包括微应用的申请、接入、微应用版本创建、发布、下线等。 控制大盘。包括业务整体维度和单接口维度的 SLA 大盘，以及错误告警管理。 插件管理。包括默认插件和自定义插件的配置管理。  平台化实现 微前端技术架构 第三部分具体介绍飞书平台化实现，包括微前端技术架构、泛化调用实践和功能扩展。\n下图是微前端的技术架构。这里涉及到三个概念，第一个是基座，即指微前端入口模块，负责组装各个模块；第二个是微应用，指独立的业务模块；第三个是微应用市场，负责管理微应用的创建，管理，版本发布等。 通过微应用市场下发的配置进行微应用组合，将基础能力下放到各个业务方。例如，现有一个新的业务线需要接入，那么它需要开发自己的微应用，打包测试并发布到我们的微应用市场，我们的基座就会从微应用市场接收到这个微应用，最后进行发布之后，就可以从 Web 看到对应模块的页面。\n泛化调用方案调研 接下来说一下后端实现的细节，即如何通过泛化调用实现整体依赖的剥离？首先讲一下这个问题的背景，Admin 的前端和后端是通过 JSON实现序列化传递的， 如果把 Admin 变成一个平台化的网关，不再维护业务逻辑，只处理通用逻辑，泛化调用是我们最好的选择。因为通过泛化调用，Admin 的网关就不需要写各种业务代码， 直接通过 RPC 接口就可以把前端传过来的 JSON 序列化数据、请求参数再传递到微服务，然后通过微服务的返回值把 JSON 序列化数据返回前端，跟前端进行交互。\n我们通过调研现有框架，如网关与微服务之间使用 gRPC、Thrift 等协议进行通信，都是通过代码生成实现的协议解析和协议传输，不能动态更新，都需要生成代码，再重新发布。 而我们内部旧的 Kite 框架（Thrift 协议）不支持泛化调用，而新框架 Kitex 是字节跳动内部的 Golang 微服务 RPC 框架，具有高性能、强可扩展的特点。 在我们使用 Kitex 的泛化调用功能之前曾调研了一些泛化调用的方案，也基于 Kitex 实现了泛化调用的类似功能。但是我们认为飞书内部实现泛化调用不如推动 Kitex 的研发人员， 让他们把泛化调用变为一个通用的功能，这样不仅仅是我们团队，公司内部其余团队以及 Kitex 开源后其他外部团队都可以使用这个功能。目前 Kitex 已经支持基于 Thrift 协议的泛化调用。\n非泛化调用 那么非泛化调用的实现方式和泛化调用的实现方式有什么不同呢？这张图就是非泛化调用的实现方式，无论 gRPC、Thrift 还是 Kitex 都是基于 IDL 生成协议代码， 服务端和客户端都需要依赖 IDL 生成静态代码，接口的迭代意味着服务端和客户端都需要升级代码重新发布。在 Admin 场景下意味着其他业务方的业务迭代， 需要我们引入代码依赖并发布服务，这并不符合我们平台化的需求。\nKitex 泛化调用 在 Kitex 泛化调用中，服务端无需做任何改造。客户端只有一份通用的协议处理代码，基于已有的 IDL 信息来动态生成协议字节流，IDL 信息可以动态更新，以维护最新的接口协议，无需生成代码。 在 Admin场景下，网关作为客户端，动态维护业务方接口的 IDL，通过泛化调用来实现 HTTP 接口到 RPC 接口的转换，不再依赖业务服务客户端代码，实现了网关和业务在代码层面的解耦。\n相关地址：https://github.com/cloudwego/kitex/tree/develop/pkg/generic/thrift\nHTTP 协议映射 Admin 网关是基于 Hertz 对外暴露 HTTP 协议的接口，Hertz 路由支持运行时新增，通过自定义 Middleware 和 HandlerFunc 可以实现接口运行时的增删改，这样可以实现解析修改后的 IDL 来进行接口调用。 这段代码就是初始化客户端的 Client，其实就是泛化调用的 Client，可以看到它会读业务方的 IDL，假如业务方的 IDL 有接口更新，我们可以通过这个进行业务更新，动态实现接口的上下线。 然后再构造 HTTP 类型的泛化调用 Client，每个业务方都会构造一个 Client 实例，比如有十几个业务线，就会生成十几个业务线微服务的实例。\n下面是泛化调用的路由，它其实是 HandlerFunc 的实现，通过这个方式可以动态注册路由，注册之后将 Hertz Request 转化成泛化调用 Request， 再通过前一步生成的泛化调用 Client 实现泛化调用，最后得到 HTTPResponse，再将它写回 Hertz Response中，这就是简单的泛化调用路由的实现。通过这个可以做很多业务拓展，比如错误码处理等等。\n功能扩展 具体给大家讲一下功能扩展。功能扩展的第一类就是 Kitex 提供的自定义注解，Kitex 内置了 API 注解来实现路由解析、参数传递等功能。 这里面有三个接口，第一个是 HTTPMapping，实现了参数传递、返回值等等自定义注解；第二个是 Route，实现了 Kitex 路由解析的功能； 第三个是 ValueMapping，是指将参数进行映射，比如目前 JSON 不支持 Int64，但 Go 可以支持 Int64，在使用 JSON 序列化的时候就要把参数类型定义为 String， 因此从 JSON 到 Go 就有一个转换的过程，这就可以通过映射来实现。我们通过自定义注解方式实现了框架未提供的功能，例如文件上传和下载、自定义参数注入、参数校验、自定义鉴权等。\n功能扩展的第二类就是接口编排，已经实现的单接口泛化调用，不能完全满足我们一些复杂场景的使用需求，例如：简单组装两个接口的结果，比如同时调用接口 A 和 B，再将两个接口进行组装； 接口有顺序依赖，一个接口结果是其他接口的参数，比如先调用 A，A 的返回值作为参数去调用 B，再将 B 的返回值作为整体接口的返回值。 Kitex 和 Hertz 还不能支持接口编排的功能，所以我们通过自定义 DSL 引擎来对简单接口进行编排，以便实现一些复杂场景的接口调用需求。\n成果 最后给大家介绍一下我们的演进成果，主要有以下三点：\n 业务迭代加速。Admin 不再关注其他业务线的需求，更加专注于自身的迭代需求。各个业务方发布完全隔离，使得他们不再依赖 Admin，加快了 Admin 整体的业务功能迭代速度。 研发效率提升。丰富的前后端组件和简单的接入方式，业务方不需要再花费时间熟悉我们的代码仓库，使得业务方接入更加便捷，研发效率大大提升。 工程质量提高。  其他团队不再向 Admin 仓库提交代码，仓库代码风格趋向统一； 去除了大量的业务逻辑，聚焦网关通用逻辑，提高了单测覆盖率； Bug 率显著下降，服务 SLA 明显提升。    未来规划 我们目前制定了一些未来的发展规划，主要有以下四点：\n 开放更多的组件，让接入的业务方聚焦在业务逻辑本身，例如组织管理里面的选人组件，之前需要各个业务方自己内部实现，之后我们会提供一套公共组件，业务方可以直接使用，包括消息中心、任务管理、安全风控、短信邮件等； 完善服务治理和运维能力，包括灰度、降级、限流、精细化大盘等； 建设通用的静态页面托管解决方案，为开发者提供便捷、稳定、高扩展性的静态页面托管服务； 对接集成测试平台，闭环路由管理生命周期，保障接口稳定性和安全性。  ","categories":"","description":"","excerpt":"案例介绍   飞书管理后台是飞书套件专为企业管理员提供的信息管理平台，在单体应用架构下，它面临了一系列的挑战。 它通过引入 Kitex 泛化 …","ref":"/zh/cooperation/feishu/","tags":"","title":"引入 CloudWeGo 后飞书管理后台平台化改造的演进史"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/hertz/tutorials/framework-exten/advanced-exten/","tags":"","title":"高级扩展"},{"body":"在 HTTP 中，GNUzip(Gzip) 压缩编码是一种用来优化 Web 应用程序性能的方式，并且 Hertz 也提供了 Gzip 的实现 。\n使用方法可参考如下 example\npackage main import ( \"context\" \"fmt\" \"net/http\" \"time\" \"github.com/cloudwego/hertz/pkg/app\" \"github.com/cloudwego/hertz/pkg/app/server\" \"github.com/hertz-contrib/gzip\" ) func main() { h := server.Default(server.WithHostPorts(\":8080\")) h.Use(gzip.Gzip(gzip.DefaultCompression)) h.GET(\"/ping\", func(ctx context.Context, c *app.RequestContext) { c.String(http.StatusOK, \"pong \"+fmt.Sprint(time.Now().Unix())) }) h.Spin() } 更多用法示例详见 gzip\n","categories":"","description":"","excerpt":"在 HTTP 中，GNUzip(Gzip) 压缩编码是一种用来优化 Web 应用程序性能的方式，并且 Hertz 也提供了 Gzip …","ref":"/zh/docs/hertz/tutorials/basic-feature/middleware/gzip/","tags":"","title":"Gzip 压缩"},{"body":"Version Requirements kitex version \u003e= v0.4.0\nOverview Fastpb is a protobuf enhancement plugin developed by ByteDance. It uses the new generated code and API to complete the protobuf encoding and decoding process. Compared to the official sdk, it avoids go-reflect and thus has better performance.\nPerformance comparison with the official protobuf please refer to here. More Fastpb details see here.\nUsage (enabled by default) Kitex integrates Fastpb by default. When using the kitex command-tool to generate code, an additional xx.pb.fast.go file will be added next to the official generated code file xx.pb.go for Fastpb faster codec.\nHow to Disable ? When using the kitex command line to generate code, add -no-fast-api to disable Fastpb.\nDeleting the xx.pb.fast.go files can also disable Fastpb. After deleting these files, the Kitex framework will automatically adapt to the official sdk for encoding/decoding.\n","categories":"","description":"","excerpt":"Version Requirements kitex version \u003e= v0.4.0\nOverview Fastpb is a …","ref":"/docs/kitex/tutorials/code-gen/fastpb/","tags":"","title":"Fastpb"},{"body":"版本要求 kitex version \u003e= v0.4.0\n概述 Fastpb 是字节跳动研发的 protobuf 增强插件，使用新的生成代码和 API 来完成 protobuf 的编解码过程，相比于官方库规避了反射，具有更好的性能。\n和官方 protobuf 的性能对比 参考这里。 更多 Fastpb 信息 参考这里。\n使用 (默认开启) Kitex 默认集成了 Fastpb，使用 kitex 命令生成代码时，会在官方的生成代码文件 xx.pb.go 旁边额外增加一份 xx.pb.fast.go 文件，用于 Fastpb 快速编解码。\n如何关闭 在使用 kitex 命令行生成代码时，加上 -no-fast-api 参数，即可关闭 Fastpb。\n删除 xx.pb.fast.go 文件也可以实质上关闭 Fastpb 能力，删除文件后，Kitex 框架会自动适配为官方库编解码。\n","categories":"","description":"","excerpt":"版本要求 kitex version \u003e= v0.4.0\n概述 Fastpb 是字节跳动研发的 protobuf 增强插件，使用新的生成代码 …","ref":"/zh/docs/kitex/tutorials/code-gen/fastpb/","tags":"","title":"Fastpb"},{"body":"Frugal is a very fast dynamic Thrift serializer \u0026 deserializer based on just-in-time compilation.\nFeatures Code Generation Free Traditional Thrift serializer and deserializer are based on generated code which is no longer needed since we can use JIT compilation to dynamically generate machine code.\nHigh Performance Thanks to JIT compilation, Frugal can generate better machine code than Go language compiler. In multi-core scenarios, Frugal’s performance is about 5 times higher than that of traditional serializer and deserializer.\nname old time/op new time/op delta MarshalAllSize_Parallel/small-16 78.8ns ± 0% 14.9ns ± 0% -81.10% MarshalAllSize_Parallel/medium-16 1.34µs ± 0% 0.32µs ± 0% -76.32% MarshalAllSize_Parallel/large-16 37.7µs ± 0% 9.4µs ± 0% -75.02% UnmarshalAllSize_Parallel/small-16 368ns ± 0% 30ns ± 0% -91.90% UnmarshalAllSize_Parallel/medium-16 11.9µs ± 0% 0.8µs ± 0% -92.98% UnmarshalAllSize_Parallel/large-16 233µs ± 0% 21µs ± 0% -90.99% name old speed new speed delta MarshalAllSize_Parallel/small-16 7.31GB/s ± 0% 38.65GB/s ± 0% +428.84% MarshalAllSize_Parallel/medium-16 12.9GB/s ± 0% 54.7GB/s ± 0% +322.10% MarshalAllSize_Parallel/large-16 11.7GB/s ± 0% 46.8GB/s ± 0% +300.26% UnmarshalAllSize_Parallel/small-16 1.56GB/s ± 0% 19.31GB/s ± 0% +1134.41% UnmarshalAllSize_Parallel/medium-16 1.46GB/s ± 0% 20.80GB/s ± 0% +1324.55% UnmarshalAllSize_Parallel/large-16 1.89GB/s ± 0% 20.98GB/s ± 0% +1009.73% name old alloc/op new alloc/op delta MarshalAllSize_Parallel/small-16 112B ± 0% 0B -100.00% MarshalAllSize_Parallel/medium-16 112B ± 0% 0B -100.00% MarshalAllSize_Parallel/large-16 779B ± 0% 57B ± 0% -92.68% UnmarshalAllSize_Parallel/small-16 1.31kB ± 0% 0.10kB ± 0% -92.76% UnmarshalAllSize_Parallel/medium-16 448B ± 0% 3022B ± 0% +574.55% UnmarshalAllSize_Parallel/large-16 1.13MB ± 0% 0.07MB ± 0% -93.54% name old allocs/op new allocs/op delta MarshalAllSize_Parallel/small-16 1.00 ± 0% 0.00 -100.00% MarshalAllSize_Parallel/medium-16 1.00 ± 0% 0.00 -100.00% MarshalAllSize_Parallel/large-16 1.00 ± 0% 0.00 -100.00% UnmarshalAllSize_Parallel/small-16 6.00 ± 0% 1.00 ± 0% -83.33% UnmarshalAllSize_Parallel/medium-16 6.00 ± 0% 30.00 ± 0% +400.00% UnmarshalAllSize_Parallel/large-16 4.80k ± 0% 0.76k ± 0% -84.10% What can you do with Frugal ? Use Frugal as Kitex serializer and deserializer No more massive serialization and deserialization code, leads to a more tidy project. No more meaningless diff of generated code in code review.\nSerialized and Deserialize struct generated by Thriftgo If you have a Thrift file, and all you need is using Frugal to do serialization and deserialization. You can use Thriftgo to generate Go struct, then you can use Frugal.\nSerialization and deserialization on a customized Go struct If you don’t want any Thrift files, and you want serialize or deserialize a customized Go struct. You can add some struct field tag to the Go struct, then you can use Frugal.\nUsage Using with Kitex 1. Update Kitex to v0.4.2 or higher version go get github.com/cloudwego/kitex@latest 2. Generate code with -thrift frugal_tag option Example:\nkitex -thrift frugal_tag -service a.b.c my.thrift If you don’t need codec code, you can use -thrift template=slim option.\nkitex -thrift frugal_tag,template=slim -service a.b.c my.thrift 3. Init clients and servers with WithPayloadCodec(thrift.NewThriftFrugalCodec()) option Client example:\npackage client import ( \"context\" \"github.com/cloudwego/kitex/client\" \"example.com/kitex_test/client/kitex_gen/a/b/c/echo\" \"github.com/cloudwego/kitex/pkg/remote/codec/thrift\" ) func Echo() { code := thrift.NewThriftCodecWithConfig(thrift.FastRead | thrift.FastWrite | thrift.FrugalRead | thrift.FrugalWrite) cli := echo.MustNewClient(\"a.b.c\", client.WithPayloadCodec(codec)) ... } Server example:\npackage main import ( \"log\" \"code.byted.org/kite/kitex/server\" c \"example.com/kitex_test/kitex_gen/a/b/c/echo\" \"github.com/cloudwego/kitex/pkg/remote/codec/thrift\" ) func main() { code := thrift.NewThriftCodecWithConfig(thrift.FastRead | thrift.FastWrite | thrift.FrugalRead | thrift.FrugalWrite) svr := c.NewServer(new(EchoImpl), server.WithPayloadCodec(code)) err := svr.Run() if err != nil { log.Println(err.Error()) } } Using with Thrift IDL Prepare Thrift file We can define a struct in Thrift file like below:\nmy.thrift:\nstructMyStruct{1:stringmsg2:i64code}Use Thriftgo to generate code Now we have thrift file, we can use Thriftgo with frugal_tag option to generate Go code.\nExample:\nthriftgo -r -o thrift -g go:frugal_tag,package_prefix=example.com/kitex_test/thrift my.thrift If you don’t need codec code, you can use template=slim option\nthriftgo -r -o thrift -g go:frugal_tag,template=slim,package_prefix=example.com/kitex_test/thrift my.thrift Use Frugal to serialize or deserialize Now we can use Frugal to serialize or deserialize the struct defined in thrift file.\nExample:\npackage main import ( \"github.com/cloudwego/frugal\" \"example.com/kitex_test/thrift\" ) func main() { ms := \u0026thrift.MyStruct{ Msg: \"my message\", Code: 1024, } ... buf := make([]byte, frugal.EncodedSize(ms)) frugal.EncodeObject(buf, nil, ms) ... got := \u0026thrift.MyStruct{} frugal.DecodeObject(buf, got) ... } Serialization and deserialization on a customized Go struct Define a Go struct We can define a struct like this:\ntype MyStruct struct { Msg string Code int64 Numbers []int64 } Add Frugal tag to struct fields Frugal tag is like frugal:\"1,default,string\", 1 is field ID, default is field requiredness, string is field type. Field ID and requiredness is always required, but field type is only required for list, set and enum.\nYou can add Frugal tag to MyStruct like below:\ntype MyStruct struct { Msg string `frugal:\"1,default\"` Code int64 `frugal:\"2,default\"` Numbers []int64 `frugal:\"3,default,list\u003ci64\u003e\"` } All types example:\ntype MyEnum int64 type Example struct { MyOptBool *bool `frugal:\"1,optional\"` MyReqBool bool `frugal:\"2,required\"` MyOptByte *int8 `frugal:\"3,optional\"` MyReqByte int8 `frugal:\"4,required\"` MyOptI16 *int16 `frugal:\"5,optional\"` MyReqI16 int16 `frugal:\"6,required\"` MyOptI32 *int32 `frugal:\"7,optional\"` MyReqI32 int32 `frugal:\"8,required\"` MyOptI64 *int64 `frugal:\"9,optional\"` MyReqI64 int64 `frugal:\"10,required\"` MyOptString *string `frugal:\"11,optional\"` MyReqString string `frugal:\"12,required\"` MyOptBinary []byte `frugal:\"13,optional\"` MyReqBinary []byte `frugal:\"14,required\"` MyOptI64Set []int64 `frugal:\"15,optional,set\u003ci64\u003e\"` MyReqI64Set []int64 `frugal:\"16,required,set\u003ci64\u003e\"` MyOptI64List []int64 `frugal:\"17,optional,list\u003ci64\u003e\"` MyReqI64List []int64 `frugal:\"18,required,list\u003ci64\u003e\"` MyOptI64StringMap map[int64]string `frugal:\"19,optional\"` MyReqI64StringMap map[int64]string `frugal:\"20,required\"` MyOptEnum *MyEnum `frugal:\"21,optional,i64\"` MyReqEnum *MyEnum `frugal:\"22,optional,i64\"` } Use Frugal to serialize or deserialize Example:\npackage main import ( \"github.com/cloudwego/frugal\" ) func main() { ms := \u0026thrift.MyStruct{ Msg: \"my message\", Code: 1024, Numbers: []int64{0, 1, 2, 3, 4}, } ... buf := make([]byte, frugal.EncodedSize(ms)) frugal.EncodeObject(buf, nil, ms) ... got := \u0026thrift.MyStruct{} frugal.DecodeObject(buf, got) ... } ","categories":"","description":"","excerpt":"Frugal is a very fast dynamic Thrift serializer \u0026 deserializer based …","ref":"/docs/kitex/tutorials/advanced-feature/codec_frugal/","tags":"","title":"Frugal"},{"body":"Frugal 是一款基于 JIT 编译的高性能动态 Thrift 编解码器。\n特点 无需生成代码 传统的 Thrift 编解码方式，要求用户必须要先生成编解码代码，Frugal 通过 JIT 编译技术在运行时动态生成编解码机器代码，避免了这一过程。\n高性能 基于 JIT 技术 Frugal 可以生成比 Go 语言编译器性能更好的机器代码，在多核场景下，Frugal 的性能可以达到传统编解码方式的 5 倍左右。\nname old time/op new time/op delta MarshalAllSize_Parallel/small-16 78.8ns ± 0% 14.9ns ± 0% -81.10% MarshalAllSize_Parallel/medium-16 1.34µs ± 0% 0.32µs ± 0% -76.32% MarshalAllSize_Parallel/large-16 37.7µs ± 0% 9.4µs ± 0% -75.02% UnmarshalAllSize_Parallel/small-16 368ns ± 0% 30ns ± 0% -91.90% UnmarshalAllSize_Parallel/medium-16 11.9µs ± 0% 0.8µs ± 0% -92.98% UnmarshalAllSize_Parallel/large-16 233µs ± 0% 21µs ± 0% -90.99% name old speed new speed delta MarshalAllSize_Parallel/small-16 7.31GB/s ± 0% 38.65GB/s ± 0% +428.84% MarshalAllSize_Parallel/medium-16 12.9GB/s ± 0% 54.7GB/s ± 0% +322.10% MarshalAllSize_Parallel/large-16 11.7GB/s ± 0% 46.8GB/s ± 0% +300.26% UnmarshalAllSize_Parallel/small-16 1.56GB/s ± 0% 19.31GB/s ± 0% +1134.41% UnmarshalAllSize_Parallel/medium-16 1.46GB/s ± 0% 20.80GB/s ± 0% +1324.55% UnmarshalAllSize_Parallel/large-16 1.89GB/s ± 0% 20.98GB/s ± 0% +1009.73% name old alloc/op new alloc/op delta MarshalAllSize_Parallel/small-16 112B ± 0% 0B -100.00% MarshalAllSize_Parallel/medium-16 112B ± 0% 0B -100.00% MarshalAllSize_Parallel/large-16 779B ± 0% 57B ± 0% -92.68% UnmarshalAllSize_Parallel/small-16 1.31kB ± 0% 0.10kB ± 0% -92.76% UnmarshalAllSize_Parallel/medium-16 448B ± 0% 3022B ± 0% +574.55% UnmarshalAllSize_Parallel/large-16 1.13MB ± 0% 0.07MB ± 0% -93.54% name old allocs/op new allocs/op delta MarshalAllSize_Parallel/small-16 1.00 ± 0% 0.00 -100.00% MarshalAllSize_Parallel/medium-16 1.00 ± 0% 0.00 -100.00% MarshalAllSize_Parallel/large-16 1.00 ± 0% 0.00 -100.00% UnmarshalAllSize_Parallel/small-16 6.00 ± 0% 1.00 ± 0% -83.33% UnmarshalAllSize_Parallel/medium-16 6.00 ± 0% 30.00 ± 0% +400.00% UnmarshalAllSize_Parallel/large-16 4.80k ± 0% 0.76k ± 0% -84.10% 用 Frugal 可以做什么？ 使用 Frugal 作为 Kitex 的编解码 可以不用再生成大量的编解码代码，使仓库变得干净整洁，review 时也不用再带上一堆无意义的 diff。然后相比于生成的编解码代码，Frugal 的性能更高。\n在 Thriftgo 生成的 struct 上进行编解码 如果你只需要使用 Thrift 的编解码能力，同时也定义好了 IDL，那么只需要用 Thriftgo 生成 IDL 对应的 Go 语言 struct，就可以使用 Frugal 的编解码能力了。\n直接定义 struct 进行编解码 如果你们连 IDL 都不想有，没问题，直径定义好 Go 语言 struct 后，给每个 Field 带上 Frugal 所需的 tag，就可以直接使用 Frugal 进行编解码了。\n使用手册 配合 Kitex 使用 1. 更新 Kitex 到 v0.4.2 以上版本 go get github.com/cloudwego/kitex@latest 2. 带上 -thrift frugal_tag 参数重新生成一次代码 示例：\nkitex -thrift frugal_tag -service a.b.c my.thrift 如果不需要编解码代码，可以带上 -thrift template=slim 参数\nkitex -thrift frugal_tag,template=slim -service a.b.c my.thrift 3. 初始化 client 和 server 时使用 WithPayloadCodec(thrift.NewThriftFrugalCodec()) option client 示例：\npackage client import ( \"context\" \"github.com/cloudwego/kitex/client\" \"example.com/kitex_test/client/kitex_gen/a/b/c/echo\" \"github.com/cloudwego/kitex/pkg/remote/codec/thrift\" ) func Echo() { code := thrift.NewThriftCodecWithConfig(thrift.FastRead | thrift.FastWrite | thrift.FrugalRead | thrift.FrugalWrite) cli := echo.MustNewClient(\"a.b.c\", client.WithPayloadCodec(codec)) ... } server 示例：\npackage main import ( \"log\" \"code.byted.org/kite/kitex/server\" c \"example.com/kitex_test/kitex_gen/a/b/c/echo\" \"github.com/cloudwego/kitex/pkg/remote/codec/thrift\" ) func main() { code := thrift.NewThriftCodecWithConfig(thrift.FastRead | thrift.FastWrite | thrift.FrugalRead | thrift.FrugalWrite) svr := c.NewServer(new(EchoImpl), server.WithPayloadCodec(code)) err := svr.Run() if err != nil { log.Println(err.Error()) } } 配合 Thriftgo 做 Thrift IDL 的编解码 编写 Thrift 文件 现在假设我们有如下 Thrift 文件： my.thrift:\nstructMyStruct{1:stringmsg2:i64code}使用 Thriftgo 生成代码 定义好需要的 Thrift 文件后，在使用 Thriftgo 生成 Go 语言代码时使用 frugal_tag 参数。 示例：\nthriftgo -r -o thrift -g go:frugal_tag,package_prefix=example.com/kitex_test/thrift my.thrift 如果不需要编解码代码，可以带上 template=slim 参数\nthriftgo -r -o thrift -g go:frugal_tag,template=slim,package_prefix=example.com/kitex_test/thrift my.thrift 使用 Frugal 进行编解码 生成所需要的结构体后，直接使用 Frugal 进行编解码即可。 示例：\npackage main import ( \"github.com/cloudwego/frugal\" \"example.com/kitex_test/thrift\" ) func main() { ms := \u0026thrift.MyStruct{ Msg: \"my message\", Code: 1024, } ... buf := make([]byte, frugal.EncodedSize(ms)) frugal.EncodeObject(buf, nil, ms) ... got := \u0026thrift.MyStruct{} frugal.DecodeObject(buf, got) ... } 直接定义 struct 进行编解码 定义 struct 现在假设我们需要如下 struct：\ntype MyStruct struct { Msg string Code int64 Numbers []int64 } 给结构体字段添加 tag Frugal 所需的 tag 形如 frugal:\"1,default,string\"，其中 1 为字段 ID， default 为字段的 requiredness， string 表示字段的类型。字段 ID 和 字段 requiredness 是必须的，但是字段类型只有当字段为 list 、set 和 enum 时是必须的。\n上述的 MyStruct 可以添加如下 tag：\ntype MyStruct struct { Msg string `frugal:\"1,default\"` Code int64 `frugal:\"2,default\"` Numbers []int64 `frugal:\"3,default,list\u003ci64\u003e\"` } 下面是完整的类型示例：\ntype MyEnum int64 type Example struct { MyOptBool *bool `frugal:\"1,optional\"` MyReqBool bool `frugal:\"2,required\"` MyOptByte *int8 `frugal:\"3,optional\"` MyReqByte int8 `frugal:\"4,required\"` MyOptI16 *int16 `frugal:\"5,optional\"` MyReqI16 int16 `frugal:\"6,required\"` MyOptI32 *int32 `frugal:\"7,optional\"` MyReqI32 int32 `frugal:\"8,required\"` MyOptI64 *int64 `frugal:\"9,optional\"` MyReqI64 int64 `frugal:\"10,required\"` MyOptString *string `frugal:\"11,optional\"` MyReqString string `frugal:\"12,required\"` MyOptBinary []byte `frugal:\"13,optional\"` MyReqBinary []byte `frugal:\"14,required\"` MyOptI64Set []int64 `frugal:\"15,optional,set\u003ci64\u003e\"` MyReqI64Set []int64 `frugal:\"16,required,set\u003ci64\u003e\"` MyOptI64List []int64 `frugal:\"17,optional,list\u003ci64\u003e\"` MyReqI64List []int64 `frugal:\"18,required,list\u003ci64\u003e\"` MyOptI64StringMap map[int64]string `frugal:\"19,optional\"` MyReqI64StringMap map[int64]string `frugal:\"20,required\"` MyOptEnum *MyEnum `frugal:\"21,optional,i64\"` MyReqEnum *MyEnum `frugal:\"22,optional,i64\"` } 使用 Frugal 进行编解码 直接使用 Frugal 进行编解码即可。 示例：\npackage main import ( \"github.com/cloudwego/frugal\" ) func main() { ms := \u0026thrift.MyStruct{ Msg: \"my message\", Code: 1024, Numbers: []int64{0, 1, 2, 3, 4}, } ... buf := make([]byte, frugal.EncodedSize(ms)) frugal.EncodeObject(buf, nil, ms) ... got := \u0026thrift.MyStruct{} frugal.DecodeObject(buf, got) ... } ","categories":"","description":"","excerpt":"Frugal 是一款基于 JIT 编译的高性能动态 Thrift 编解码器。\n特点 无需生成代码 传统的 Thrift 编解码方式，要求用户 …","ref":"/zh/docs/kitex/tutorials/advanced-feature/codec_frugal/","tags":"","title":"Frugal"},{"body":"Some users will ask how to let the client-side receive the corresponding error type of the server-side, here to explain, RPC communicates through the protocol, and error handling is also based on the protocol. Usually when the server returns Error, the framework will unify the Error encoding and return it to the client side. If you want the client to return the same error as the server-side, you need to define a set of error codes for handling. However, considering that RPC does not have a unified error code specification, and internal error codes are not necessarily applicable to external users, so the open source part of Kitex does not expose the error code definition, and users can customize their own error handler by using the provided ErrorHandler.\nRecommended Usage ErrorHandler is configured via the client/server Option, but usually, a microservice system will have a unified error handler specification, if you are an enterprise user, it is recommended to customize the Option through the Suite, so that the service developer does not need to pay attention to the configuration of error handler.\nServer-side Configuration // server option server.WithErrorHandler(yourServerErrorHandler) This function is executed after the server-side handler and before the middleware is executed, and can be used to return custom error codes and messages to the client-side. Note that although this is supported, business-level custom error codes are still not recommended to be handled by ErrorHandler, because we want to distinguish RPC errors from business errors, which indicate a failed RPC request, such as timeout, circuit breaker, rate limiting, which is a failed request at the RPC level, but business errors are at the business logic level, which is actually a successful request at the RPC level. Kitex will develop a custom exception specification to distinguish between business errors and RPC level errors.\n  ErrorHandler example：\nKitex wraps the error returned by the handler as kerrors.ErrBiz, if you want to get the original error you need to Unwrap it first.\n  // convert errors that can be serialized func ServerErrorHandler(err error) error { if errors.Is(err, kerrors.ErrBiz) { err = errors.Unwrap(err) } if errCode, ok := GetErrorCode(err); ok { // for Thrift、KitexProtobuf  return remote.NewTransError(errCode, err) } return err } // convert errors that can be serialized func ServerErrorHandler(err error) error { if errors.Is(err, kerrors.ErrBiz) { err = errors.Unwrap(err) } if errCode, ok := GetErrorCode(err); ok { // for gRPC  // status use github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/status  return status.Errorf(errCode, err.Error()) } return err } Client-side Configuration // client option client.WithErrorHandler(yourClientErrorHandler) The handler is executed after the remote call and before the middleware is executed. The framework has a default ClientErrorHandler, it will be used by default if no ClientErrorHandler is configured. The behavior of the default Handler is to return ErrRemoteOrNetwork when an error is received from the server-side or when an exception occurs at the transport layer on the client side. In addition, for Thrift and KitexProtobuf, the error msg contains a ‘[remote]’ message to identify that it is an error on the other side; for gRPC, if the other side returns an error constructed by status.Error, use status.FromError(err) on the local side to get *status.Status, note that Status needs to be provided by Kitex, the package path is github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/status.\n ErrorHandler example：  func ClientErrorHandler(err error) error { // for thrift、KitexProtobuf  if e, ok := err.(*remote.TransError); ok { // TypeID is error code  return buildYourError(e.TypeID(), e) } // for gRPC  if s, ok := status.FromError(err); ok { return buildYourErrorWithStatus(s.Code(), s) } return kerrors.ErrRemoteOrNetwork.WithCause(err) } Error Code Definition Range Because some of the error codes are built-in to the framework, users should avoid using the built-in error codes, currently built-in error codes:\n  Thrift、KitexProtobuf：0 - 10。\n  gRPC：0 - 17。\n  ErrorHandler Execution Mechanism The ErrorHandler is executed in the Middleware, either on the client-side or on the server-side ErrorHandler is executed as the innermost Middleware, as shown below:\n","categories":"","description":"","excerpt":"Some users will ask how to let the client-side receive the …","ref":"/docs/kitex/tutorials/advanced-feature/error_handler/","tags":"","title":"Customize Error Handler"},{"body":"​\tKitex provides the WithGRPCUnknownServiceHandler function when transport is using gRPC. When the server receives a request from an unknown gRPC method, it will execute the unknown service handler:\nfunc handler(ctx context.Context, methodName string, stream streaming.Stream) error { // .... handle unknown service  } func RunServer(){ // ...  svr := service.NewServer(server.WithGRPCUnknownServiceHandler(handler),xxx,xxx) // ...  } ​\tA gRPC Proxy Server can be implemented through the gRPCUnknownServiceHandler provided by Kitex. In the grpc proxy Kitex Example, the gRPC Proxy implementations of two scenarios are shown respectively, namely:\n Read gRPC Frame and forward it directly Read gRPC and decode it into a structure, and then forward it after checking or customizing the structure  ​\tThe following two proxy implementation ideas in Kitex Example are explained, so that users can refer to them and implement them according to their own needs.\nRedirecting gRPC Frame ​\tWhen the gRPC Proxy we want to implement does not care about the specific content of RPC, it does not need to encode and decode, and directly forwards the obtained gRPC Frame message to the target end, without introducing other codes such as stub modules. An example is as follows:\nfunc GRPCFrameProxyHandler(ctx context.Context, methodName string, stream streaming.Stream) error { // find target address by methodName \tnetwork, address := proxy.Resolve(methodName) // create a new RPC Info and modify some infos if you want. \tsri := rpcinfo.GetRPCInfo(ctx) ri := rpcinfo.NewRPCInfo(sri.From(), sri.To(), sri.Invocation(), sri.Config(), sri.Stats()) clientCtx := rpcinfo.NewCtxWithRPCInfo(context.Background(), ri) conn, err := connPool.Get(clientCtx, network, address, remote.ConnOption{ Dialer: netpoll.NewDialer(), ConnectTimeout: 0, }) if err != nil { return err } clientConn := conn.(nphttp2.GRPCConn) defer func() { clientConn.Close() connPool.Put(clientConn) }() serverConn, err := nphttp2.GetServerConn(stream) if err != nil { return err } s2c := redirectFrame(serverConn, clientConn) c2s := redirectFrame(clientConn, serverConn) // ... } ​\tFirst, get IP Address and other information of the target terminal, and then directly obtain a peer connection from the connection pool, encapsulate it as a GRPCConn structure, and use its ReadFrame and WriteFrame to send data.\n​\tIt should be noted that the user needs to create a new connection pool, set the corresponding parameters, and perform the corresponding connection release and other operations after using the connection in the Unknown Service Handler. For the relevant writing method, please refer to the code in this example.\n​\tThe code for reading and forwarding gRPC Frame is implemented as follows:\nfunc redirectFrame(from, to nphttp2.GRPCConn) chan error { ret := make(chan error) go func() { for { hdr, data, err := from.ReadFrame() if err != nil { // write last empty data frame with END_STREAM flag \tto.WriteFrame(hdr, data) ret \u003c- err break } _, err = to.WriteFrame(hdr, data) if err != nil { ret \u003c- err break } } }() return ret } ​\tReadFrame is used to continuously read gRPC Frame and write to the forwarding destination. When the last read ReadFrame receives a Data Frame with an EndStream identifier, ReadFrame will receive io.EOF, which means the connection is in a half-closed state. At this time, the values of hdr and data are both nil, so using WriteFrame is also far away. The end sends an empty packet with EndStream, indicating that the sending content is over, otherwise the proxy server may be blocked.\nDecoding and Redrecting ​\tIn some proxy server scenarios, we need to decode and obtain the structure object, perform some custom processing (such as reading the request for judgment, or modify some fields of the request), and then resend the structure to the remote end. In this scenario, it may be necessary to introduce the corresponding client stub module code in the proxy server code. An example is as follows:\nfunc GRPCStructProxyHandler(ctx context.Context, methodName string, serverStream streaming.Stream) error { // find target address by methodName \t_, address := proxy.Resolve(methodName) // \tclient, _ := servicea.NewClient(\"destService\", client.WithHostPorts(address), client.WithTransportProtocol(transport.GRPC)) clientStream, err := client.Chat(context.Background()) if err != nil { return err } s2c := redirectStruct(serverStream, clientStream) c2s := redirectStruct(clientStream, serverStream) // ... } ​\tFirst, get IP Address and other information of the target, and then create a client to connect with the target. Next, the decoding and forwarding processing of the data is performed. This example is a bidirectional streaming scenario, so the client also performs multiple structure sending and receiving operations through clientStream. Write the following code to make serverStream read and decode the structure, and then forward it to clientStream:\nfunc redirectStruct(from, to streaming.Stream) chan error { ret := make(chan error) go func() { for { req := \u0026grpcproxy.Request{} err := from.RecvMsg(req) if err != nil { from.Close() ret \u003c- err break } // do your own filter logic here  //if req.Name==xxx{  // continue  //}  err = to.SendMsg(req) if err != nil { ret \u003c- err break } } }() return ret } ​\tIn this part, the data read through RecvMsg is serialized and written into the structure, and the judgment and modification operations on the structure fields can be added, and then forwarded.\n","categories":"","description":"","excerpt":"​\tKitex provides the WithGRPCUnknownServiceHandler function when …","ref":"/docs/kitex/tutorials/advanced-feature/grpcproxy/","tags":"","title":"gRPC Proxy"},{"body":"​\tKitex 对 gRPC 场景提供了 WithGRPCUnknownServiceHandler 功能，当服务器接收到未注册的 gRPC 方法调用的请求时，将执行自定义的 Unknown Service Handler 函数进行处理：\nfunc handler(ctx context.Context, methodName string, stream streaming.Stream) error { // .... handle unknown service  } func RunServer(){ // ...  svr := service.NewServer(server.WithGRPCUnknownServiceHandler(handler),xxx,xxx) // ...  } ​\t通过 Kitex 提供的 gRPCUnknownServiceHandler，可以实现 gRPC Proxy 代理服务器。在 Kitex Example 中的grpc proxy 示例中，分别展示了两种场景的 gRPC Proxy 实现，分别是：\n 读取 gRPC Frame 并直接转发 读取 gRPC 并解码为结构体，在对结构体进行检查或自定义操作后再进行转发  ​\t下文对 Kitex Example 中的两种 Proxy 实现思路进行讲解，以便使用者参考，并按照自己的需求进行实现。\ngRPC Frame 直接转发 ​\t当我们要实现的 gRPC Proxy 并不关心 RPC 的具体内容时，无需编解码，直接将拿到的 gRPC Frame 报文转发至目标端，不需要引入桩模块等其他代码。示例如下：\nfunc GRPCFrameProxyHandler(ctx context.Context, methodName string, stream streaming.Stream) error { // find target address by methodName \tnetwork, address := proxy.Resolve(methodName) // create a new RPC Info and modify some infos if you want. \tsri := rpcinfo.GetRPCInfo(ctx) ri := rpcinfo.NewRPCInfo(sri.From(), sri.To(), sri.Invocation(), sri.Config(), sri.Stats()) clientCtx := rpcinfo.NewCtxWithRPCInfo(context.Background(), ri) conn, err := connPool.Get(clientCtx, network, address, remote.ConnOption{ Dialer: netpoll.NewDialer(), ConnectTimeout: 0, }) if err != nil { return err } clientConn := conn.(nphttp2.GRPCConn) defer func() { clientConn.Close() connPool.Put(clientConn) }() serverConn, err := nphttp2.GetServerConn(stream) if err != nil { return err } s2c := redirectFrame(serverConn, clientConn) c2s := redirectFrame(clientConn, serverConn) // ... } ​\t首先获取目标端的 IP Address 等信息，然后从连接池中直接获取到一条对端连接，封装为 GRPCConn 结构体，利用其 ReadFrame 与 WriteFrame 进行数据发送。\n​\t需要注意的是，这里需要用户自己创建新的连接池，设置相应的参数，并在 Unknown Service Handler 中使用完连接后进行相应的连接释放等操作，相关写法可以参考本示例中的代码。\n​\t读取并转发 gRPC Frame 的代码实现如下：\nfunc redirectFrame(from, to nphttp2.GRPCConn) chan error { ret := make(chan error) go func() { for { hdr, data, err := from.ReadFrame() if err != nil { // write last empty data frame with END_STREAM flag \tto.WriteFrame(hdr, data) ret \u003c- err break } _, err = to.WriteFrame(hdr, data) if err != nil { ret \u003c- err break } } }() return ret } ​\t这里使用 ReadFrame 不断读取 gRPC Frame 并写入转发目标端。当最后一次读取 ReadFrame 收到带有 EndStream 标识符的 Data Frame 后，ReadFrame 会收到 io.EOF，代表连接处于半关闭状态，此时 hdr 和 data 的值都为 nil，所以使用 WriteFrame 也向远端发送一个带有 EndStream 的空包，表示发送内容结束，否则可能会出现 proxy server 阻塞的场景。\n解码处理后转发 ​\t在有些 proxy server 的场景中，我们需要解码获取到结构体对象，并进行一些自定义的处理（例如读取请求做判断，或者修改请求的某些字段），再将结构体重新发送到远端。这种场景下，可能需要在 proxy server 的代码中引入对应的 client 桩模块代码。示例如下：\nfunc GRPCStructProxyHandler(ctx context.Context, methodName string, serverStream streaming.Stream) error { // find target address by methodName \t_, address := proxy.Resolve(methodName) // \tclient, _ := servicea.NewClient(\"destService\", client.WithHostPorts(address), client.WithTransportProtocol(transport.GRPC)) clientStream, err := client.Chat(context.Background()) if err != nil { return err } s2c := redirectStruct(serverStream, clientStream) c2s := redirectStruct(clientStream, serverStream) // ... } ​\t首先获取到目标端的 IP Address 等信息，然后创建客户端，和目标端进行连接。接下来进行数据的解码和转发处理。本示例为双向流的场景，所以客户端也通过 clientStream 进行多次的结构体收发操作，编写如下代码，使 serverStream 读取并解码结构体，然后转发到 clientStream 中：\nfunc redirectStruct(from, to streaming.Stream) chan error { ret := make(chan error) go func() { for { req := \u0026grpcproxy.Request{} err := from.RecvMsg(req) if err != nil { from.Close() ret \u003c- err break } // do your own filter logic here  //if req.Name==xxx{  // continue  //}  err = to.SendMsg(req) if err != nil { ret \u003c- err break } } }() return ret } ​\t在这部分中，通过 RecvMsg 读取到的数据经过序列化，会写入结构体中，可以加入对结构体字段的判断和修改操作，然后再进行转发。\n","categories":"","description":"","excerpt":"​\tKitex 对 gRPC 场景提供了 WithGRPCUnknownServiceHandler 功能，当服务器接收到未注册的 gRPC …","ref":"/zh/docs/kitex/tutorials/advanced-feature/grpcproxy/","tags":"","title":"gRPC Proxy"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/hertz/tutorials/framework-exten/","tags":"","title":"Framework Extension"},{"body":"Hertz uses the open source library go-tagexpr for parameter binding and validation. The following describes the usage of parameter binding and parameter validation.\nUsage func main() { r := server.New() r.GET(\"/hello\", func(c context.Context, ctx *app.RequestContext) { // Parameter binding needs to be used with a specific go tag \ttype Test struct { A string `query:\"a\" vd:\"$!='Hertz'\"` } // BindAndValidate  var req Test err := ctx.BindAndValidate(\u0026req) ... // Bind  req = Test{} err = ctx.Bind(\u0026req) ... // Validate, need to use \"vd\" tag  err = ctx.Validate(\u0026req) ... }) ... } Supported tags and parameter binding priorities Supported tags    go tag description     path This tag is used to bind parameters on url like {:param} or {*param}. For example: if we defined route is: /v:version/example, you can specify the path parameter as the route parameter: path:\"version\". In this case if url is http://127.0.0.1:8888/v1/ , you can bind the path parameter “1”.   form This tag is used to bind the key-value of the form in request body which content-type is multipart/form-data or application/x-www-form-urlencoded   query This tag is used to bind query parameter in request   header This tag is used to bind header parameters in request   json This tag is used to bind json parameters in the request body which content-type is application/json   raw_body This tag is used to bind the original body (bytes type) of the request, and parameters can be bound even if the bound field name is not specified. (Note: raw_body has the lowest binding priority. When multiple tags are specified, once other tags successfully bind parameters, the body content will not be bound)   vd vd short for validator, The grammar of validation parameter    Parameter binding priority path \u003e form \u003e query \u003e cookie \u003e header \u003e json \u003e raw_body  Note: If the request content-type is application/json, json unmarshal processing will be done by default before parameter binding\n Common uses Customize the error of binding and validation When an error occurs in the binding parameter and the parameter validation fails, user can customize the Error（demo）For example：\n// Error implements error interface. func (e *BindError) Error() string { if e.Msg != \"\" { return e.ErrType + \": expr_path=\" + e.FailField + \", cause=\" + e.Msg } return e.ErrType + \": expr_path=\" + e.FailField + \", cause=invalid\" } type ValidateError struct { ErrType, FailField, Msg string } // Error implements error interface. func (e *ValidateError) Error() string { if e.Msg != \"\" { return e.ErrType + \": expr_path=\" + e.FailField + \", cause=\" + e.Msg } return e.ErrType + \": expr_path=\" + e.FailField + \", cause=invalid\" } func init() { CustomBindErrFunc := func(failField, msg string) error { err := BindError{ ErrType: \"bindErr\", FailField: \"[bindFailField]: \" + failField, Msg: \"[bindErrMsg]: \" + msg, } return \u0026err } CustomValidateErrFunc := func(failField, msg string) error { err := ValidateError{ ErrType: \"validateErr\", FailField: \"[validateFailField]: \" + failField, Msg: \"[validateErrMsg]: \" + msg, } return \u0026err } binding.SetErrorFactory(CustomBindErrFunc, CustomValidateErrFunc) } Customize type resolution In parameter binding, all request parameters to string or []string by default. When some field types are non-basic types or cannot be converted directly through string, you can customize type resolution（demo). For example：\nimport \"github.com/cloudwego/hertz/pkg/app/server/binding\" type Nested struct { B string C string } type TestBind struct { A Nested `query:\"a,required\"` } func init() { binding.MustRegTypeUnmarshal(reflect.TypeOf(Nested{}), func(v string, emptyAsZero bool) (reflect.Value, error) { if v == \"\" \u0026\u0026 emptyAsZero { return reflect.ValueOf(Nested{}), nil } val := Nested{ B: v[:5], C: v[5:], } return reflect.ValueOf(val), nil }) } Customize the validation function You can implement complex validation logic in the vd tag by registering a custom validation function（demo)，For example：\nimport \"github.com/cloudwego/hertz/pkg/app/server/binding\" func init() { binding.MustRegValidateFunc(\"test\", func(args ...interface{}) error { if len(args) != 1 { return fmt.Errorf(\"the args must be one\") } s, _ := args[0].(string) if s == \"123\" { return fmt.Errorf(\"the args can not be 123\") } return nil }) } Configure “looseZero” In some cases, the information sent from the front end is only the key but value empty, which causes cause=parameter type does not match binding data when binding a numeric type. At this time, you need to configure looseZero mode (demo). For example：\nimport \"github.com/cloudwego/hertz/pkg/app/server/binding\" func init() { // Default false, take effect globally  binding.SetLooseZeroMode(true) } Configure other json unmarshal libraries When binding parameters, if the request body is json, a json unmarshal will be performed. If users need to use other json libraries (hertz uses the open source json library sonic by default), they can configure it themselves. For example:\nimport \"github.com/cloudwego/hertz/pkg/app/server/binding\" func init() { // use the standard library  binding.UseStdJSONUnmarshaler() // use gjson  binding.UseGJSONUnmarshaler() // use other json unmarshal methods  binding.UseThirdPartyJSONUnmarshaler() } Set default values The parameter supports the default tag to configure the default value. For example:\n// generate code type UserInfoResponse struct { NickName string `default:\"Hertz\" json:\"NickName\" query:\"nickname\"` } Bind files Parameter binding supports binding files. For example:\n// content-type: multipart/form-data type FileParas struct { F *multipart.FileHeader `form:\"F1\"` } h.POST(\"/upload\", func(ctx context.Context, c *app.RequestContext) { var req FileParas err := binding.BindAndValidate(c, \u0026req) }) Analysis of common problems 1. string to int error: json: cannot unmarshal string into Go struct field xxx of type intxx\nReason: string and int conversion is not supported by default\nSolution：\n  We are recommended to use the string tag of the standard package json. For example：\nA int `json:\"A, string\"`   Configure other json libraries that support this operation.\n  ","categories":"","description":"","excerpt":"Hertz uses the open source library go-tagexpr for parameter binding …","ref":"/docs/hertz/tutorials/basic-feature/binding-and-validate/","tags":"","title":"Binding and validate"},{"body":"部分用户会问如何让调用端收到服务端对应的错误类型，这里解释一下，RPC 是通过协议进行通信，错误的处理也是基于协议，通常服务端返回 Error，RPC 框架统一进行错误编码返回回调用端，如果要让调用端返回和服务端一样的错误，需要定义一套错误码进行处理。但考虑到 RPC 并没有统一的错误码规范且内部的错误码不一定适用于外部用户，所以 Kitex 的开源部分没有暴露错误码定义，用户可以通过提供 ErrorHandler 来定制自己的错误处理。\n建议使用方式 ErrorHandler 通过 client/server 的 Option 配置，但通常一个微服务体系会有统一的异常处理规范，如果是企业用户建议通过 Suite 封装定制 Option，服务开发者就不用具体关注异常处理的配置。\n服务端配置 // server option server.WithErrorHandler(yourServerErrorHandler) 该函数会在服务端 handler 执行后，中间件执行前被执行，可以用于给调用端返回自定义的错误码和信息。注意，虽然对此提供了支持，但业务层面自定义的错误码依然不建议通过 ErrorHandler 处理，因为我们希望将 RPC 错误和业务的错误能够区分开，RPC 错误表示一次RPC 请求失败，比如超时、熔断、限流，从 RPC 层面是失败的请求，但业务错误属于业务逻辑层面，在 RPC 层面其实是请求成功。Kitex 会制定一个业务自定义异常规范用于区分业务错误和RPC层面错误。\n  ErrorHandler 示例：\nKitex 对 handler 返回的 error 统一封装为 kerrors.ErrBiz，如果要获取原始的 error 需要先进行 Unwrap。\n  // convert errors that can be serialized func ServerErrorHandler(err error) error { if errors.Is(err, kerrors.ErrBiz) { err = errors.Unwrap(err) } if errCode, ok := GetErrorCode(err); ok { // for Thrift、KitexProtobuf  return remote.NewTransError(errCode, err) } return err } // convert errors that can be serialized func ServerErrorHandler(err error) error { if errors.Is(err, kerrors.ErrBiz) { err = errors.Unwrap(err) } if errCode, ok := GetErrorCode(err); ok { // for gRPC  // status use github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/status  return status.Errorf(errCode, err.Error()) } return err } 调用端配置 // client option client.WithErrorHandler(yourClientErrorHandler) 该 handler 在远程调用结束，中间件执行前被执行。框架有默认的 ClientErrorHandler，如果未配置将使用默认的，默认 Handler 的行为是：接收到服务端的错误返回或者调用端在传输层出现了异常，统一返回 ErrRemoteOrNetwork。另外，对于 Thrift 和 KitexProtobuf，error msg 会包含 ‘[remote]’ 信息用来标识这是对端的错误；对于 gRPC 如果对端通过 status.Error 构造的错误返回，本端使用 status.FromError(err) 可以获取 *status.Status，注意 Status 需使用 Kitex 提供的，包路径是 github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/status。\n ErrorHandler 示例：  func ClientErrorHandler(err error) error { // for thrift、KitexProtobuf \tif e, ok := err.(*remote.TransError); ok { // TypeID is error code \treturn buildYourError(e.TypeID(), e) } // for gRPC  if s, ok := status.FromError(err); ok { return buildYourErrorWithStatus(s.Code(), s) } return kerrors.ErrRemoteOrNetwork.WithCause(err) } 错误码定义范围 因为部分错误码是框架内置的，所以使用者应当避开内置错误码，目前内置的错误码：\n  Thrift、KitexProtobuf：0 - 10。\n  gRPC：0 - 17。\n  ErrorHandler 执行机制 ErrorHandler 在 Middleware 中被执行，无论是调用端还是服务端 ErrorHandler 都作为最里层的 Middleware 被执行，如图所示：\n","categories":"","description":"","excerpt":"部分用户会问如何让调用端收到服务端对应的错误类型，这里解释一下，RPC 是通过协议进行通信，错误的处理也是基于协议， …","ref":"/zh/docs/kitex/tutorials/advanced-feature/error_handler/","tags":"","title":"定制框架错误处理"},{"body":"hertz 使用开源库 go-tagexpr 进行参数的绑定及验证，下面分别介绍参数绑定和参数验证的用法。\n使用方法 func main() { r := server.New() r.GET(\"/hello\", func(c context.Context, ctx *app.RequestContext) { // 参数绑定需要配合特定的go tag使用 \ttype Test struct { A string `query:\"a\" vd:\"$!='Hertz'\"` } // BindAndValidate  var req Test err := ctx.BindAndValidate(\u0026req) ... // Bind  req = Test{} err = ctx.Bind(\u0026req) ... // Validate，需要使用 \"vd\" tag  err = ctx.Validate(\u0026req) ... }) ... } 支持的 tag 及参数绑定优先级 支持的 tag    go tag 说明     path 绑定 url 上的路径参数，相当于 hertz 路由{:param}或{*param}中拿到的参数。例如：如果定义的路由为: /v:version/example，可以把 path 的参数指定为路由参数：path:\"version\"，此时，url: http://127.0.0.1:8888/v1/example，可以绑定path参数\"1\"   form 绑定请求的 body 内容。content-type -\u003e multipart/form-data 或 application/x-www-form-urlencoded，绑定 form 的 key-value   query 绑定请求的 query 参数   header 绑定请求的 header 参数   json 绑定请求的 body 内容 content-type -\u003e application/json，绑定 json 参数   raw_body 绑定请求的原始 body(bytes)，绑定的字段名不指定，也能绑定参数。（注：raw_body 绑定优先级最低，当指定多个 tag 时，一旦其他 tag 成功绑定参数，则不会绑定 body 内容。）   vd 参数校验，校验语法    参数绑定优先级 path \u003e form \u003e query \u003e cookie \u003e header \u003e json \u003e raw_body  注：如果请求的 content-type 为 application/json，那么会在参数绑定前做一次 json unmarshal 处理作为兜底。\n 常见用法 自定义 bind 和 validate 的 Error 绑定参数发生错误和参数校验失败的时候，用户可以自定义的 Error（demo ），使用方法如下：\nimport \"github.com/cloudwego/hertz/pkg/app/server/binding\" type BindError struct { ErrType, FailField, Msg string } // Error implements error interface. func (e *BindError) Error() string { if e.Msg != \"\" { return e.ErrType + \": expr_path=\" + e.FailField + \", cause=\" + e.Msg } return e.ErrType + \": expr_path=\" + e.FailField + \", cause=invalid\" } type ValidateError struct { ErrType, FailField, Msg string } // Error implements error interface. func (e *ValidateError) Error() string { if e.Msg != \"\" { return e.ErrType + \": expr_path=\" + e.FailField + \", cause=\" + e.Msg } return e.ErrType + \": expr_path=\" + e.FailField + \", cause=invalid\" } func init() { CustomBindErrFunc := func(failField, msg string) error { err := BindError{ ErrType: \"bindErr\", FailField: \"[bindFailField]: \" + failField, Msg: \"[bindErrMsg]: \" + msg, } return \u0026err } CustomValidateErrFunc := func(failField, msg string) error { err := ValidateError{ ErrType: \"validateErr\", FailField: \"[validateFailField]: \" + failField, Msg: \"[validateErrMsg]: \" + msg, } return \u0026err } binding.SetErrorFactory(CustomBindErrFunc, CustomValidateErrFunc) } 自定义类型解析 在参数绑定的时候，所有的 request 参数都是 string 或者 []string；当有一些 field 的类型为非基础类型或者无法直接通过 string 转换，则可以自定义类型解析（demo ）。使用方法如下:\nimport \"github.com/cloudwego/hertz/pkg/app/server/binding\" type Nested struct { B string C string } type TestBind struct { A Nested `query:\"a,required\"` } func init() { binding.MustRegTypeUnmarshal(reflect.TypeOf(Nested{}), func(v string, emptyAsZero bool) (reflect.Value, error) { if v == \"\" \u0026\u0026 emptyAsZero { return reflect.ValueOf(Nested{}), nil } val := Nested{ B: v[:5], C: v[5:], } return reflect.ValueOf(val), nil }) } 自定义验证函数 可以通过注册自定义验证函数，在’vd’注解中实现复杂的验证逻辑（demo ），使用方法如下：\nimport \"github.com/cloudwego/hertz/pkg/app/server/binding\" func init() { binding.MustRegValidateFunc(\"test\", func(args ...interface{}) error { if len(args) != 1 { return fmt.Errorf(\"the args must be one\") } s, _ := args[0].(string) if s == \"123\" { return fmt.Errorf(\"the args can not be 123\") } return nil }) } 配置 looseZero 在一些场景下，前端有时候传来的信息只有 key 没有 value，这会导致绑定数值类型的时候，会报错 cause=parameter type does not match binding data。 这时需要配置 looseZero 模式（demo ），使用方法如下：\nimport \"github.com/cloudwego/hertz/pkg/app/server/binding\" func init() { // 默认false，全局生效  binding.SetLooseZeroMode(true) } 配置其他 json unmarshal 库 在绑定参数的时候，如果请求体为 json，会进行一次 json 的 unmarshal，如果用户需要使用特定的 json 库可以自己配置（hertz 默认使用开源 json 库 sonic ）。使用方法如下：\nimport \"github.com/cloudwego/hertz/pkg/app/server/binding\" func init() { // 使用标准库  binding.UseStdJSONUnmarshaler() // 使用gjson  binding.UseGJSONUnmarshaler() // 使用第三方json unmarshal方法  binding.UseThirdPartyJSONUnmarshaler() } 设置默认值 参数支持 “default” tag 进行默认值的配置，使用方法如下：\n// 生成的代码 type UserInfoResponse struct { NickName string `default:\"Hertz\" json:\"NickName\" query:\"nickname\"` } 绑定文件 参数绑定支持绑定文件，使用方法如下：\n// 需要请求的content-type为：multipart/form-data type FileParas struct { F *multipart.FileHeader `form:\"F1\"` } h.POST(\"/upload\", func(ctx context.Context, c *app.RequestContext) { var req FileParas err := binding.BindAndValidate(c, \u0026req) }) 常见问题分析 1. string 转 int 报错：json: cannot unmarshal string into Go struct field xxx of type intxx\n原因：默认不支持 string 和 int 互转\n解决方法：\n 建议使用标准包 json 的 string tag, 例如： A int `json:\"A, string\"`  配置其他支持这种行为的 json 库  ","categories":"","description":"","excerpt":"hertz 使用开源库 go-tagexpr 进行参数的绑定及验证，下面分别介绍参数绑定和参数验证的用法。\n使用方法 func main() …","ref":"/zh/docs/hertz/tutorials/basic-feature/binding-and-validate/","tags":"","title":"绑定与校验"},{"body":"会议主题： CloudWeGo 社区会议 4.8\n参会人员： YangruiEmma, liu-song, yccpt, AshleeT, GuangmingLuo, CoderPoet, HeyJavaBean, jayantxie, JZK-Keven, Xiwen Li, joway, bodhisatan\n会前必读： http://www.cloudwego.io/; https://github.com/cloudwego\n议程 1 ：新成员自我介绍 内容：一位社区新成员进行简要的自我介绍，主要包含个人基本情况、个人未来规划。\n议程 2 ：Kitex 单测任务进展介绍  没有提交 PR 的同学可以尽早提交 PR，便于后续相关同学进行 review。 后期会为大家邮寄礼品。  议程 3 ：Kitex 4月发版计划  发版时间：4 月 28 日，发布中版本。 4 月 20 日前，各变更需完成独立的功能验证和性能测试；如无特殊情况， 4 月 22 日前，完成所有变更合并，然后进入整体的功能验证和性能测试阶段。  议程 4 ：Q\u0026A Q：社区后续工作规划有哪些？\nA：可以参考前期的会议纪要：https://github.com/cloudwego/community/tree/main/meeting_notes ， 以及 Kitex 的 RoadMap: https://github.com/cloudwego/kitex/blob/develop/ROADMAP.md 。 其它的规划还包括：1. 推进 xDS 的对接实现； 2. Kitex 对接支持业界开源服务治理能力和云平台。\n","categories":"","description":"","excerpt":"会议主题： CloudWeGo 社区会议 4.8\n参会人员： YangruiEmma, liu-song, yccpt, AshleeT, …","ref":"/zh/community/meeting_notes/2022-04-08/","tags":"","title":"CloudWeGo 社区会议 4.8"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kitex/tutorials/code-gen/","tags":"","title":"Code Generation"},{"body":"The Kitex framework provides a middleware builder to support adding customized access control that rejects requests under certain conditions. The belowing is a simple example that randomly rejects 1% of all requests:\npackage myaccesscontrol import ( \"math/rand\" \"github.com/cloudwego/kitex/pkg/acl\" ) var errRejected = errors.New(\"1% rejected\") // Implements a judge function. func reject1percent(ctx context.Context, request interface{}) (reason error) { if rand.Intn(100) == 0 { return errRejected // an error should be returned when a request is rejected  } return nil } var MyMiddleware = acl.NewACLMiddleware(reject1percent) // create the middleware Then, you can enable this middleware with WithMiddleware(myaccesscontrol.MyMiddleware) at the creation of a client or a server.\n","categories":"","description":"","excerpt":"The Kitex framework provides a middleware builder to support adding …","ref":"/docs/kitex/tutorials/service-governance/access_control/","tags":"","title":"Customized Access Control"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kitex/reference/","tags":"","title":"Reference"},{"body":"kitex-contrib has provided the DNS service discovery extensions.\nIf you want to adopt other service discovery protocol, such as ETCD, you can implement the Resolver interface, and clients can inject it by WithResolver Option.\nInterface Definition The interface is defined in pkg/discovery/discovery.go and is defined as follows:\ntype Resolver interface { Target(ctx context.Context, target rpcinfo.EndpointInfo) string Resolve(ctx context.Context, key string) (Result, error) Diff(key string, prev, next Result) (Change, bool) Name() string } type Result struct { Cacheable bool // if can be cached  CacheKey string // the unique key of cached result  Instances []Instance // the result of service discovery } // the diff result type Change struct { Result Result Added []Instance Updated []Instance Removed []Instance } Resolver interface detail:\n Resolve: as the core method of Resolver, it obtains the service discovery result from target key Target: it resolves the unique target endpoint that from the downstream endpoints provided by Resolve, and the result will be used as the unique key of the cache Diff: it is used to compare the discovery results with the last time. The differences in results are used to notify other components, such as loadbalancer and circuitbreaker, etc Name: it is used to specify a unique name for Resolver, and will use it to cache and reuse Resolver  Usage Example You need to implement the the Resolver interface, and using it by Option:\nimport ( \"xx/kitex/client\" ) func main() { opt := client.WithResolver(YOUR_RESOLVER) // new client  xxx.NewClient(\"destServiceName\", opt) } Attention To improve performance, Kitex reusing Resolver, so the Resolver method implementation must be concurrent security.\n","categories":"","description":"","excerpt":"kitex-contrib has provided the DNS service discovery extensions.\nIf …","ref":"/docs/kitex/tutorials/framework-exten/service_discovery/","tags":"","title":"Service Discovery Extension"},{"body":"If you want to send the request to downstream that address determined, you can choose visiting directly without service discovery.\nSpecify RPC Host and Port You can use callopt.WithHostPort to specify host and port, supports two parameters:\n Normal IP address, in the form of host:port, support IPv6 Sock file address, communicating with UDS (Unix Domain Socket)  import \"github.com/cloudwego/kitex/client/callopt\" ... resp, err := cli.Echo(context.Background(), req, callopt.WithHostPort(\"127.0.0.1:8888\")) if err != nil { log.Fatal(err) } Specify RPC URL You can use callopt.WithURL to specify a URL, which will be resolved by default DNS resolver to get host and port. It’s functionally equal to callopt.WithHostPort.\nimport \"github.com/cloudwego/kitex/client/callopt\" ... url := callopt.WithURL(\"http://myserverdomain.com:8888\") resp, err := cli.Echo(context.Background(), req, url) if err != nil { log.Fatal(err) } Customized DNS resolver You can also use your own DNS resolver\nresolver interface(pkg/http)：\ntype Resolver interface { Resolve(string) (string, error) } The only parameter is URL，return value should be “host:port”.\nYou can use client.WithHTTPResolver to specify DNS resolver.\nimport \"github.com/cloudwego/kitex/client/callopt\" ... dr := client.WithHTTPResolver(myResolver) cli, err := echo.NewClient(\"echo\", dr) if err != nil { log.Fatal(err) } ","categories":"","description":"","excerpt":"If you want to send the request to downstream that address determined, …","ref":"/docs/kitex/tutorials/basic-feature/visit_directly/","tags":"","title":"Visit Directly"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kitex/reference/","tags":"","title":"参考"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kitex/tutorials/code-gen/","tags":"","title":"代码生成"},{"body":"kitex-contrib 中提供了 DNS 服务发现扩展。\n用户如果需要更换其他的服务发现，例如 ETCD，用户可以根据需求实现 Resolver  接口，client 通过 WithResolver Option 来注入。\n接口定义 接口在 pkg/discovery/discovery.go 中，具体定义如下：\n// 服务发现接口定义 type Resolver interface { Target(ctx context.Context, target rpcinfo.EndpointInfo) string Resolve(ctx context.Context, key string) (Result, error) Diff(key string, prev, next Result) (Change, bool) Name() string } type Result struct { Cacheable bool // 是否可以缓存  CacheKey string // 缓存的唯一 key  Instances []Instance // 服务发现结果 } // diff 的结果 type Change struct { Result Result Added []Instance Updated []Instance Removed []Instance } Resolver 接口定义如下:\n Resolve：作为 Resolver 的核心方法， 从 target key 中获取我们需要的服务发现结果 Result。 Target：从 Kitex 提供的对端 EndpointInfo 中解析出 Resolve 需要使用的唯一 target, 同时这个 target 将作为缓存的唯一 key。 Diff：用于计算两次服务发现的变更， 计算结果一般用于通知其他组件， 如 loadbalancer 和熔断等， 返回的是变更 Change。 Name：用于指定 Resolver 的唯一名称， 同时 Kitex 会用它来缓存和复用 Resolver。  自定义 Resolver 首先需要实现 Resolver 接口需要的方法， 通过配置项指定 Resolver。\nKitex 提供了 Client 初始化配置项 :\nimport ( \"xx/kitex/client\" ) func main() { opt := client.WithResolver(YOUR_RESOLVER) // new client  xxx.NewClient(\"destServiceName\", opt) } 注意事项  我们通过复用 Resolver 的方式来提高性能， 要求 Resolver 的方法实现需要是并发安全的。  ","categories":"","description":"","excerpt":"kitex-contrib 中提供了 DNS 服务发现扩展。\n用户如果需要更换其他的服务发现，例如 ETCD， …","ref":"/zh/docs/kitex/tutorials/framework-exten/service_discovery/","tags":"","title":"服务发现扩展"},{"body":"在明确要访问某个下游地址时，可以选择直连访问的方式，不需要经过服务发现。\n指定 IP 和 Port 进行调用 在进行调用时，可以通过 callopt.WithHostPort 指定，支持两种参数:\n 普通 IP 地址，形式为 “host:port”，支持 IPv6 sock 文件地址，通过 UDS (Unix Domain Socket) 通信  import \"github.com/cloudwego/kitex/client/callopt\" ... resp, err := cli.Echo(context.Background(), req, callopt.WithHostPort(\"127.0.0.1:8888\")) if err != nil { log.Fatal(err) } 指定 URL 进行调用 在进行调用时，可以通过 callopt.WithURL 指定，通过该 option 指定的 URL，会经过默认的 DNS resolver 解析后拿到 host 和 port，此时其等效于 callopt.WithHostPort。\nimport \"github.com/cloudwego/kitex/client/callopt\" ... url := callopt.WithURL(\"http://myserverdomain.com:8888\") resp, err := cli.Echo(context.Background(), req, url) if err != nil { log.Fatal(err) } 自定义 DNS resolver 此外也可以自定义 DNS resolver\nresolver 定义如下 (pkg/http)：\ntype Resolver interface { Resolve(string) (string, error) } 参数为 URL，返回值为访问的 server 的 “host:port”。\n通过 client.WithHTTPResolver 指定用于 DNS 解析的 resolver。\nimport \"github.com/cloudwego/kitex/client/callopt\" ... dr := client.WithHTTPResolver(myResolver) cli, err := echo.NewClient(\"echo\", dr) if err != nil { log.Fatal(err) } ","categories":"","description":"","excerpt":"在明确要访问某个下游地址时，可以选择直连访问的方式，不需要经过服务发现。\n指定 IP 和 Port 进行调用 在进行调用时， …","ref":"/zh/docs/kitex/tutorials/basic-feature/visit_directly/","tags":"","title":"直连访问"},{"body":"Kitex 框架提供了一个简单的中间件构造器，可以支持用户自定义访问控制的逻辑，在特定条件下拒绝请求。下面是一个简单的例子，随机拒绝 1% 的请求：\npackage myaccesscontrol import ( \"math/rand\" \"github.com/cloudwego/kitex/pkg/acl\" ) var errRejected = errors.New(\"1% rejected\") // 实现一个判断函数 func reject1percent(ctx context.Context, request interface{}) (reason error) { if rand.Intn(100) == 0 { return errRejected // 拒绝请求时，需要返回一个错误  } return nil } var MyMiddleware = acl.NewACLMiddleware(reject1percent) // 创建中间件 随后，你可以在创建 client 或者 server 的时候，通过 WithMiddleware(myaccesscontrol.MyMiddleware) 启用该中间件。\n","categories":"","description":"","excerpt":"Kitex 框架提供了一个简单的中间件构造器，可以支持用户自定义访问控制的逻辑，在特定条件下拒绝请求。下面是一个简单的例子，随机拒绝 1%  …","ref":"/zh/docs/kitex/tutorials/service-governance/access_control/","tags":"","title":"自定义访问控制"},{"body":"Wrong setting of NumLoops If your server is running on a physical machine, the number of P created by the Go process is equal to the number of CPUs of the machine. But the server may not use so many cores. In this case, too many pollers will cause performance degradation.\nThere are several solutions:\n Use the taskset command to limit CPU usage, such as:  taskset -c 0-3 $run_your_server Actively set the number of P, for instance:  package main import ( \"runtime\" ) func init() { runtime.GOMAXPROCS(num_you_want) } Actively set the number of pollers, e.g:  package main import ( \"github.com/cloudwego/netpoll\" ) func init() { netpoll.SetNumLoops(num_you_want) } ","categories":"","description":"","excerpt":"Wrong setting of NumLoops If your server is running on a physical …","ref":"/docs/netpoll/caution/","tags":"","title":"Caution"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/volo/guide/","tags":"","title":"Guide"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/netpoll/","tags":"","title":"Netpoll"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/netpoll/","tags":"","title":"Netpoll"},{"body":"前言 基于 TTHeader，我们提供了在协议头传递一些元信息的能力。\n尽管这种能力很方便，我们仍然不推荐业务使用这种能力进行大量信息的传递或者作为通用的参数传递方式，原因如下：\n 这并不符合 API 的规范，纯粹是实现强相关的行为； 这样的话，使用 Thrift / Protobuf IDL 定义并约束 RPC API 就没有意义了； TTHeader 对于头部信息总大小有限制（64K），如果略大就可能导致传输直接失败，而如果大量采用这个方法进行传递，早晚有一天会达到上限，并导致所有 RPC 请求均失败； 这种方式性能差，无论是实现上的性能还是传递时每一跳都会额外消耗的资源； 可以想象一下如果\u0008所有业务都通过这种方式来透传字段，未来的可维护性几乎为零，且无法收敛。  方案介绍 针对正向传递（从上游往下游传递），我们提供了两种方式：\n Persistent：这类元信息会沿着调用链路一直向下透传，直到被某一跳中主动删除（不推荐大量使用） Transient：这类元信息仅会透传一跳，也即到当前服务的下游（首选这种方式）  针对反向传递（从下游返回给上游），我们仅提供一种方式：Transient（也就是仅会返回一跳）。我们不认为从最底层的服务一直返回并透传给最上层服务某个元信息是合理的需求和场景。 如果真的有这种需求，请直接把相关字段定义在 IDL 中并显式返回。\n使用 Volo-Thrift 使用这个功能，需要使用 TTHeader 协议；Volo-gRPC 没有额外要求。\n如果你的场景中，是由 Volo-Thrift server 生成的脚手架代码作为入口的，那么你可以直接引入对应的 Trait 并进行获取或者设置：\nusemetainfo::{Backward,Forward};pubstruct S;#[volo::async_trait]implvolo_gen::volo::example::ItemServiceforS{asyncfn get_item(\u0026self,req: volo_gen::volo::example::GetItemRequest,)-\u003e Result\u003cvolo_gen::volo::example::GetItemResponse,pilota::AnyhowError\u003e{metainfo::METAINFO.with(|mi|{letmutmi=mi.borrow_mut();println!(\"{:?}, {:?}\",mi.get_all_persistents(),mi.get_all_upstreams());mi.set_backward_transient(\"test_backward\",\"test_backward_value\");});Ok(Default::default())}}如果你是自己写的 main 函数并使用 client 调用，那么首先你需要把 MetaInfo 塞到 task local 中：\nuselazy_static::lazy_static;usemetainfo::{Backward,Forward,MetaInfo,METAINFO};usestd::{cell::RefCell,net::SocketAddr};usevolo_example::LogLayer;lazy_static!{staticrefCLIENT: volo_gen::volo::example::ItemServiceClient={letaddr: SocketAddr=\"127.0.0.1:8080\".parse().unwrap();volo_gen::volo::example::ItemServiceClientBuilder::new(\"volo-example-item\").layer_inner(LogLayer).address(addr).build()};}#[volo::main]asyncfn main(){letmutmi=MetaInfo::new();mi.set_persistent(\"test_persistent_key\",\"test_persistent\");mi.set_transient(\"test_transient_key\",\"test_transient\");METAINFO.scope(RefCell::new(mi),asyncmove{letreq=volo_gen::volo::example::GetItemRequest{id: 1024};letresp=CLIENT.clone().get_item(req).await;matchresp{Ok(info)=\u003etracing::info!(\"{:?}\",info),Err(e)=\u003etracing::error!(\"{:?}\",e),}METAINFO.with(|mi|{println!(\"{:?}\",mi.borrow().get_all_backward_downstreams());})}).await;}当然，如果你的 client 调用是从 server 接收到请求之后进行的，那么不用自己进行 metainfo task local 的设置，Volo-Thrift 框架会自动帮你设置好这些值。\n总结 简单来说，步骤分为以下几步：\n 确定你当前有 METAINFO 的 task local（server 的方法中默认就有，不需要自己创建）； 引入 metainfo 中的 trait； 调用对应的方法设置、获取值。  第三方框架接入 如果想与其它的框架（如 HTTP 框架）集成元信息传递功能，只需要遵守 metainfo 包中定义的 header 常量字符串即可。\nCloudWeGo 开源的 Kitex、Hertz 框架默认支持也支持元信息传递的规范。\n","categories":"","description":"","excerpt":"前言 基于 TTHeader，我们提供了在协议头传递一些元信息的能力。\n尽管这种能力很方便，我们仍然不推荐业务使用这种能力进行大量信息的传递 …","ref":"/zh/docs/volo/guide/metadata/","tags":"","title":"元信息传递"},{"body":"内存使用率高 客户端不规范使用没有关连接 如果 Client 侧发起大量连接而不关闭的话，极端情况下会有较大的资源浪费，随着时间的增长，可能会造成内存使用率高的问题。\n解决办法\n合理配置 idleTimeout，超时后 Hertz Server 会把连接关掉保证 Server 侧的稳定性。默认配置为3分钟。\n超大请求/响应  如果请求和响应非常大，并且没有使用一些其他发送模式如 stream、chunk 时，数据会全部进入内存，给内存造成较大压力。 netpoll 网络库下的流式为假流式。由于 netpoll 使用 LT 触发模式，当数据到达时，会触发 netpoll 读取数据；在接口设计上，也因此没有实现 Reader 接口。为了实现流式的能力，Hertz 将 netpoll 封装为 Reader，但其本身数据仍然不可控的进入了内存，所以在超大流式请求的情况下，可能会造成内存压力。  解决办法\n超大请求的场景下，使用流式 + go net 的组合。\n常见错误码排查 如果框架报以下的错误码，可以按照可能原因进行排查。如果出现非以下错误码，则不是框架打出来的，需要由使用方定位一下是否自行设置或者由某些中间件设置了错误码。\n404  访问到了错误的端口上了，常见访问到了 debug 端口  解决方案：区分框架服务的监听端口和 debug server 的监听端口，默认 xxx   未匹配到路由  根据启动日志查看是否所有预期路由都正常注册 查看访问方法是否正确 查看某些配置项是否开启，如 xxx    417 server 在执行完自定义的 ContinueHandler 之后返回 false（server 主动拒绝掉 100 Continue 后续的 body）。\n500  中间件或者 handlerFunc 中抛 panic  解决方案：panic 栈信息定位具体问题   fs 场景 path 携带 /../，可能出现访问预期之外的文件，server 端 app log 中伴随错误日志：cannot serve path with '/../' at position %d due to security reasons: %q。  解决方案：检查是否存在非法请求    上下文使用指南 说明 Hertz 在 HandlerFunc 设计上，同时提供了一个标准 context.Context 和一个请求上下文作为函数的入参。 handler/middleware 函数签名为：\ntype HandlerFunc func(c context.Context, ctx *RequestContext) 元数据存储方面 两个上下文都有储值能力，使用时具体选择哪一个的简单依据：所储存值的生命周期和所选择的上下文要匹配。\n具体细节\nctx 主要用来存储请求级别的变量,请求结束就回收了，特点是查询效率高（底层是 map），协程不安全，且未实现 context.Context 接口。 c 作为上下文在中间件 /handler 之间传递。拥有 context.Context 的所有语义，协程安全。所有需要 context.Context 接口作为入参的地方，直接传递 c 即可。\n除此之外，如果面对一定要异步传递 ctx 的场景，hertz 也提供了 ctx.Copy() 接口，方便业务能够获取到一个协程安全的副本。\n","categories":"","description":"","excerpt":"内存使用率高 客户端不规范使用没有关连接 如果 Client 侧发起大量连接而不关闭的话，极端情况下会有较大的资源浪费，随着时间的增长，可能 …","ref":"/zh/docs/hertz/faq/","tags":"","title":"常见问题"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/hertz/tutorials/framework-exten/","tags":"","title":"框架扩展"},{"body":"错误设置 NumLoops 如果你的服务器运行在物理机上，Go 进程创建的 P 个数就等于机器的 CPU 核心数。 但是 Server 可能不会使用这么多核心。在这种情况下，过多的 poller 会导致性能下降。\n这里提供了以下几种解决方案：\n 使用 taskset 命令来限制 CPU 个数，例如：  taskset -c 0-3 $run_your_server 主动设置 P 的个数，例如：  package main import ( \"runtime\" ) func init() { runtime.GOMAXPROCS(num_you_want) } 主动设置 poller 的个数，例如：  package main import ( \"github.com/cloudwego/netpoll\" ) func init() { netpoll.SetNumLoops(num_you_want) } ","categories":"","description":"","excerpt":"错误设置 NumLoops 如果你的服务器运行在物理机上，Go 进程创建的 P 个数就等于机器的 CPU 核心数。 但是 Server 可能 …","ref":"/zh/docs/netpoll/caution/","tags":"","title":"注意事项"},{"body":"接下来，让我们来看下如何给 volo 添加一个中间件。\n例如，我们需要一个中间件，打印出我们收到的请求、返回的响应以及消耗的时间，那我们可以在 lib.rs 中写这么一个 Service：\n#[derive(Clone)]pubstruct LogService\u003cS\u003e(S);#[volo::service]impl\u003cCx,Req,S\u003evolo::Service\u003cCx,Req\u003eforLogService\u003cS\u003ewhereReq: Send +'static,S: Send +'static+volo::Service\u003cCx,Req\u003e,Cx: Send +'static,{asyncfn call(\u0026mutself,cx: \u0026mutCx,req: Req)-\u003e Result\u003cS::Response,S::Error\u003e{letnow=std::time::Instant::now();letresp=self.0.call(cx,req).await;tracing::info!(\"Request took {}ms\",now.elapsed().as_millis());resp}}随后，我们给这个 Service 包装一层 Layer：\npubstruct LogLayer;impl\u003cS\u003evolo::Layer\u003cS\u003eforLogLayer{type Service=LogService\u003cS\u003e;fn layer(self,inner: S)-\u003e Self::Service{LogService(inner)}}最后，我们在 client 和 server 里面加一下这个 Layer：\nusevolo_example::LogLayer;// client.rs staticrefCLIENT: volo_gen::volo::example::ItemServiceClient={letaddr: SocketAddr=\"127.0.0.1:8080\".parse().unwrap();volo_gen::volo::example::ItemServiceClientBuilder::new(\"volo-example\").layer(LogLayer).target(addr).build()};// server.rs volo_gen::volo::example::ItemServiceServer::new(S).layer(LogLayer).run(addr).await.unwrap();这时候，在 info 日志级别下，我们会打印出请求的耗时。\n","categories":"","description":"","excerpt":"接下来，让我们来看下如何给 volo 添加一个中间件。\n例如，我们需要一个中间件，打印出我们收到的请求、返回的响应以及消耗的时间，那我们可以 …","ref":"/zh/docs/volo/volo-grpc/getting-started/part_4/","tags":"","title":"Part 4. 添加一个中间件"},{"body":"接下来，让我们来看下如何给 volo 添加一个中间件。\n例如，我们需要一个中间件，打印出我们收到的请求、返回的响应以及消耗的时间，那我们可以在 lib.rs 中写这么一个 Service：\n#[derive(Clone)]pubstruct LogService\u003cS\u003e(S);#[volo::service]impl\u003cCx,Req,S\u003evolo::Service\u003cCx,Req\u003eforLogService\u003cS\u003ewhereReq: std::fmt::Debug+Send+'static,S: Send +'static+volo::Service\u003cCx,Req\u003e,S::Response: std::fmt::Debug,S::Error: std::fmt::Debug,Cx: Send +'static,{asyncfn call(\u0026mutself,cx: \u0026mutCx,req: Req)-\u003e Result\u003cS::Response,S::Error\u003e{letnow=std::time::Instant::now();tracing::debug!(\"Received request {:?}\",\u0026req);letresp=self.0.call(cx,req).await;tracing::debug!(\"Sent response {:?}\",\u0026resp);tracing::info!(\"Request took {}ms\",now.elapsed().as_millis());resp}}随后，我们给这个 Service 包装一层 Layer：\npubstruct LogLayer;impl\u003cS\u003evolo::Layer\u003cS\u003eforLogLayer{type Service=LogService\u003cS\u003e;fn layer(self,inner: S)-\u003e Self::Service{LogService(inner)}}最后，我们在 client 和 server 里面加一下这个 Layer：\nusevolo_example::LogLayer;// client.rs staticrefCLIENT: volo_gen::volo::example::ItemServiceClient={letaddr: SocketAddr=\"127.0.0.1:8080\".parse().unwrap();volo_gen::volo::example::ItemServiceClientBuilder::new(\"volo-example\").layer_inner(LogLayer).target(addr).build()};// server.rs volo_gen::volo::example::ItemServiceServer::new(S).layer(LogLayer).run(addr).await.unwrap();这时候，在 info 日志级别下，我们会打印出请求的耗时；在 debug 日志级别下，我们还会打出请求和响应的详细数据。\n","categories":"","description":"","excerpt":"接下来，让我们来看下如何给 volo 添加一个中间件。\n例如，我们需要一个中间件，打印出我们收到的请求、返回的响应以及消耗的时间，那我们可以 …","ref":"/zh/docs/volo/volo-thrift/getting-started/part_4/","tags":"","title":"Part 4. 添加一个中间件"},{"body":"Hertz 提供了国际化 (i18n) 的中间件扩展 ，它参考了 Gin 的实现 。\n使用方法可参考如下 example\npackage main import ( \"context\" \"github.com/cloudwego/hertz/pkg/app\" \"github.com/cloudwego/hertz/pkg/app/server\" hertzI18n \"github.com/hertz-contrib/i18n\" \"github.com/nicksnyder/go-i18n/v2/i18n\" \"golang.org/x/text/language\" \"gopkg.in/yaml.v3\" ) func main() { h := server.New(server.WithHostPorts(\":3000\")) h.Use(hertzI18n.Localize( hertzI18n.WithBundle(\u0026hertzI18n.BundleCfg{ RootPath: \"./localize\", AcceptLanguage: []language.Tag{language.Chinese, language.English}, DefaultLanguage: language.Chinese, FormatBundleFile: \"yaml\", UnmarshalFunc: yaml.Unmarshal, }), hertzI18n.WithGetLangHandle(func(c context.Context, ctx *app.RequestContext, defaultLang string) string { lang := ctx.Query(\"lang\") if lang == \"\" { return defaultLang } return lang }), )) h.GET(\"/:name\", func(c context.Context, ctx *app.RequestContext) { ctx.String(200, hertzI18n.MustGetMessage(\u0026i18n.LocalizeConfig{ MessageID: \"welcomeWithName\", TemplateData: map[string]string{ \"name\": ctx.Param(\"name\"), }, })) }) h.GET(\"/\", func(c context.Context, ctx *app.RequestContext) { ctx.String(200, hertzI18n.MustGetMessage(\"welcome\")) }) h.Spin() } 更多用法示例详见 i18n\n","categories":"","description":"","excerpt":"Hertz 提供了国际化 (i18n) 的中间件扩展 ，它参考了 Gin 的实现 。\n使用方法可参考如下 example\npackage …","ref":"/zh/docs/hertz/tutorials/basic-feature/middleware/i18n/","tags":"","title":"国际化"},{"body":"Kitex provides a global Hook for injecting your processing logic on the server-side after triggering startup and before exiting.\nAlso, you can modify the main function to customize some processing logic before the server-side starts and after it exits (listening for port closures).\nAfter the server-side start and before exit Since the server-side logic after startup and before exit is processed inside the framework, users who want to customize the business logic need to use Hook for injection.\nInject StartHook triggered after startup After triggering Server startup, the framework executes StartHooks and performs service registration. Note that since the Server starts asynchronously, the Hook is not guaranteed to be executed after the fully ready status of the server.\n  Usage\nimport \"github.com/cloudwego/kitex/server\" server.RegisterStartHooks(yourStartHook) // support for injecting multiple Hooks // server.RegisterStartHooks(hook)   Inject ShutdownHook triggered before exit After receiving an exit signal or when the user initiates a stop command, the framework executes ShutdownHooks first, followed by service deregistration (from the registry) and Shutdown of the service.\n Usage  import \"github.com/cloudwego/kitex/server\" server.RegisterShutdownHook(yourShundownHook) // support inject multiple Hooks // server.RegisterShutdownHook(hook) Before the server-side startup and after exit Customizing the logic before the server-side startup and after exit is easier for users. What you need to do is adding your processing logic before and after the execution of the Run() method in the main.go generated by the framework.\nNote: The logic after Run() is not executed until the Server has finished exiting. If you want to execute your logic before Server exits, you should use ShutdownHook.\nThe following is an example of the main function generated by the framework, you may write your own logic in the position of comments.\nfunc main() { svr := greetservice.NewServer(new(GreetServiceImpl)) // yourLogicBeforeServerRun()  err := svr.Run() // yourLogicAfterServerRun()  if err != nil { log.Println(err.Error()) } } ","categories":"","description":"","excerpt":"Kitex provides a global Hook for injecting your processing logic on …","ref":"/docs/kitex/tutorials/advanced-feature/start_shutdown_hook/","tags":"","title":"Customize Hook for Start/Shutdown of Server-side"},{"body":"Hertz supports both server and client stream processing to improve the usability of the framework.\nServer For example:\nimport ( \"github.com/cloudwego/hertz/pkg/app/server\" \"github.com/cloudwego/hertz/pkg/network/standard\" ) func main() { h := server.New( server.WithStreamBody(), server.WithTransport(standard.NewTransporter), ) ... } Due to different trigger modes of netpoll and go net, netpoll streams are “pseudo” streams (Due to Level Triggered in epoll, the network library will read the data into the buffer of the network library).In the case of processing large data packets (eg: updating files), there may be memory problems when using netpoll, so we recommend using the go net method above.\nClient For example:\nc, err := client.NewClient(client.WithResponseBodyStream(true)) Since the client has the problem of multiplexing connections, if streaming is used, the connection will be handled by the user(resp.BodyStream() is encapsulated by connection) once streaming is used. There are some differences in the management of connections in the above case:\n  If the user doesn’t close the connection, the connection will eventually be closed by the GC without causing a connection leak. However, due to the need to wait for 2 Round-Trip Time to close the connection, in the case of high concurrency, the consequence is that there will be too many open files and creating a new connection will be impossible.\n  Users can recycle the connection by calling the relevant interface. After recycling, the connection will be put into the connection pool for reuse, so as to achieve higher resource utilization and better performance. The following methods will recycle the connection. Warning: Recycling can only be done once\n Show call: protocol.ReleaseResponse(), resp.Reset(), resp.ResetBody() Implicit call: The server side will also recycle the response. Assign the client side response to the server side or pass the server side response to the client (eg: client uses reverse proxy), there is no need to display the method of calling the recovery.    ","categories":"","description":"","excerpt":"Hertz supports both server and client stream processing to improve the …","ref":"/docs/hertz/tutorials/basic-feature/stream/","tags":"","title":"Stream"},{"body":"Kitex 提供了全局的 Hook 注入能力，用于在服务端触发启动后和退出前注入自己的处理逻辑。\n同时，你也可以修改启动 main 方法在服务启动前和退出后（监听端口关闭）定制一些业务逻辑。\n服务启动后和退出前 由于服务端启动后和退出前都在框架内部处理，用户如果想定制业务逻辑需要通过 Hook 注入。\n注入触发启动后的 StartHook 触发 Server 启动后，框架会执行 StartHooks，然后进行服务注册。注意，由于 Server 启动是异步执行，所以该 Hook 的执行不保证在 Server 完全就绪后。\n  使用方式\nimport \"github.com/cloudwego/kitex/server\" server.RegisterStartHooks(yourStartHook) // 支持注入多个 Hook // server.RegisterStartHooks(hook)   注入退出前的 ShutdownHook 接收到退出信号后或用户主动通过 Stop 退出时，框架会先执行 ShutdownHooks，然后执行服务注销（从注册中心注销）和服务的Shutdown。\n  使用方式\nimport \"github.com/cloudwego/kitex/server\" server.RegisterShutdownHook(yourShundownHook) // support inject multiple Hooks // server.RegisterShutdownHook(hook)   服务启动前和退出后 服务启动前和退出后的定制逻辑会更简单些，可以完全由用户自行控制，只需在框架生成的 main.go 中 Run() 方法执行前后添加你的逻辑即可。\n注意：Run() 后面是在 Server 完成退出后才会执行，如果希望在 Server 退出前执行你的逻辑应该使用 ShutdownHook。\n如下是框架生成的 main 方法示例，注释的位置写入你的逻辑：\nfunc main() { svr := greetservice.NewServer(new(GreetServiceImpl)) // yourLogicBeforeServerRun()  err := svr.Run() // yourLogicAfterServerRun()  if err != nil { log.Println(err.Error()) } } ","categories":"","description":"","excerpt":"Kitex 提供了全局的 Hook 注入能力，用于在服务端触发启动后和退出前注入自己的处理逻辑。\n同时，你也可以修改启动 main 方法在服 …","ref":"/zh/docs/kitex/tutorials/advanced-feature/start_shutdown_hook/","tags":"","title":"服务端 启动/退出 前后定制业务逻辑"},{"body":"Hertz 同时支持 Server 和 Client 的流式处理，提高框架的可用性。\nServer 开启流式:\nimport ( \"github.com/cloudwego/hertz/pkg/app/server\" \"github.com/cloudwego/hertz/pkg/network/standard\" ) func main() { h := server.New( server.WithStreamBody(), server.WithTransport(standard.NewTransporter), ) ... } 由于 netpoll 和 go net 触发模式不同，netpoll 流式为“伪”流式（由于 LT 触发，会由网络库将数据读取到网络库的 buffer 中），在大包的场景下（如：上传文件等）可能会有内存问题，推荐使用 go net，使用方式如上。\nClient 开启流式:\nc, err := client.NewClient(client.WithResponseBodyStream(true)) 由于 client 有复用连接的问题，但是如果使用了流式，那连接就会交由用户处理( resp.BodyStream() 底层是对 connection 的封装)，这个时候对连接的管理会有一些不同：\n 如果用户不关闭连接，连接最终会被 GC 关掉，不会造成连接泄漏。但是，由于关闭连接需要等待 2RTT，在高并发情况下可能会出现 fd 被打满导致无法新建连接的情况。 用户可以调用相关接口回收连接，回收后，该连接会放入连接池中复用，资源使用率更好，性能更高。以下几种方式都会回收连接，注意回收只能回收一次。  显示调用 protocol.ReleaseResponse(), resp.Reset(), resp.ResetBody() 非显示调用：server 侧也会有回收 resp 的逻辑。如果将 client 的 response 赋值给 server或者直接把 server 的 response 传递给 client 的情况下（如： client 作为反向代理 ）就不需要显示调用回收的方法了。    ","categories":"","description":"","excerpt":"Hertz 同时支持 Server 和 Client 的流式处理，提高框架的可用性。\nServer 开启流式:\nimport ( …","ref":"/zh/docs/hertz/tutorials/basic-feature/stream/","tags":"","title":"流式处理"},{"body":"会议主题： CloudWeGo 社区会议 4.21\n参会人员： YangruiEmma, liu-song, baiyutang, yccpt, AshleeT, CoderPoet, Quan Hu, li-jin-gou, JZK-Keven, EastHorse, GuangmingLuo, HeyJavaBean, jayantxie, ppzqh, Shizheng Hou, andrewshan, simon0-o, yiyun, Wanqi Su, Zheming Li, Xianjie Yao, LoveScotty.\n会前必读： http://www.cloudwego.io/; https://github.com/cloudwego\n议程 1 ：新成员自我介绍 内容：社区新成员分别进行自我介绍，主要包含个人基本情况、开源贡献经历和后续参与社区规划。\n议程 2 ：Kitex 单测任务回顾、总结、建议 (@GuangmingLuo 负责介绍)\n  完成进度： 2/14\n  提交 PR 注意事项 ：\na. 写 PR 描述：直接写 Issue 的编号即可，不要写 Fix 和 Resolve。\nb. 提交 PR 前：完成本地 Coding 后，运行gofumpt -l -extra -w .，检测代码是否存在语法、 License等问题。Contributor 在本地 Fix 这些问题后，能够顺利通过 CI 流程检测，顺利进入 review 环节。\nc. 提交 PR 时：若 PR 处于 Working In Progress 状态，大家在提交 PR 时可以选择 Draft PR ，或者在 PR 描述中加入 WIP 标识，便于 Reviewer 优先处理完成状态的 PR。\nd. 单测描述：需要在描述部分清晰、详细说明单测方法的场景，便于后续快速、准确地 Review 代码逻辑。\n  后续安排： 将完成的 PR 正式地 Release 在 Kitex V0.3.0 中，并在 Release Notes 中公示。\n  议程 3：Kitex 源码解析活动介绍与讨论 （@baiyutang 负责介绍）\n  介绍： 源码解析活动方案草案主要包含六大模块：学习资料、设计理念、目标、课题、解读思路、产出形式。\na. “目标”：① 作为 Contributor 的学习产出；② 作为框架新人的学习资料；③ 丰富社区资源和内容，提高 Kitex 的知名度。\nb. “课题”：包括 Kitex 模块设计及调用链路、服务治理、框架公共模块等内容。\nc. “解读思路”：包括代码设计现状、设计背景、Q\u0026A、最佳实践等内容。\n  后续安排： 确定源码解读的优先级，继续补充、优化文档内容。\n  议程 4：Kitex 与阿里云 Nacos + Trace 对接工作进展介绍与讨论 （@li-jin-gou 负责介绍）\n  背景介绍： 我们希望将 Demo(Easy-Note) 部署到阿里云，主要的工作是验证 Kitex 接入阿里云的 ARMS 的链路追踪 和 MSE 的 Nacos 注册中心。\n  工作开展： 目前主要进行了 Kitex Nacos 扩展改造，包括改造初始化方式、设置环境变量和默认值、自定义 Logger 注入等。\n  后续安排： 相关实践文档在完成正式验证和完善后，将通过官方渠道，对外发布。\n  议程 5：Kitex 对接开源服务治理 SDK 方案介绍与讨论 （@jayantxie 负责介绍)\n  背景介绍： 为方便 Kitex 用户上云，计划对接腾讯的开源服务治理平台 Polaris，通过集成 go sdk，满足诸如熔断限流和动态路由等 Polaris 平台的治理能力。\n  方案介绍： 目前存在两种方案设计，二者之间的区别主要体现在接入方式的不同。由于我们需要结合两个框架，此时必然会有一个框架的接口需要被调整改动。其中，方案一，倾向于保留 Kitex 现有框架的设计；方案二，倾向于保留 Polaris 的接口。经过讨论，决定采用方案一，详见 Issue。\n  议程 6：社区建议 大家围绕源码解析文章的收集、发布形式展开了讨论。\n  收集形式： ① 建议将其以“ RPC 框架学习百科全书”，或者“框架学习指南”的形式，作为开源活动放在社区里，由大家进行内容补充；② 也可以以任务认领的形式发布社区，邀请社区成员参与源码解析。\n  发布形式： ① 可以考虑通过公众号宣传；② 也可以发布在 CloudWeGo 官网 和 Github 的 Wiki 里面。\n  议程 7：Q\u0026A Q：我们是否也要把对接阿里云的相关基础设施统一放在一个 Suite 里面？\nA：目前还没有对接服务治理，我们的规划是：第一阶段支持注册中心和可观测系统的接入。比如，对Nacos 注册中心这块做了一些无侵入式配置的扩展对接，然后我们通过 OpenTelemetry 去接入阿里云的可观测系统；第二阶段，我们准备通过 Middleware 或 Suite 的方式对接开放服务治理的能力。现阶段的工作主要是对 Kitex 的 Nacos registry 展开了优化，然后结合 OpenTelemetry 这一扩展，去重构 Kitex Easy-Note Demo。\nQ：单测每次 CI 都会出报告吗？上次想加到 awesome go ，但是他们对覆盖度有要求。\nA：可以点进 CI 的 Show all checks 查看测试报告，同时，在单测新手任务完成之后，项目整体的单测覆盖率提高之后，我们也可以去设置覆盖率的门禁，后续 CI 检测会去检查单测覆盖率，并且输出到 PR 评论中。\n相关资讯 截至 4 月 21 日，历时 5 个月，CloudWeGo-Kitex 完成了 3000 Stars 到 4000 Stars 的跨越，来到新的里程碑！\n","categories":"","description":"","excerpt":"会议主题： CloudWeGo 社区会议 4.21\n参会人员： YangruiEmma, liu-song, baiyutang, …","ref":"/zh/community/meeting_notes/2022-04-21/","tags":"","title":"CloudWeGo 社区会议 4.21"},{"body":"Kitex provides Short Connection, Long Connection Pool and Connection Multiplexing for different business scenarios. Kitex uses Long Connection Pool by default after v0.0.2, but adjusting the Pool Config according to your need is suggested.\nShort Connection Every request needs to create a connection, the performance is bad, so it is not suggested normally.\nEnable Short Connection：\nxxxCli := xxxservice.NewClient(\"destServiceName\", client.WithShortConnection()) Long Connection Pool Kitex enable Long Connection Pool after \u003e= v0.0.2, default config params are as below：\nconnpool2.IdleConfig{ MaxIdlePerAddress: 10, MaxIdleGlobal: 100, MaxIdleTimeout: time.Minute, } Adjusting the Pool Config according to your need is suggested, config as below:\nxxxCli := xxxservice.NewClient(\"destServiceName\", client.WithLongConnection(connpool.IdleConfig{10, 1000, time.Minute})) Parameter description:\n MaxIdlePerAddress: the maximum number of idle connections per downstream instance MaxIdleGlobal: the global maximum number of idle connections MaxIdleTimeout: the idle duration of the connection. A connection that has been idle for more than MaxIdleTimeout will be closed (minimum value is 3s, the default value is 30s)  Internal Implementation Each downstream address corresponds to a connection pool, the connection pool is a ring composed of connections, and the size of the ring is MaxIdlePerAddress.\nWhen getting a connection of downstream address, proceed as follows:\n Try to fetch a connection from the ring, if fetching failed (no idle connections remained), then try to establish a new connection. In other words, the number of connections may exceed MaxIdlePerAddress If fetching succeed, then check whether the idle time of the connection (since the last time it was placed in the connection pool) has exceeded MaxIdleTimeout. If yes, this connection will be closed and a new connection will be created.  When the connection is ready to be returned after used, proceed as follows:\n Check whether the connection is normal, if not, close it directly Check whether the idle connection number exceeds MaxIdleGlobal, and if yes, close it directly Check whether free space remained in the ring of the target connection pool, if yes, put it into the pool, otherwise close it directly  Parameter Setting Suggestion The setting of parameters is suggested as follows:\n MaxIdlePerAddress: the minimum value is 1, otherwise long connections would degenerate to short connections  What value should be set should be determined according to the throughput of downstream address. The approximate estimation formula is: MaxIdlePerAddress = qps_per_dest_host*avg_response_time_sec For example, the cost of each request is 100ms, and the request spread to each downstream address is 100QPS, the value is suggested to set to 10, because each connection handles 10 requests per second, 100QPS requires 10 connections to handle In the actual scenario, the fluctuation of traffic is also necessary to be considered. Pay attention, the connection within MaxIdleTimeout will be recycled if it is not used Summary: this value be set too large or too small would lead to degenerating to the short connection   MaxIdleGlobal: should be larger than the total number of downstream targets number * MaxIdlePerAddress  Notice: this value is not very valuable, it is suggested to set it to a super large value. In subsequent versions, considers discarding this parameter and providing a new interface   MaxIdleTimeout: since the server will clean up inactive connections within 10min, the client also needs to clean up long-idle connections in time to avoid using invalid connections. This value cannot exceed 10min when the downstream is also a Kitex service  Connection Multiplexing The client invokes the Server only need one connection normally when enabling Connection Multiplexing. Connection Multiplexing not only reduces the number of connections but also performs better than Connection Pool.\nSpecial Note:\n Connection Multiplexing here is just for Thrift and Kitex Protobuf protocol. If you choose the gRPC protocol, it utilizes Connection Multiplexing mode by default. When the client enables connection multiplexing, the server must also be enabled, otherwise, it will lead to request timeout. The server side has no restrictions on the client to enable connection multiplexing, it can accept requests for short connection, long connection pool, and connection multiplexing.    Server Side Enable:\noption: WithMuxTransport\nsvr := xxxservice.NewServer(handler, server.WithMuxTransport())   Client Side Enable: option: WithMuxConnection\n1-2 connection is enough normally, it is no need to config more.\nxxxCli := NewClient(\"destServiceName\", client.WithMuxConnection(1))    Status Monitoring Connection pooling defines the Reporter interface for connection pool status monitoring, such as the reuse rate of long connections. Users should implement the interface themselves and inject it by SetReporter.\n// Reporter report status of the connection pool. type Reporter interface { ConnSucceed(poolType ConnectionPoolType, serviceName string, addr net.Addr) ConnFailed(poolType ConnectionPoolType, serviceName string, addr net.Addr) ReuseSucceed(poolType ConnectionPoolType, serviceName string, addr net.Addr) } // SetReporter set the common reporter of a connection pool, that can only be set once. func SetReporter(r Reporter) ","categories":"","description":"","excerpt":"Kitex provides Short Connection, Long Connection Pool and Connection …","ref":"/docs/kitex/tutorials/basic-feature/connection_type/","tags":"","title":"Connection Type"},{"body":"Kitex 支持短连接、长连接池、连接多路复用，用户可以根据自己的业务场景来选择。\u003e= v0.0.2 默认配置了连接池，但建议用户还是根据实际情况调整连接池的大小。\n短连接 每次请求都会创建一次连接，性能不佳，通常不建议使用。但部分场景必须使用短连接，如上游实例数过多时，会增加下游服务的负担，请根据情况来选择。\n配置短连接：\nxxxCli := xxxservice.NewClient(\"destServiceName\", client.WithShortConnection()) 长连接池 Kitex \u003e= v0.0.2 默认配置了连接池，配置参数如下：\nconnpool2.IdleConfig{ MaxIdlePerAddress: 10, MaxIdleGlobal: 100, MaxIdleTimeout: time.Minute, } 建议用户根据实际情况调整连接池大小，配置方式如下：\nxxxCli := xxxservice.NewClient(\"destServiceName\", client.WithLongConnection(connpool.IdleConfig{10, 1000, time.Minute})) 其中：\n MaxIdlePerAddress 表示每个后端实例可允许的最大闲置连接数 MaxIdleGlobal 表示全局最大闲置连接数 MaxIdleTimeout 表示连接的闲置时长，超过这个时长的连接会被关闭（最小值 3s，默认值 30s ）  实现 长连接池的实现方案是每个 address 对应一个连接池，这个连接池是一个由连接构成的 ring，ring 的大小为 MaxIdlePerAddress。\n当选择好目标地址并需要获取一个连接时，按以下步骤处理 :\n 首先尝试从这个 ring 中获取，如果获取失败（没有空闲连接），则发起新的连接建立请求，即连接数量可能会超过 MaxIdlePerAddress 如果从 ring 中获取成功，则检查该连接的空闲时间（自上次放入连接池后）是否超过了 MaxIdleTimeout，如果超过则关闭该连接并新建 全部成功后返回给上层使用  在连接使用完毕准备归还时，按以下步骤依次处理：\n 检查连接是否正常，如果不正常则直接关闭 查看空闲连接是否超过全局的 MaxIdleGlobal，如果超过则直接关闭 待归还到的连接池的 ring 中是否还有空闲空间，如果有则直接放入，否则直接关闭  参数设置建议 下面是参数设置的一些建议：\n MaxIdlePerAddress 表示池化的连接数量，最小为 1，否则长连接会退化为短连接  具体的值与每个目标地址的吞吐量有关，近似的估算公式为：MaxIdlePerAddress = qps_per_dest_host*avg_response_time_sec  举例如下，假设每个请求的响应时间为 100ms，平摊到每个下游地址的请求为 100QPS，该值建议设置为10，因为每条连接每秒可以处理 10 个请求, 100QPS 则需要 10 个连接进行处理 在实际场景中，也需要考虑到流量的波动。需要特别注意的是，即 MaxIdleTimeout 内该连接没有被使用则会被回收 总而言之，该值设置过大或者过小，都会导致连接复用率低，长连接退化为短连接   MaxIdleGlobal 表示总的空闲连接数应大于 下游目标总数*MaxIdlePerAddress，超出部分是为了限制未能从连接池中获取连接而主动新建连接的总数量  注意：该值存在的价值不大，建议设置为一个较大的值，在后续版本中考虑废弃该参数并提供新的接口   MaxIdleTimeout 表示连接空闲时间，由于 server 在 10min 内会清理不活跃的连接，因此 client 端也需要及时清理空闲较久的连接，避免使用无效的连接，该值在下游也为 Kitex 时不可超过 10min  连接多路复用 开启连接多路复用，Client 访问 Server 常规只需要1个连接即可，相比连接池极限测试吞吐表现更好（目前的极限测试配置了2个连接），且能大大减少连接数量。\n特别说明：\n 这里的连接多路复用是针对于 Thrift 和 Kitex Protobuf，如果配置 gRPC 协议，默认是连接多路复用。 Client 开启连接多路复用，Server 必须也开启，否则会导致请求超时；Server 开启连接多路复用对 Client 没有限制，可以接受短连接、长连接池、连接多路复用的请求。    Server 配置\noption: WithMuxTransport\nsvr := xxxservice.NewServer(handler, server.WithMuxTransport())   Client 配置 option: WithMuxConnection\n建议配置1-2 个连接\nxxxCli := NewClient(\"destServiceName\", client.WithMuxConnection(1))   状态监控 连接池定义了 Reporter 接口，用于连接池状态监控，例如长连接的复用率。\n如有需求，用户需要自行实现该接口，并通过 SetReporter 注入。\n// Reporter report status of connection pool. type Reporter interface { ConnSucceed(poolType ConnectionPoolType, serviceName string, addr net.Addr) ConnFailed(poolType ConnectionPoolType, serviceName string, addr net.Addr) ReuseSucceed(poolType ConnectionPoolType, serviceName string, addr net.Addr) } // SetReporter set the common reporter of connection pool, that can only be set once. func SetReporter(r Reporter) ","categories":"","description":"","excerpt":"Kitex 支持短连接、长连接池、连接多路复用，用户可以根据自己的业务场景来选择。\u003e= v0.0.2 默认配置了连接池，但建议用户还是根据实 …","ref":"/zh/docs/kitex/tutorials/basic-feature/connection_type/","tags":"","title":"连接类型"},{"body":"Kitex provides two LoadBalancers:\n WeightedRandom ConsistentHash  These two LoadBalancers can cover most of the use cases, but you can also customize your own LoadBalancer if they doesn’t meet your needs.\nInterface Loadbalancer is defined at pkg/loadbalance/loadbalancer.go:\n// Loadbalancer generates pickers for the given service discovery result. type Loadbalancer interface { GetPicker(discovery.Result) Picker // Name should be unique  Name() string } As you see, LoadBalancer gets a Result and generates a Picker for the current request, the Picker is defined as follows:\n// Picker picks an instance for next RPC call. type Picker interface { Next(ctx context.Context, request interface{}) discovery.Instance } In a single rpc request, the selected instance may not be connected and should to be retried, that’s why it’s been designed like that.\nIf there are no more instances to retry, the Next method should return nil.\nThere are another special interface, defined as follows:\n// Rebalancer is a kind of Loadbalancer that performs rebalancing when the result of service discovery changes. type Rebalancer interface { Rebalance(discovery.Change) Delete(discovery.Change) } If LoadBalancer supports Cache, make sure to implement the Rebalancer interface, otherwise the service will not be notified when discovery results changes.\nKitex client will execute the following code during initialization to ensure that the Rebalancer is notified when discovery results changes.\nif rlb, ok := balancer.(loadbalance.Rebalancer); ok \u0026\u0026 bus ! = nil { bus.Watch(discovery.DiscoveryChangeEventName, func(e *event.Event) { change := e.Extra.(*discovery.Change) rlb.Rebalance(*change) }) } Attention  If you are using dynamic service discovery, you should implement caching which can improve performance. If you are using cache, you should be better to implement the Rebalancer interface, otherwise you will not be notified when discovery results changes. LoadBalancer customization is not being supported in the case of Proxy.  Example You can refer to the implementation of WeightedRandom.\n","categories":"","description":"","excerpt":"Kitex provides two LoadBalancers:\n WeightedRandom ConsistentHash …","ref":"/docs/kitex/tutorials/framework-exten/loadbalance/","tags":"","title":"Customize LoadBalancer"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kitex/tutorials/framework-exten/","tags":"","title":"Framework Extension"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kitex/tutorials/framework-exten/","tags":"","title":"框架扩展"},{"body":"在 Kitex 中，提供了两种 LoadBalancer：\n WeightedRandom ConsistentHash  这两种 LoadBalancer 能覆盖绝大多数的应用场景，如果业务有一些 Corner Case 无法覆盖到，可以选择自己自定义 LoadBalancer。\n接口 LoadBalancer 接口在 pkg/loadbalance/loadbalancer.go 中，具体定义如下：\n// Loadbalancer generates pickers for the given service discovery result. type Loadbalancer interface { GetPicker(discovery.Result) Picker // 名称需要唯一  Name() string } 可以看到 LoadBalancer 获取到一个 Result 并且生成一个针对当次请求的 Picker，Picker 定义如下：\n// Picker picks an instance for next RPC call. type Picker interface { Next(ctx context.Context, request interface{}) discovery.Instance } 有可能在一次的 rpc 请求中，所选择的实例连接不上，而要进行重试，所以设计成这样。\n如果说已经没有实例可以重试了，Next 方法应当返回 nil。\n除了以上接口之外，还有一个比较特殊的接口，定义如下：\n// Rebalancer is a kind of Loadbalancer that performs rebalancing when the result of service discovery changes. type Rebalancer interface { Rebalance(discovery.Change) Delete(discovery.Change) } 如果 LoadBalancer 支持 Cache，务必实现 Rebalancer 接口，否则服务发现变更就无法被通知到了。\nKitex client 会在初始化的时候执行以下代码来保证当服务发现变更时，能通知到 Rebalancer：\nif rlb, ok := balancer.(loadbalance.Rebalancer); ok \u0026\u0026 bus != nil { bus.Watch(discovery.DiscoveryChangeEventName, func(e *event.Event) { change := e.Extra.(*discovery.Change) rlb.Rebalance(*change) }) } 注意事项  如果是动态服务发现，那么尽可能实现缓存，可以提高性能； 如果使用了缓存，那么务必实现 Rebalancer 接口，否则当服务发现变更时无法收到通知； 使用了 Proxy 场景下，不支持自定义 LoadBalancer。  Example 可以参考一下 Kitex 默认的 WeightedRandom 实现。\n","categories":"","description":"","excerpt":"在 Kitex 中，提供了两种 LoadBalancer：\n WeightedRandom ConsistentHash …","ref":"/zh/docs/kitex/tutorials/framework-exten/loadbalance/","tags":"","title":"负载均衡扩展"},{"body":"Kitex Framework Q: Does Kitex support Windows？\n Currently Windows is not particularly supported by Kitex. If your development environment is Windows, you are suggested to use WSL2.  Q: Does Kitex support HTTP？\n Kitex does not specifically provide HTTP request support, CloudWeGo is expected to open source HTTP framework “Hertz” in the first half of 2022. For API gateway scenario, Kitex provides a HTTP mapping generic call regarding Thrift and sends the Thrift encoding of the HTTP request to the server.  Q: How to configure multiplexing?\n If you are using Thrift or Kitex Protobuf, to configure multiplexing:  Configure WithMuxTransport() on your server. Configure WithMuxConnection(1) on your client   If you are using gRPC, multiplexing is configured by default.  Q: In the scenario of direct local call, why does the configuration of connection pool not take effect?\n The ip of your local test needs to be 127.0.0.1. For example, “client.WithHostPorts(“127.0.0.1:8888”)”.  Q: What are the differences between “Kitex Protobuf” and “gRPC” protocol?\n Kitex Protobuf is a Protobuf Message Protocol defined by Kitex framework, with similar structure to Thrift. gRPC provides support to gRPC message protocol and enables Kitex to interact with gRPC framework.  Q: Issues regarding Thrift interface compiling, such as “not enough arguments in call to iprot.ReadStructBegin”\n Based on Thrift v0.13, Kitex cannot be upgraded directly, as there is a breaking change in the interface of Apache Thrift v0.14. The reason for such problems could be that a new version of Thrift is pulled during upgrades. The use of -u parameters is not recommended during upgrades, you can run the following command to fix the version: “go mod edit -replace github.com/apache/thrift=github.com/apache/thrift@v0.13.0”  Kitex Code Generation Tool Q:‘not enough arguments’ problem when installing the code generation tool\n Please try: “go mod：GO111MODULE=on go get github.com/cloudwego/kitex/tool/cmd/kitex@latest”  Q: Would code generated by new interface overwrite handler.go\n Generated code under kitex_gen/ will be overwritten. But handler.go of server will not be overwritten, only new methods will be added correspondingly.  Q: Do the templates of the code generator support customization?\n We have no plan to support template customization at present, because it will make the design for parameter passing much more complex. And the plugin mechanism can achieve almost any functionality equivalent to a customized template.  Q: Is it possible for the ‘-type’ argument of the code generator to be determined automatically by IDL filename extension? Can the generator use the module name from a go.mod for ‘-module’ if it can find one, and require a value when there is no go.mod or ambiguity exists?\n Further discussion is needed on ‘-type’. Filename extensions are currently restricted to a few choices, but arbitrary value may be supported in the future. We’ll consider the advice for ‘-module’.  ","categories":"","description":"","excerpt":"Kitex Framework Q: Does Kitex support Windows？\n Currently Windows is …","ref":"/docs/kitex/faq/","tags":"","title":"FAQ"},{"body":"这里列出与 Kitex 有关的常见问题。\nKitex 框架 Q: 支持 Windows 吗？\n 暂时没有针对 Windows 做支持，如果本地开发环境是 Windows 建议使用 WSL2。  Q: 是否支持 HTTP？\n Kitex 不专门提供 HTTP 请求支持，CloudWeGo 后续会开源 HTTP 框架 Hertz，预计开源时间是 2022 上半年。 如果是 API 网关场景，针对 Thrift 提供了 HTTP 映射的泛化调用，Kitex 会将 HTTP 请求做 Thrift 编码发给服务端。  Q: 如何配置开启连接多路复用？\n 如果使用 Thrift 或 Kitex Protobuf ，开启连接多路复用：服务端配置 WithMuxTransport()，调用端配置 WithMuxConnection(1)。 如果使用 gRPC， 默认是连接多路复用。  Q: 本地直连场景下，配置长连接池为什么没有生效？\n 本地测试 ip 需要改成 127.0.0.1，如 client.WithHostPorts(“127.0.0.1:8888”)。  Q: Kitex Protobuf 和 gRPC 协议区别\n Kitex Protobuf 是 Kitex 自定义的 Protobuf 消息协议，协议格式类似 Thrift。 gRPC 是对 gRPC 消息协议的支持，可以与 gRPC 框架互通。  Q: 出现 Thrift 接口编译问题，如 not enough arguments in call to iprot.ReadStructBegin\n Kitex 依赖 Thrift v0.13，因为Apache Thrift v0.14 接口有 breaking change，无法直接升级。出现该问题是拉到了新版本的 Thrift，升级版本时建议不使用 -u 参数，可以执行命令固定版本 go mod edit -replace github.com/apache/thrift=github.com/apache/thrift@v0.13.0  Kitex 代码生成工具 Q: 安装代码生成工具，出现了 ‘not enough arguments’ 问题\n 请开启go mod：GO111MODULE=on go get github.com/cloudwego/kitex/tool/cmd/kitex@latest  Q: 新增接口重新生成代码，是否会覆盖handler.go\n kitex_gen/ 下的生成代码会重新生成覆盖，但服务端的 handler.go 不会覆盖，只会新增对应方法。  Q: 请问目前代码生成工具中的模板是否支持用户自定义?\n 目前没有支持自定义模板的打算，因为传参设计会复杂很多。现在的插件机制完全可以实现任意等价的功能  Q: 代码生成工具中的 –type 是否可以通过 IDL 文件扩展名自动确定, –module 能否默认使用找到的 go.mod 里的模块名，仅在没有 go.mod 或者有歧义的时候才要求指定值？\n -type 需要更多讨论，目前是限制了后缀，但将来可能会支持任意后缀；-module 的这个建议我们会考虑一下。  ","categories":"","description":"","excerpt":"这里列出与 Kitex 有关的常见问题。\nKitex 框架 Q: 支持 Windows 吗？\n 暂时没有针对 Windows 做支持，如果本 …","ref":"/zh/docs/kitex/faq/","tags":"","title":"FAQ"},{"body":"为什么 Client 端中间件使用 Arc？ 细心的你会发现，我们在 Client 端的生成代码中，会把用户传入的 Req 包装成 Arc 再真正执行 volo_thrift 的 client 的 call 方法，而在 server 端则是直接使用 Req。\n这么设计的原因是因为，Client 端相比 Server 端，需要做更多复杂的服务治理逻辑，特别是有一些服务治理的逻辑和 Rust 的所有权模型是相悖的，比如：如果连接失败，那么就换个节点重试；甚至更复杂的超时重试等逻辑。如果我们直接使用 Req，那么当我们第一次执行 inner service 的 call 时，所有权就已经被 move 了，我们没有办法做重试逻辑。\n同时，使用 Arc 还能帮助我们规避在 middleware 下并发访问带来的问题（如做 mirror/diff 等场景），在此不过多赘述。\n并且，client 端本身不应该修改 Req，所以也不需要拿到可变的权限。\n而在 server 端，不会有这么复杂的使用场景，并且最终是要把所有权给到用户的 handler 的，因此 server 端直接使用 Req 所有权即可。\n为什么 volo-cli 生成的代码中要单独分拆成出 volo-gen crate？ 因为 Rust 的编译是以 crate 为单元的，把生成代码单独作为一个 crate 可以更好地利用编译缓存（idl 一般不会经常变动）。\n和 Kitex 的兼容性如何？ Volo 与 Kitex 完全兼容，包括元信息传递等功能。\n","categories":"","description":"","excerpt":"为什么 Client 端中间件使用 Arc？ 细心的你会发现，我们在 Client 端的生成代码中，会把用户传入的 Req 包装成 Arc  …","ref":"/zh/docs/volo/faq/","tags":"","title":"FAQ"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/hertz/tutorials/toolkit/","tags":"","title":"hz command line tool"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/hertz/tutorials/toolkit/","tags":"","title":"hz 命令行工具"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/motore/","tags":"","title":"Motore"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/motore/","tags":"","title":"Motore"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/hertz/reference/","tags":"","title":"Reference"},{"body":"恭喜你，阅读到了这里！\n至此，我们已经基本学会了 Volo 的大部分使用了，可以使用 Volo 来开启我们愉快的 Rust 之旅啦～\n接下来，你可能需要选择合适的组件，组装在一起，和你的系统进行对接。\nVolo 维护的相关生态会集中在：https://github.com/volo-rs 中，我们正在努力打造我们的生态，也非常欢迎大家一起参与～\n如果有比较急缺的组件，也欢迎在官方仓库：https://github.com/cloudwego/volo 的 issue 中提出，我们也会优先支持社区最急缺的组件。\n同时，欢迎加入我们的飞书用户群，交流 Volo 的使用心得～\n  期待你使用 Volo 创造出属于你的独一无二的作品～\n","categories":"","description":"","excerpt":"恭喜你，阅读到了这里！\n至此，我们已经基本学会了 Volo 的大部分使用了，可以使用 Volo 来开启我们愉快的 Rust 之旅啦～\n接下 …","ref":"/zh/docs/volo/volo-grpc/getting-started/part_5/","tags":"","title":"Part 5. What's next?"},{"body":"恭喜你，阅读到了这里！\n至此，我们已经基本学会了 Volo 的大部分使用了，可以使用 Volo 来开启我们愉快的 Rust 之旅啦～\n接下来，你可能需要选择合适的组件，组装在一起，和你的系统进行对接。\nVolo 维护的相关生态会集中在：https://github.com/volo-rs 中，我们正在努力打造我们的生态，也非常欢迎大家一起参与～\n如果有比较急缺的组件，也欢迎在官方仓库：https://github.com/cloudwego/volo 的 issue 中提出，我们也会优先支持社区最急缺的组件。\n同时，欢迎加入我们的飞书用户群，交流 Volo 的使用心得～\n  期待你使用 Volo 创造出属于你的独一无二的作品～\n","categories":"","description":"","excerpt":"恭喜你，阅读到了这里！\n至此，我们已经基本学会了 Volo 的大部分使用了，可以使用 Volo 来开启我们愉快的 Rust 之旅啦～\n接下 …","ref":"/zh/docs/volo/volo-thrift/getting-started/part_5/","tags":"","title":"Part 5. What's next?"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/hertz/reference/","tags":"","title":"参考"},{"body":"Hertz 提供了 Session扩展 ，它参考了 Gin 的 实现 。\n使用方法可参考如下 example\npackage main import ( \"context\" \"github.com/cloudwego/hertz/pkg/app\" \"github.com/cloudwego/hertz/pkg/app/server\" \"github.com/cloudwego/hertz/pkg/common/utils\" \"github.com/hertz-contrib/sessions\" \"github.com/hertz-contrib/sessions/cookie\" ) func main() { h := server.New(server.WithHostPorts(\":8000\")) store := cookie.NewStore([]byte(\"secret\")) h.Use(sessions.Sessions(\"mysession\", store)) h.GET(\"/incr\", func(ctx context.Context, c *app.RequestContext) { session := sessions.Default(c) var count int v := session.Get(\"count\") if v != nil { count = v.(int) count++ } session.Set(\"count\", count) session.Save() c.JSON(200, utils.H{\"count\": count}) }) h.Spin() } 更多用法示例详见 sessions\n","categories":"","description":"","excerpt":"Hertz 提供了 Session扩展 ，它参考了 Gin 的 实现 。\n使用方法可参考如下 example\npackage main …","ref":"/zh/docs/hertz/tutorials/basic-feature/middleware/session/","tags":"","title":"Session扩展"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kitex/tutorials/options/","tags":"","title":"Option"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kitex/tutorials/options/","tags":"","title":"Option"},{"body":"Error type In order to handle errors more efficiently, Hertz has predefined the following error types:\n// Error in binding process ErrorTypeBind ErrorType = 1 \u003c\u003c iota // Error in rendering process ErrorTypeRender // Hertz private errors that business need not be aware ErrorTypePrivate // Hertz public errors that require external perception as opposed to Private ErrorTypePublic // Other Error ErrorTypeAny It is recommended to define the corresponding errors according to the error type.\nRelevant interfaces // shortcut for creating a public *Error from string func NewPublic(err string) *Error { return New(errors.New(err), ErrorTypePublic, nil) } // shortcut for creating a private *Error from string func NewPrivate(err string) *Error { return New(errors.New(err), ErrorTypePrivate, nil) } func New(err error, t ErrorType, meta interface{}) *Error { return \u0026Error{ Err: err, Type: t, Meta: meta, } } ErrorChain In addition to the conventions for error definition, the framework also provides ErrorChain capability. As the name implies, it is easy for the business to bind all errors encountered on a request processing to the error chain, which can facilitate the subsequent (usually in the middleware) unified processing of all errors. The corresponding API is: RequestContext.Error(err), and calling this API will tie the err to the corresponding request context.\nMethod to get all the errors that have been bound by the request context: RequestContext.Errors (ErrorChain). ErrorChain currently provides the following API:\nByType：Return the corresponding sub-error chain by error type Errors：Converting error chains to standard error arrays JSON：Convert all errors to json objects Last： Return the last or latest error String：Show all errors in readable text ","categories":"","description":"","excerpt":"Error type In order to handle errors more efficiently, Hertz has …","ref":"/docs/hertz/tutorials/basic-feature/error-handle/","tags":"","title":"Error Handle"},{"body":"错误类型 为了更高效的处理错误，Hertz 针对错误类型做了如下预定义：\n// binding 过程的错误 ErrorTypeBind ErrorType = 1 \u003c\u003c iota // rendering 过程的错误 ErrorTypeRender // Hertz内部错误，业务无需感知 ErrorTypePrivate // 相对于Private来说，需要外部感知的错误 ErrorTypePublic // 其他错误 ErrorTypeAny 建议按照错误类别定义相应的错误。\n相关接口 // shortcut for creating a public *Error from string func NewPublic(err string) *Error { return New(errors.New(err), ErrorTypePublic, nil) } // shortcut for creating a private *Error from string func NewPrivate(err string) *Error { return New(errors.New(err), ErrorTypePrivate, nil) } func New(err error, t ErrorType, meta interface{}) *Error { return \u0026Error{ Err: err, Type: t, Meta: meta, } } ErrorChain 除了针对错误定义的约定以外，框架同时提供 ErrorChain（错误链）能力。顾名思义，能够方便业务将一次请求处理上所遇到的所有错误绑定到错误链上，可以方便后续（一般是在中间件中）对所有错误进行统一处理。 对应的 API 为：RequestContext.Error(err)，调用该 API 会将 err 绑到对应的请求上下文上之上。\n获取请求上下文已绑定的所有错误的方式：RequestContext.Errors（ErrorChain）。ErrorChain 目前提供的 API：\nByType：按错误类型返回对应的子错误链 Errors：将错误链转换为标准错误数组 JSON：将所有错误转换为json对象 Last： 返回最后（最新）的一个错误 String：可读性强的文本展示所有错误 ","categories":"","description":"","excerpt":"错误类型 为了更高效的处理错误，Hertz 针对错误类型做了如下预定义：\n// binding 过程的错误 ErrorTypeBind …","ref":"/zh/docs/hertz/tutorials/basic-feature/error-handle/","tags":"","title":"错误处理"},{"body":"会议主题：CloudWeGo 社区会议 5.19\n参会人： YangruiEmma, ag9920, Jiang Xuewu, liu-song, Joway, yccpt, Huang Yuting, CoderPoet, li-jin-gou, GuangmingLuo, simon0-o, scotty, yiyun, Authorixy, JZK-Keven, bodhisatan, ppzqh, Jacob953, Ivnszn, cyyolo, debug-LiXiwen, baize\n会前必读： http://www.cloudwego.io/ https://github.com/cloudwego\n录屏链接： https://bytedance.feishu.cn/minutes/obcn3zdn1g46avv887i11ms9?from=from_copylink\n议程 1 ：社区近期项目开源规划介绍 @GuangmingLuo  介绍新的开源项目 Frugal，欢迎感兴趣的同学熟悉并参与此项目。Kitex 下一个版本正式支持 Frugal，Kitex 新版本发布之后正式对外发文分享与宣传 Frugal。 Hertz 预计会在 5 月底或者 6 月初正式对外开源。正式开源后会发布会发布新手任务（代码层面 + 文档翻译），欢迎大家踊跃参与。   议程 2 ：Kitex 单测任务进展梳理 @GuangmingLuo   完成进度：7/14\n  提交 PR 注意事项：\n 一定注意 CI 报错，及时修复错误； 遵守 Issue Description 提到的要求； 提交 PR 的同学加快进度，团队内部负责 Review 的同学加紧跟进。争取在 5 月份的下一个版本发布之前，可以合入这些 PR。 需要 Rebase 的 Develop 代码单测错误已得到修复，Rebase 一下 Develop 分支代码即可解决。    因疫情影响，目前居家办公，给所有贡献者邮寄礼物进程会推迟，复工后统一邮寄。请同学们不用过于担心，承诺的礼物一定送到。\n   议程 3：Kitex 对接 xDS 方案介绍 @ppzqh @CoderPoet  相关文档：Kitex 对接 xDS 总体技术方案设计 提炼功能，在 Kitex 上面提一个 Issue，对要做的 Feature 做背景和方案概述，拆分开发工作量，方便社区里面感兴趣的同学参加。 会议后已建好 Issue：https://github.com/cloudwego/kitex/issues/461   议程 4：Kitex mall demo 介绍 @bodhisatan  很多 RPC 框架都有一个偏官方的电商 Demo，如 Kratos 和 Go-zero。 相关文档：KiteX mall demo 拆分细化任务之后，号召社区交流群及社区用户群的同学来参与，同时对外宣传。 目前社区任务分工：@daidai21(付韦虎) @@clark(王伟超)   议程 5：Kitex 源码分析活动介绍 @yiyun  背景： a. 从 Java 转到 Golang 或者 Go 语言的同学对 Kitex 有学习需求； b. 高校同学参加开源夏令营和培训活动，关注到 Kitex。 活动地址：https://github.com/cloudwego/community/issues/24 产出： a. 导师：整体梳理 Kitex 模块，把各个技术点的学习资料做成一套从 0 到 1 的学习笔记。（第一期导师@clark(王伟超) ） b. 学生：学习笔记、源码解读文章。   议程 6：CloudWeGo 公众号官宣 @yiyun  公众号定位：发布包括但不限于社区运行状态、社区新闻、项目版本发布、重要节点活动宣传、Committer 专访。 官宣：https://mp.weixin.qq.com/s/nSybru-NMdZmdaaQgLM2bQ   议程7：新成员自我介绍  新成员名单：ag9920 baize Jacob953 社区新成员分别进行自我介绍，主要包含个人基本情况、开源贡献经历和后续参与社区工作内容。   相关资讯： 新的开源项目 Frugal 已经 Public，欢迎大家熟悉了解并积极参与。 地址：https://github.com/cloudwego/frugal。\n","categories":"","description":"","excerpt":"会议主题：CloudWeGo 社区会议 5.19\n参会人： YangruiEmma, ag9920, Jiang Xuewu, …","ref":"/zh/community/meeting_notes/2022-05-19/","tags":"","title":"CloudWeGo 社区会议 5.19"},{"body":"kitex-contrib has provided the prometheus monitoring extensions.\nIf you want to get the more detailed monitoring, such as message packet size, or want to adopt other data source, such as InfluxDB, you can implement the Trace interface according to your requirements and inject by WithTracer Option.\n// Tracer is executed at the start and finish of an RPC. type Tracer interface { Start(ctx context.Context) context.Context Finish(ctx context.Context) } RPCInfo can be obtained from ctx, and further request time cost, package size, and error information returned by the request can be obtained from RPCInfo, for example:\ntype clientTracer struct { // contain entities which recording metric } // Start record the beginning of an RPC invocation. func (c *clientTracer) Start(ctx context.Context) context.Context { // do nothing  return ctx } // Finish record after receiving the response of server. func (c *clientTracer) Finish(ctx context.Context) { ri := rpcinfo.GetRPCInfo(ctx) rpcStart := ri.Stats().GetEvent(stats.RPCStart) rpcFinish := ri.Stats().GetEvent(stats.RPCFinish) cost := rpcFinish.Time().Sub(rpcStart.Time()) // TODO: record the cost of request } ","categories":"","description":"","excerpt":"kitex-contrib has provided the prometheus monitoring extensions.\nIf …","ref":"/docs/kitex/tutorials/framework-exten/monitoring/","tags":"","title":"Monitoring Extension"},{"body":"There are two types of timeout in Kitex, RPC timeout and connection timeout. Both can be specified by client option and call option.\nRPC Timeout  You can specify RPC timeout in client initialization, it will works for all RPC started by this client by default.  import \"github.com/cloudwego/kitex/client\" ... rpcTimeout := client.WithRPCTimeout(3*time.Second) client, err := echo.NewClient(\"echo\", rpcTimeout) if err != nil { log.Fatal(err) }  And you can also specify timeout for a specific RPC call.  import \"github.com/cloudwego/kitex/client/callopt\" ... rpcTimeout := callopt.WithRPCTimeout(3*time.Second) resp, err := client.Echo(context.Background(), req, rpcTimeout) if err != nil { log.Fatal(err) } Connection Timeout  You can specify connection timeout in client initialization, it will works for all RPC started by this client by default.  import \"github.com/cloudwego/kitex/client\" ... connTimeout := client.WithConnectTimeout(50*time.Millisecond) client, err := echo.NewClient(\"echo\", connTimeout) if err != nil { log.Fatal(err) } And you can also specify timeout for a specific RPC call.  import \"github.com/cloudwego/kitex/client/callopt\" ... connTimeout := callopt.WithConnectTimeout(50*time.Millisecond) resp, err := client.Echo(context.Background(), req, connTimeout) if err != nil { log.Fatal(err) } ","categories":"","description":"","excerpt":"There are two types of timeout in Kitex, RPC timeout and connection …","ref":"/docs/kitex/tutorials/basic-feature/timeout/","tags":"","title":"Timeouts"},{"body":"用户如果需要更详细的打点，例如包大小，或者想要更换其他数据源，例如 influxDB，用户可以根据自己的需求实现 Trace 接口，并通过 WithTracer Option来注入。\n// Tracer is executed at the start and finish of an RPC. type Tracer interface { Start(ctx context.Context) context.Context Finish(ctx context.Context) } 从 ctx 中可以获得 RPCInfo，进一步的从 RPCInfo 中获取请求耗时、包大小和请求返回的错误信息等，举例：\ntype clientTracer struct { // contain entities which recording metric } // Start record the beginning of an RPC invocation. func (c *clientTracer) Start(ctx context.Context) context.Context { // do nothing \treturn ctx } // Finish record after receiving the response of server. func (c *clientTracer) Finish(ctx context.Context) { ri := rpcinfo.GetRPCInfo(ctx) rpcStart := ri.Stats().GetEvent(stats.RPCStart) rpcFinish := ri.Stats().GetEvent(stats.RPCFinish) cost := rpcFinish.Time().Sub(rpcStart.Time()) // TODO: record the cost of request } ","categories":"","description":"","excerpt":"用户如果需要更详细的打点，例如包大小，或者想要更换其他数据源，例如 influxDB，用户可以根据自己的需求实现 Trace 接口， …","ref":"/zh/docs/kitex/tutorials/framework-exten/monitoring/","tags":"","title":"监控扩展"},{"body":"Kitex 支持了两种超时，RPC 超时和连接超时，两种超时均支持 client 级别和调用级别的配置。\nRPC 超时  在 client 初始化时配置，配置的 RPC 超时将对此 client 的所有调用生效  import \"github.com/cloudwego/kitex/client\" ... rpcTimeout := client.WithRPCTimeout(3*time.Second) client, err := echo.NewClient(\"echo\", rpcTimeout) if err != nil { log.Fatal(err) } 在发起调用时配置，配置的 RPC 超时仅对此次调用生效  import \"github.com/cloudwego/kitex/client/callopt\" ... rpcTimeout := callopt.WithRPCTimeout(3*time.Second) resp, err := client.Echo(context.Background(), req, rpcTimeout) if err != nil { log.Fatal(err) } 连接超时  在 client 初始化时配置，配置的连接超时将对此 client 的所有调用生效  import \"github.com/cloudwego/kitex/client\" ... connTimeout := client.WithConnectTimeout(50*time.Millisecond) client, err := echo.NewClient(\"echo\", connTimeout) if err != nil { log.Fatal(err) } 在发起调用时配置，配置的连接超时仅对此次调用生效  import \"github.com/cloudwego/kitex/client/callopt\" ... connTimeout := callopt.WithConnectTimeout(50*time.Millisecond) resp, err := client.Echo(context.Background(), req, connTimeout) if err != nil { log.Fatal(err) } ","categories":"","description":"","excerpt":"Kitex 支持了两种超时，RPC 超时和连接超时，两种超时均支持 client 级别和调用级别的配置。\nRPC 超时  在 client  …","ref":"/zh/docs/kitex/tutorials/basic-feature/timeout/","tags":"","title":"超时控制"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/pilota/","tags":"","title":"Pilota"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/pilota/","tags":"","title":"Pilota"},{"body":"1. 重试功能说明 目前有三类重试：异常重试、Backup Request，建连失败重试（默认）。其中建连失败是网络层面问题，由于请求未发出，框架会默认重试。 本文档介绍前两类重试的使用：\n 异常重试：提高服务整体的成功率 Backup Request：减少服务的延迟波动  因为很多的业务请求不具有幂等性，这两类重试不会作为默认策略。\n1.1 注意：  确认你的服务具有幂等性，再开启重试 异常重试会增加延迟  2. 重试策略 异常重试和 Backup Request 策略方法粒度上只能配置其中之一。\n  异常重试\n默认只对超时重试，可配置支持指定异常或 Resp 重试。\n     配置项 默认值 说明 限制     MaxRetryTimes 2 最大重试次数，不包含首次请求。如果配置为 0 表示停止重试。 合法值：[0-5]   MaxDurationMS 0 累计最大耗时，包括首次失败请求和重试请求耗时，如果耗时达到了限制的时间则停止后续的重试。0 表示无限制。注意：如果配置，该配置项必须大于请求超时时间。    EERThreshold 10% 重试熔断错误率阈值, 方法级别请求错误率超过阈值则停止重试。 合法值：(0-30%]   ChainStop - 链路中止, 默认启用。如果上游请求是重试请求，不会重试。 \u003e= v0.0.5 后作为默认策略   DDLStop false 链路超时中止，该策略是从链路的超时时间判断是否需要重试。注意，Kitex 未内置该实现，需通过 retry.RegisterDDLStop(ddlStopFunc) 注册 DDL func，结合链路超时判断，实现上建议基于上游的发起调用的时间戳和超时时间判断。​​    BackOff None 重试等待策略，默认立即重试（NoneBackOff）。可选：固定时长退避 (FixedBackOff)、随机时长退避 (RandomBackOff)。    RetrySameNode false 框架默认选择其他节点重试，若需要同节点重试，可配置为 true。      Backup Request     配置项 默认值 说明 限制     RetryDelayMS - Backup Request 的等待时间，若该时间内若请求未返回，会发送新的请求。必须手动配置，建议参考 TP99。    MaxRetryTimes 1 最大重试次数，不包含首次请求。 如果配置为 0 表示停止重试。 合法值：[0-2]   EERThreshold 10% 重试熔断错误率阈值，方法级别请求错误率超过阈值则停止重试。 合法值：(0-30%]   ChainStop - 链路中止, 默认启用。如果上游请求是重试请求，不会发送 Backup Request。 \u003e= v0.0.5 后作为默认策略   RetrySameNode false 框架默认选择其他节点重试，若需要同节点重试，可配置为 true     3. 使用方式 3.1 代码配置开启 注意：若通过代码配置开启重试，动态配置 (见 3.3) 则无法生效。\n3.1.1 异常重试配置  配置示例：  // import \"github.com/cloudwego/kitex/pkg/retry\" fp := retry.NewFailurePolicy() fp.WithMaxRetryTimes(3) // 配置最多重试3次 xxxCli := xxxservice.NewClient(\"destServiceName\", client.WithFailureRetry(fp))  策略选择：  fp := retry.NewFailurePolicy() // 重试次数, 默认2，不包含首次请求 fp.WithMaxRetryTimes(xxx) // 总耗时，包括首次失败请求和重试请求耗时达到了限制的duration，则停止后续的重试。 fp.WithMaxDurationMS(xxx) // 关闭链路中止 fp.DisableChainRetryStop() // 开启DDL中止 fp.WithDDLStop() // 退避策略，默认无退避策略 fp.WithFixedBackOff(fixMS int) // 固定时长退避 fp.WithRandomBackOff(minMS int, maxMS int) // 随机时长退避  // 开启重试熔断 fp.WithRetryBreaker(errRate float64) // 同一节点重试 fp.WithRetrySameNode() 3.1.1.1 指定结果重试（异常/Resp） 支持版本 v0.4.0。\n可配置支持指定结果重试，结果可以是请求失败，也可以指定 Resp。因为业务可能在 Resp 设置状态信息，针对某类返回重试，所以支持指定 Resp 重试，这里统称为异常重试。\n 配置示例：  // import \"github.com/cloudwego/kitex/pkg/retry\"  var opts []client.Option opts = append(opts, client.WithSpecifiedResultRetry(yourResultRetry)) xxxCli := xxxservice.NewClient(targetService, opts...)  retry.ShouldResultRetry 定义  为了能具体到方法粒度对 error 和 resp 做判断，提供 rpcinfo 作为入参，可以通过 ri.To().Method() 获取方法。\n// ShouldResultRetryit is used for specifying which error or resp need to be retried type ShouldResultRetry struct { ErrorRetry func(err error, ri rpcinfo.RPCInfo) bool RespRetry func(resp interface{}, ri rpcinfo.RPCInfo) bool }   指定 异常/Resp 实现示例：\n  关于 Resp：\nThrift 和 KitexProtobuf 协议 Resp 对应的是生成代码中的 *XXXResult，不是真实的业务 Resp，获取真实的 Resp 需要断言 interface{ GetResult() interface{} }；\n  关于 Error：\n对端返回的 error，kitex 都会统一封装为 kerrors.ErrRemoteOrNetwork，对于 Thrift 和 KitexProtobuf 以下示例可以获取对端返回 Error Msg；对于 gRPC 如果对端通过 status.Error 构造的错误返回，本端使用 status.FromError(err) 可以获取 *status.Status，注意 Status 需使用 Kitex 提供的，包路径是 github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/status。\n    // retry with specify Resp for one method respRetry := func(resp interface{}, ri rpcinfo.RPCInfo) bool { if ri.To().Method() == \"mock\" { // Notice: you should test with your code, this is only a demo, thrift gen-code of Kitex has GetResult() interface{}  if respI, ok1 := resp.(interface{ GetResult() interface{} }); ok1 { if r, ok2 := respI.GetResult().(*xxx.YourResp); ok2 \u0026\u0026 r.Msg == retryMsg { return true } } } return false } // retry with specify Error for one method errorRetry := func(err error, ri rpcinfo.RPCInfo) bool { if ri.To().Method() == \"mock\" { if te, ok := errors.Unwrap(err).(*remote.TransError); ok \u0026\u0026 te.TypeID() == -100 { return true } } return false } // client option yourResultRetry := \u0026retry.ShouldResultRetry{ErrorRetry:errorRetry , RespRetry: respRetry} opts = append(opts, client.WithSpecifiedResultRetry(yourResultRetry)) 特别地，对于 Thrift 的 Exception，rpc 调用层面虽然返回了 error，但对框架内部处理其实视为一次成本的 RPC 请求（因为有实际的返回），如果要对其做判断需注意两点：\n 通过 resp 做判断而不是 error 若该方法重试成功既 GetSuccess() != nil，需重置 Exception 为 nil，因为重试使用的是一个 XXXResult，且 Resp 和 Exception 对应的是 XXXResult 的两个字段，第一次返回 Exception 已经做了赋值，第二次成功对 Resp 赋值但框架层面不会重置 Exception，需要用户自行重置。  示例如下：\nrespRetry := func(resp interface{}, ri rpcinfo.RPCInfo) bool { if ri.To().Method() == \"testException\" { teResult := resp.(*stability.TestExceptionResult) if teResult.GetSuccess() != nil { teResult.SetStException(nil) } else if teResult.IsSetXXException() \u0026\u0026 teResult.XxException.Message == xxx { return true } } return false } 3.1.2 Backup Request 配置  Retry Delay 建议  建议配置为 TP99，则 1% 请求会触发 Backup Request。\n 配置示例：  // 首次请求 xxx ms未返回，发起 backup 请求，并开启链路中止 bp := retry.NewBackupPolicy(xxx) xxxCli := xxxservice.NewClient(targetService, client.WithBackupRequest(bp))  策略选择：  bp := retry.NewBackupPolicy(xxx) // 重试次数, 默认1，不包含首次请求 bp.WithMaxRetryTimes(xxx) // 关闭链路中止 bp.DisableChainRetryStop() // 开启重试熔断 bp.WithRetryBreaker(errRate float64) // 同一节点重试 bp.WithRetrySameNode() 3.1.3 方法粒度配置重试 支持版本 v0.4.0。\n3.1.1,3.1.2 的示例配置会对所有方法生效，如果希望只对部分方法配置重试，或对不同方法分别配置 失败重试 或 BackupRequest，配置如下：\n 配置实例：  // import \"github.com/cloudwego/kitex/pkg/retry\" methodPolicies := client.WithRetryMethodPolicies(map[string]retry.Policy{ \"method1\": retry.BuildFailurePolicy(retry.NewFailurePolicy()), \"method2\": retry.BuildFailurePolicy(retry.NewFailurePolicyWithResultRetry(yourResultRetry))}) // other methods do backup request except above methods otherMethodPolicy := client.WithBackupRequest(retry.NewBackupPolicy(10)) var opts []client.Option opts = append(opts, methodPolicies, otherMethodPolicy) xxxCli := xxxservice.NewClient(targetService, opts...)  如果同时配置了 WithFailureRetry 或 WithBackupRequest，则 WithRetryMethodPolicies 未配置的方法会按照 WithFailureRetry 或 WithBackupRequest 策略执行。但 WithFailureRetry 和 WithBackupRequest 因为会对 client 所有方法生效，不能同时配置。\n 3.1.4 请求级别配置重试（callopt） 支持版本 v0.4.0。\n 配置示例：  import ( \"github.com/cloudwego/kitex/pkg/retry\" ) // demo1: call with failure retry policy, default retry error is Timeout resp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildFailurePolicy(retry.NewFailurePolicy()))) // demo2: call with customized failure retry policy resp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildFailurePolicy(retry. NewFailurePolicyWithResultRetry(retry.AllErrorRetry())))) // demo3: call with backup request policy bp := retry.NewBackupPolicy(10) bp.WithMaxRetryTimes(1) resp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildBackupRequest(bp))) 3.2 复用熔断器 当开启了服务的熔断配置可以复用熔断的统计减少额外的 CPU 消耗，注意重试的熔断阈值须低于服务的熔断阈值。\n 配置实例：  // 1. 初始化 kitex 内置的 cbsuite cbs := circuitbreak.NewCBSuite(circuitbreak.RPCInfo2Key) // 2. 初始化 retryContainer，传入ServiceControl和ServicePanel retryC := retry.NewRetryContainerWithCB(cs.cbs.ServiceControl(), cs.cbs.ServicePanel()) var opts []client.Option // 3. 配置 retryContainer opts = append(opts, client.WithRetryContainer(retryC)) // 4. 配置 Service circuit breaker opts = append(opts, client.WithMiddleware(cbs.ServiceCBMW())) // 5. 初始化 Client, 传入配置 option cli, err := xxxservice.NewClient(targetService, opts...) 3.3 动态开启或调整策略 若需要结合远程配置，动态开启重试或运行时调整策略，可以通过 retryContainer 的 NotifyPolicyChange 方法生效，目前 Kitex 开源版本暂未提供远程配置模块，使用者可集成自己的配置中心。注意：若已通过代码配置开启，动态配置则无法生效。\n 配置示例：  retryC := retry.NewRetryContainer() // demo // 1. define your change func // 2. exec yourChangeFunc in your config module yourChangeFunc := func(key string, oldData, newData interface{}) { newConf := newData.(*retry.Policy) method := parseMethod(key) retryC.NotifyPolicyChange(method, policy) } // configure retryContainer cli, err := xxxservice.NewClient(targetService, client.WithRetryContainer(retryC)) 4. 监控埋点 Kitex 对重试的请求在 rpcinfo 中记录了重试次数和之前请求的耗时，可以在Client侧的 metric 或日志中根据 retry tag 区分上报或输出。获取方式：\nvar retryCount string var lastCosts string toInfo := rpcinfo.GetRPCInfo(ctx).To() if retryTag, ok := toInfo.Tag(rpcinfo.RetryTag); ok { retryCount = retryTag if lastCostTag, ok := toInfo.Tag(rpcinfo.RetryLastCostTag); ok { lastCosts = lastCostTag } } 5. 下游识别重试请求 如果使用 TTHeader 作为传输协议，下游 handler 可以通过如下方式判断当前是否是重试请求，自行决定是否继续处理。\nretryReqCount, exist := metainfo.GetPersistentValue(ctx,retry.TransitKey) 比如 retryReqCount = 2，表示第二次重试请求（不包括首次请求），则采取业务降级策略返回部分或 mock 数据返回（非重试请求没有该信息）。\n Q: 框架默认开启链路中止，业务是否还有必要识别重试请求？\n链路中止是指链路上的重试请求不会重试，比如 A-\u003eB-\u003eC，A 向 B 发送的是重试请求，如果 B-\u003eC 超时了或者配置了 Backup，则 B 不会再发送重试请求到 C。如果业务自行识别重试请求，可以直接决定是否继续请求到 C。简言之链路中止避免了 B 向 C 发送重试请求导致重试放大，业务自己控制可以完全避免 B 到 C 的请求。\n ","categories":"","description":"","excerpt":"1. 重试功能说明 目前有三类重试：异常重试、Backup Request，建连失败重试（默认）。其中建连失败是网络层面问题，由于请求未发 …","ref":"/zh/docs/kitex/tutorials/basic-feature/retry/","tags":"","title":"请求重试"},{"body":"Hertz provides a default way to print logs in the standard output. It also provides several global functions, such as hlog.Info, hlog.Errorf, hlog.CtxTracef, etc., which are implemented in pkg/common/hlog, to call the corresponding methods of the default logger.\nHow to print logs Hertz can call the method under the pkg/common/hlog package directly, which will call the corresponding method on the defaultLogger. For example, implement a middleware that prints AccessLog.\nfunc AccessLog() app.HandlerFunc { return func(c context.Context, ctx *app.RequestContext) { start := time.Now() ctx.Next(c) end := time.Now() latency := end.Sub(start).Microseconds hlog.CtxTracef(c, \"status=%d cost=%d method=%s full_path=%s client_ip=%s host=%s\", ctx.Response.StatusCode(), latency, ctx.Request.Header.Method(), ctx.Request.URI().PathOriginal(), ctx.ClientIP(), ctx.Request.Host()) } } Redirects the output of the default logger Hertz can use hlog.SetOutput to redirect the output of the default logger provided by hlog. For example, to redirect the output of the default logger to . /output.log, you may do as follows:\npackage main import ( \"os\" \"github.com/cloudwego/hertz/pkg/common/hlog\" ) func main() { f, err := os.OpenFile(\"./output.txt\", os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0644) if err != nil { panic(err) } defer f.Close() hlog.SetOutput(f) ... // continue to set up your server } Set the logLevel Hertz can use hlog.SetLevel to set the log level above which logs will be printed.\nhlog.SetLevel(hlog.LevelInfo) The following log levels are currently supported:\nLevelTrace LevelDebug LevelInfo LevelNotice LevelWarn LevelError LevelFatal ","categories":"","description":"","excerpt":"Hertz provides a default way to print logs in the standard output. It …","ref":"/docs/hertz/tutorials/basic-feature/log/","tags":"","title":"Log"},{"body":"Hertz 提供打印日志的方式，默认打在标准输出。实现在 pkg/common/hlog 中，Hertz 同时也提供了若干全局函数，例如 hlog.Info、hlog.Errorf、hlog.CtxTracef 等，用于调用默认 logger 的相应方法。\n如何打印日志 hertz 中可以直接调用 pkg/common/hlog 包下的方法打日志，该方法会调用 defaultLogger 上对应的方法。如实现一个打印 AccessLog 的中间件。\nfunc AccessLog() app.HandlerFunc { return func(c context.Context, ctx *app.RequestContext) { start := time.Now() ctx.Next(c) end := time.Now() latency := end.Sub(start).Microseconds hlog.CtxTracef(c, \"status=%d cost=%d method=%s full_path=%s client_ip=%s host=%s\", ctx.Response.StatusCode(), latency, ctx.Request.Header.Method(), ctx.Request.URI().PathOriginal(), ctx.ClientIP(), ctx.Request.Host()) } } 重定向默认 logger 的输出 可以使用 hlog.SetOutput 来重定向 hlog 提供的默认 logger 的输出。 例如，要把默认 logger 的输出重定向到启动路径下的 ./output.log，可以这样实现：\npackage main import ( \"os\" \"github.com/cloudwego/hertz/pkg/common/hlog\" ) func main() { f, err := os.OpenFile(\"./output.txt\", os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0644) if err != nil { panic(err) } defer f.Close() hlog.SetOutput(f) ... // continue to set up your server } 设置 logLevel 可以使用 hlog.SetLevel 来设置日志等级，高于该日志等级的日志才能够被打印出来。\nhlog.SetLevel(hlog.LevelInfo) 目前支持的日志等级有\nLevelTrace LevelDebug LevelInfo LevelNotice LevelWarn LevelError LevelFatal ","categories":"","description":"","excerpt":"Hertz 提供打印日志的方式，默认打在标准输出。实现在 pkg/common/hlog 中，Hertz 同时也提供了若干全局函数， …","ref":"/zh/docs/hertz/tutorials/basic-feature/log/","tags":"","title":"日志"},{"body":"1. Introduction There are currently three types of retries: Timeout Retry, Backup Request and Connection Failed Retry. Among them, Connection Failed Retry is a network-level problem, since the request is not sent, the framework will retry by default. Here we only present the use of the first two types of retries:\n Timeout Retry: Improve the overall success rate of the service. Backup Request: Reduce delay jitter of request.  Because many requests are not idempotent, these two types of retries are not used as the default policy.\n1.1 Attention  Confirm that your service is idempotent before enable retry. Timeout Retry will increase overall latency.  2. Retry Policy Only one of the Timeout Retry and Backup Request policies can be configured.\n Timeout Retry     Configuration Item Default value Description Limit     MaxRetryTimes 2 The first request is not included. If it is configured as 0, it means to stop retrying. Value: [0-5]   MaxDurationMS 0 Including the time-consuming of the first failed request and the retry request. If the limit is reached, the subsequent retry will be stopped. 0 means unlimited. Note: if configured, the configuration item must be greater than the request timeout.    EERThreshold 10% If the method-level request error rate exceeds the threshold, retry stops. Value: (0-30%]   ChainStop - Chain Stop is enabled by default. If the upstream request is a retry request, it will not be retried after timeout. \u003e= v0.0.5 as the default policy.   DDLStop false If the timeout period of overall request chain is reached, the retry request won’t be sent with this policy. Notice, Kitex doesn’t provide build-in implementation, use retry.RegisterDDLStop(ddlStopFunc) to register is needed.    BackOff None Retry waiting strategy, NoneBackOff by default. Optional: FixedBackOff, RandomBackOff.    RetrySameNode false By default, Kitex selects another node to retry. If you want to retry on the same node, set this parameter to true.      Backup Request     Configuration Item Default value Description Limit     RetryDelayMS - Duration of waiting for initiating a Backup Requset when the first request is not returned. This parameter must be set manually. It is suggested to set as TP99.    MaxRetryTimes 1 The first request is not included. If it is configured as 0, it means to stop retrying. Value: [0-2]   EERThreshold 10% If the method-level request error rate exceeds the threshold, retry stops. Value: (0-30%]   ChainStop false Chain Stop is enabled by default. If the upstream request is a retry request, it will not be retried after timeout. \u003e= v0.0.5 as the default policy.   RetrySameNode false By default, Kitex selects another node to retry. If you want to retry on the same node, set this parameter to true.     3. How to use 3.1 Enable by Code Configuration Note: Dynamic configuration (see 3.3) cannot take effect if retry is enabled by code configuration.\n3.1.1 Timeout Retry Configuration  Configuration e.g.  // import \"github.com/cloudwego/kitex/pkg/retry\" fp := retry.NewFailurePolicy() fp.WithMaxRetryTimes(3) // set the maximum number of retries to 3 xxxCli := xxxservice.NewClient(\"destServiceName\", client.WithFailureRetry(fp))  Strategy selection:  fp := retry.NewFailurePolicy() // Number of retries. The default value is 2, excluding the first request. fp.WithMaxRetryTimes(xxx) // Total time consuming. Including the time-consuming for the first failed request and retry request. If the duration limit is reached, the subsequent retries are stopped. fp.WithMaxDurationMS(xxx) // Disable `Chain Stop` fp.DisableChainRetryStop() // Enable DDL abort fp.WithDDLStop() // Backoff policy. No backoff strategy by default. fp.WithFixedBackOff(fixMS int) // Fixed backoff fp.WithRandomBackOff(minMS int, maxMS int) // Random backoff  // Set errRate for retry circuit breaker fp.WithRetryBreaker(errRate float64) // Retry on the same node fp.WithRetrySameNode() 3.1.2 Backup Request Configuration  Retry Delay recommendations  It is recommended to configure as TP99, then 1% request will trigger Backup Request.\n Configuration e.g.  // If the first request is not returned after XXX ms, the backup request will be initiated and the `Chain Retry Stop` is enabled bp := retry.NewBackupPolicy(xxx) xxxCli := xxxservice.NewClient(\"destServiceName\", client.WithBackupRequest(bp))  Strategy selection:  bp := retry.NewBackupPolicy(xxx) // Number of retries. The default value is 1, excluding the first request. bp.WithMaxRetryTimes(xxx) // Disable `Chain Stop` bp.DisableChainRetryStop() // Set errRate for retry circuit breaker bp.WithRetryBreaker(errRate float64) // Retry on the same node bp.WithRetrySameNode() 3.2 Circuit Breaker Reuse When circuit breaker is enabled for a service, you can reuse the breaker’s statistics to reduce additional CPU consumption. Note that the error rate threshold for retries must be lower than the threshold for a service, as follows:\n// 1. Initialize kitex's built-in cbsuite cbs := circuitbreak.NewCBSuite(circuitbreak.RPCInfo2Key)// 2. Initialize retryContainer, passing in ServiceControl and ServicePanel retryC := retry.NewRetryContainerWithCB(cs.cbs.ServiceControl(), cs.cbs.ServicePanel()) var opts []client.Option // 3. Set retryContainer opts = append(opts, client.WithRetryContainer(retryC)) // 4. Set Service circuit breaker opts = append(opts, client.WithMiddleware(cbs.ServiceCBMW())) // 5. Initialize Client and pass in the configuration option cli, err := xxxservice.NewClient(targetService, opts...) 3.3 Dynamic open or adjust strategy If you want to adjust the policy in combination with remote configuration, dynamic open retry, or runtime, you can take effect through the NotifyPolicyChange method of retryContainer. Currently, the open source version of Kitex does not provide a remote configuration module, and users can integrate their own configuration center. Note: If it is turned on through code configuration, dynamic configuration cannot take effect.\nUse case:\nretryC := retry.NewRetryContainer() // demo // 1. define your change func // 2. exec yourChangeFunc in your config module yourChangeFunc := func(key string, oldData, newData interface{}) { newConf := newData.(*retry.Policy) method := parseMethod(key) retryC.NotifyPolicyChange(method, policy) } // configure retryContainer cli, err := xxxservice.NewClient(targetService, client.WithRetryContainer(retryC)) 4. Tracking Kitex records the retry times and previous request time in rpcInfo. You can report or output a retry request based on the retry Tag in Client’s metric or log through:\nvar retryCount string var lastCosts string toInfo := rpcinfo.GetRPCInfo(ctx).To() if retryTag, ok := toInfo.Tag(rpcinfo.RetryTag); ok { retryCount = retryTag if lastCostTag, ok := toInfo.Tag(rpcinfo.RetryLastCostTag); ok { lastCosts = lastCostTag } } 5. Downstream identification If using TTHeader as the transport protocol, you can determine if the downstream handler is currently a retry request and decide whether to continue processing.\nretryReqCount, exist := metainfo.GetPersistentValue(ctx,retry.TransitKey) For example, retryReqCount = 2, which means the second retry request (excluding the first request), then the business degradation strategy can be adopted(non-retry requests do not have this information).\n Question: Chain Stop is enabled by default, is it necessary for services to identify retry requests?\n  Answer：Chain Stop means that the retry request on the chain will not be retried. Assuming that there is a request chain A-\u003eB-\u003eC, A sends a retry request to B, while during B-\u003eC, if a timeout occurs or Backup is configured, B will not send a retry request to C. If the service can identify the retry request, it can directly decide whether to continue the request to C. In short, Chain Stop avoids retry amplification caused by B sending a retry request to C. The service’s own control can completely avoid requests from B to C.\n ","categories":"","description":"","excerpt":"1. Introduction There are currently three types of retries: Timeout …","ref":"/docs/kitex/tutorials/basic-feature/retry/","tags":"","title":"Retry"},{"body":"会议主题 ：CloudWeGo 社区会议 6.2\n参会人 ：YangruiEmma, baiyutang, joway, yccpt, Huang Yuting, CoderPoet, li-jin-gou, GuangmingLuo, simon0-o, yiyun, JZK-Keven, bodhisatan, Jacob953, cyyolo, debug-LiXiwen, baize, zstone12, You Gaoming, HeyJavaBean, jayantxie, Skyenought\n会前必读 ：http://www.cloudwego.io/；https://github.com/cloudwego\n议程 1 ：新人介绍  新成员名单：Skyenought, zstone12, You Gaoming 社区新成员分别进行自我介绍，主要包含个人基本情况、开源贡献经历和后续参与社区工作内容。   议程 2 ：Frugal 项目介绍 @simon0-o  相关文档：https://mp.weixin.qq.com/s/b17bSqx9y5AIH3WEx1haog   议程 3：Integrate Polaris Go SDK to Support Their Service Governance Ability 任务介绍 @jayantxie  地址：https://github.com/cloudwego/kitex/issues/421 已认领 Issue @debug-LiXiwen 继注册发现能力之后，再以 Polaris 为服务治理中心，集成服务治理能力。熔断部分会在 Kitex 中注入 Middleware，通过 Middleware 上报每次请求结果。上报时需要 Polaris 实例，所以需要转换一下，这个转换可以通过缓存用 Key 去做查找。 此 PR 已经支持外部限流器的实现。可以通过扩展接口传入外部实现的限流器，对接 Polaris 的限流功能。 关于动态路由和负载均衡，通过扩展 Kitex LoadBalancer 实现。在 Polaris LoadBalancer 里，通过服务发现的实例去构造 Kitex 的实例，在LoadBalancer 接口里把这个实例转换成 Polaris 的实例，再把它写到缓存里。每一次 Pick 时，从 Pick 里找到调用 Polaris 动态路由的 API 对应的子集，再从这些子集里调用负载均衡的 API ，拿到对应实例。这个实例需要转成 Kitex 的 Instance， 所以也需要通过 Key 做查找操作。扩展 Next Picker 只会在第一次选择时执行这个逻辑。 新建 Polaris 仓库，把原有 Registry-Polaris 仓库代码复制过去，后续只维持新仓库。   议程 4：Kitex 源码分析活动进展介绍 @yiyun  第一期 5.19 — 6.30 进程近半，现有参与人数 93 人，开始撰写及提交笔记 10 人左右。6.1 晚第一次活动会议，讨论过后开了“百人共享”，共同整理 Kitex 基础教程。 相关文档：Kitex前传：RPC框架那些你不得不知的故事 。后续对这些感兴趣同学，可以直接联系逸云，加到 CloudWeGo study group 完成这个教程。   议程 5：社区开放性讨论 \u0026 QA @GuangmingLuo  希望有更多感兴趣的同学帮助官网做一些优化，尤其是 Community 内容展示和主页。有前端背景或者对开源项目的前端技术、网站建设感兴趣同学可以联系广明。   相关资讯： Kitex v0.3.2 已发布！\nhttps://github.com/cloudwego/kitex/releases/tag/v0.3.2\n","categories":"","description":"","excerpt":"会议主题 ：CloudWeGo 社区会议 6.2\n参会人 ：YangruiEmma, baiyutang, joway, yccpt, …","ref":"/zh/community/meeting_notes/2022-06-02/","tags":"","title":"CloudWeGo 社区会议 6.2"},{"body":"会议主题 ：CloudWeGo 社区会议 6.16\n参会人 ：YangruiEmma, joway, yccpt, CoderPoet, GuangmingLuo, simon0-o, yiyun, bodhisatan, Jacob953, cyyolo, HeyJavaBean, Skyenought, Quan Hu, ppzqh, ZhangHanAA, Suo Dianjun, Yin Xuran\n会前必读 ：http://www.cloudwego.io/；https://github.com/cloudwego\n议程 1 ：Hertz 项目介绍 @Yin Xuran  项目地址： https://github.com/cloudwego/hertz/blob/develop/README_cn.md 项目背景： Hertz 之前，字节跳动内部使用的 HTTP 框架是基于 Gin 进行了一层封装。存在的问题：Gin 出现 Bug 无法修复；难以迭代支持一些 Feature；随着业务发展性能不足逐渐显现，且难以改变。 Hertz 定位 ：   超大规模企业级实现，拥有极强的稳定性。 微服务框架。完善 CloudWeGo 的生态矩阵，让 CloudWeGo 成为云原生最佳的解决方案之一，从而向客户推广。 开箱即用的框架。包括比如搭积木的能力，用户可以按需组装模块；可能会生成一些 Client 代码，可以方便用户 Benchmark 或者帮助用户去调试，甚至生成一些生产上的代码。 “三高”的框架。高扩展性、高易用性和高性能。  内部使用情况： 是内部最大的 HTTP 框架，在内部线上有 1w+ 的服务峰值，QPS 4000w+。某些典型服务迁移 Hertz 后，相比 Gin 框架，CPU 使用率降低 30%—60%。 Roadmap：   无缝接入微服务体系。支持 xDS API，从 Istio 动态获取服务配置。 有更完善的生态。如 CORS、Trace、Metrics 、反向代理、Session 等。 支持多协议。Hertz 目前只开源了 HTTP1 的部分，未来还会开源其他协议，如：HTTP2、Websocket、ALPN 等。 更高的性能。结合用户需求，持续迭代。  6.21 官宣后会开放新手任务，以及社区参与指南，欢迎大家参与 Hertz 社区贡献。   议程 2 ：Hertz Swagger \u0026 JWT Middleware @bodhisatan  项目地址：https://github.com/hertz-contrib/swagger；https://github.com/hertz-contrib/jwt 贡献了 Hertz 的两个插件，Swagger 和 JWT。Fork 了 Gin 排名比较高的对应的仓库，然后对赫兹做适配，争取让开发者比较方便的从 Gin 切换到 Hertz。过程中需要看一些赫兹的接口源码，保证 Hertz 和 Gin 的表现相同。 对 Hertz 源码感兴趣的初学者可以从这里入手，建议社区也可以考虑把一些 Gin 里面常见的中间件以 First-good-issue 的形式开放。   议程 3：CloudWeGo “全新”社区页面介绍 @Skyenought @yiyun  地址：https://www.cloudwego.io/zh/community/ 参考 Google Kubernetes 社区实现，从社区获得相应的图片和文字描述，进行组合。目前上线了中文页面，后续根据社区要求进行改动，比如不定时更新的近期活动可以拆成模板，方便更新。   议程 4：CCF 活动进展同步 \u0026 CloudWeGo Meetup 预告 @yiyun  目前已经有 126 个同学加入社区，竞争 5 个 Issue。同学反馈问题是给到高校群体的开发任务量比较少，因此 Hertz 开源建设中，后续会开放出大量的新手任务，如小型的开发任务、文档类的整理任务、活动类任务等。 已有近 26 位同学参与 Issue 选拔。后续也会有社区的导师持续地跟进开发，11 月底活动结束。   议程 5：Q \u0026 A Q：社区 Committer 的申请要求是什么？\nA：相关链接：https://github.com/cloudwego/community/blob/main/COMMUNITY_MEMBERSHIP.md\n对社区有贡献的同学可以在 Issue 上面提出申请，相关人员会确认是否同意这个同学成为 Committer。一般来讲贡献比较多的同学会被提名，然后让这个同学自己在 Issue 上面申请。当然，如果同学个人觉得自己贡献比较多，也可以自己提名。如果大家同意会在下面回复，同意的人数足够多就可以加入。\n 相关资讯： 6 月 21 日，Hertz 正式官宣开源！ 官宣链接：https://mp.weixin.qq.com/s/D1Pol8L9F_5-Yte_k4DH8A\n技术解读：https://mp.weixin.qq.com/s/RC-BJOTEO7WaEemG96yR6w\n5 月 26 日 — 6 月 24 日，CloudWeGo - GLCC 开源编程夏令营开始报名，活动报名链接：https://mp.weixin.qq.com/s/owd13tN5XfKPQs7DeONWng\n6 月 25 日，CloudWeGo \u0026 稀土掘金 Meetup 活动直播，邀请到来自字节跳动、森马电商和华兴证券的资深开发者，向社区分享 CloudWeGo 的最新企业落地实践。 活动链接：https://mp.weixin.qq.com/s/D93dk-9dw2pQocI4anBXfg\n","categories":"","description":"","excerpt":"会议主题 ：CloudWeGo 社区会议 6.16\n参会人 ：YangruiEmma, joway, yccpt, CoderPoet, …","ref":"/zh/community/meeting_notes/2022-06-16/","tags":"","title":"CloudWeGo 社区会议 6.16"},{"body":"Hertz supports graceful shutdown, which is executed as follows：\n Set the state of engine to closed Sequential non-blocking trigger callback function []OnShutDown (consistent with standard library net/http) Shut down the signal listening of the network library Select waits for the business coroutine to exit：  For netpoll network library, turn on ticker with default 1s (set in netpoll, not changeable at the moment) and check if active conn (business handle exits and connection is not in blocking read state) is 0 at regular intervals; for go net network library, turn off listening and do not process the connection. Triggered by the context of ExitWaitTime, default 5s   Uniformly add Connection:Close header to request packets in the process of closing  If you want to modify the wait timeout, you can configure it with server.WithExitWaitTime().\nIf you want to register the hook function, you can do so by getting the Engine and registering it:\nh.Engine.OnShutdown = append(h.Engine.OnShutdown, shutDownFunc) ","categories":"","description":"","excerpt":"Hertz supports graceful shutdown, which is executed as follows：\n Set …","ref":"/docs/hertz/tutorials/basic-feature/graceful-shutdown/","tags":"","title":"Graceful Shutdown"},{"body":"Hertz 支持优雅退出，优雅退出过程如下：\n 设置 engine 状态为 closed 顺序非阻塞触发回调函数 []OnShutDown（与标准包 net/http 一致） 关闭网络库的信号监听 Select 等待业务协程退出：  对于 netpoll 网络库，开启默认1s（netpoll 中设置，暂时不可更改）的 ticker，定时查看 active conn（业务 handle 退出且连接不处于阻塞读状态）是否为0；对于 go net 网络库，则关闭监听，不对连接做处理。 等待超时时间为 ExitWaitTime 的 context 触发，默认 5s   对处于关闭过程中的请求回包统一带上 Connection:Close header  如需修改等待超时时间，可通过 server.WithExitWaitTime() 进行配置。\n如需注册退出 hook 函数，可通过获取到 Engine 后进行注册:\nh.Engine.OnShutdown = append(h.Engine.OnShutdown, shutDownFunc) ","categories":"","description":"","excerpt":"Hertz 支持优雅退出，优雅退出过程如下：\n 设置 engine 状态为 closed 顺序非阻塞触发回调函数 []OnShutDown（ …","ref":"/zh/docs/hertz/tutorials/basic-feature/graceful-shutdown/","tags":"","title":"优雅退出"},{"body":"Kitex supports extending protocols, including overall Codec and Payloadcodec. Generally, RPC protocol includes application layer transport protocol and payload protocol. For example, HTTP/HTTP2 belong to application layer transport protocol, payloads with different formats and protocols can be carried over HTTP/HTTP2.\nKitex supports built-in TTHeader as transport protocol, and supports Thrift, Kitex Protobuf, gRPC protocol as payload. In addition, Kitex integrates netpoll-http2 to support HTTP2. At present, it is mainly used for gRPC, Thrift over HTTP2 is considered to support in the future.\nThe definition of TTHeader transport protocol as follows, service information can be transparently transmitted through the TTHeader to do service governance.\n* TTHeader Protocol * +-------------2Byte--------------|-------------2Byte-------------+ * +----------------------------------------------------------------+ * | 0| LENGTH | * +----------------------------------------------------------------+ * | 0| HEADER MAGIC | FLAGS | * +----------------------------------------------------------------+ * | SEQUENCE NUMBER | * +----------------------------------------------------------------+ * | 0| Header Size(/32) | ... * +--------------------------------- * * Header is of variable size: * (and starts at offset 14) * * +----------------------------------------------------------------+ * | PROTOCOL ID |NUM TRANSFORMS . |TRANSFORM 0 ID (uint8)| * +----------------------------------------------------------------+ * | TRANSFORM 0 DATA ... * +----------------------------------------------------------------+ * | ... ... | * +----------------------------------------------------------------+ * | INFO 0 ID (uint8) | INFO 0 DATA ... * +----------------------------------------------------------------+ * | ... ... | * +----------------------------------------------------------------+ * | | * | PAYLOAD | * | | * +----------------------------------------------------------------+ Extension API of Codec Codec API is defined as follows:\n// Codec is the abstraction of the codec layer of Kitex. type Codec interface { Encode(ctx context.Context, msg Message, out ByteBuffer) error Decode(ctx context.Context, msg Message, in ByteBuffer) error Name() string } Codec is the overall codec interface, which is extended in combination with the transmission protocol and payload to be supported. The PayloadCodec interface is called according to the protocol type. Decode needs to detect the protocol to judge the transmission protocol and payload. Kitex provides defaultCodec extension implementation by default.\nExtension API of PayloadCodec PayloadCodec API is defined as follows:\n// PayloadCodec is used to marshal and unmarshal payload. type PayloadCodec interface { Marshal(ctx context.Context, message Message, out ByteBuffer) error Unmarshal(ctx context.Context, message Message, in ByteBuffer) error Name() string } By default, the payload supported by Kitex includes Thrift, Kitex Protobuf and gRPC protocols. Kitex Protobuf is the message protocol based Protobuf, the protocol definition is similar to Thrift message.\nIn particular, generic call of Kitex is also implemented by extending payloadcodec:\nDefault Codec Usage Kitex will use the built-in Codec if no customized codec provider set.\n Set default codec size limit, no limit by default option: codec.NewDefaultCodecWithSizeLimit  maxSizeBytes = 1024 * 1024 * 10 // 10 MB  // server side svr := xxxservice.NewServer(handler, server.WithCodec(codec.NewDefaultCodecWithSizeLimit(maxSizeBytes))) // client side cli, err := xxxservice.NewClient(targetService, client.WithCodec(codec.NewDefaultCodecWithSizeLimit(maxSizeBytes))) Customized Codec or PayloadCodec Usage Specify customized Codec and PayloadCodec through option.\n Specify Codec option: WithCodec  // server side svr := xxxservice.NewServer(handler, server.WithCodec(yourCodec)) // client side cli, err := xxxservice.NewClient(targetService, client.WithCodec(yourCodec))  Specify PayloadCodec option: WithPayloadCodec  // server side svr := xxxservice.NewServer(handler, server.WitWithPayloadCodechCodec(yourPayloadCodec)) // client side cli, err := xxxservice.NewClient(targetService, client.WithPayloadCodec(yourPayloadCodec)) ","categories":"","description":"","excerpt":"Kitex supports extending protocols, including overall Codec and …","ref":"/docs/kitex/tutorials/framework-exten/codec/","tags":"","title":"Extension of Codec"},{"body":"Kitex provides two LoadBalancers officially:\n WeightedRandom ConsistentHash  Kitex uses WeightedRandom by default.\nWeightedRandom WeightedRandom uses a random strategy based on weights, which is also Kitex’s default strategy.\nThis LoadBalancer will be weighted randomly according to the weight of the instance, and ensure that the load assigned to each instance is proportional to its own weight.\nIf all instances have the same weights, Kitex has special optimization for this scenario and will use a purely random implementation to avoid extra overhead of weighting calculations, so there is no need to worry about the performance in this scenario.\nConsistentHash Introduction Consistent hashing is mainly suitable for scenarios with high dependence on context (such as instance local cache). If you want the same type of request to hit the same endpoint, you can use this load balancing method.\nIf you don’t know what a consistent hash is, or don’t know the side effects, DO NOT use a consistent hash.\nUsage If you want to use a consistent hash, you can pass the parameter with client.WithLoadBalancer(loadbalance.NewConsistBalancer(loadbalance.NewConsistentHashOption(keyFunc))) when initializing the client.\nConsistentHashOption is defined as follows:\ntype ConsistentHashOption struct { GetKey KeyFunc // Whether or not to use Replica  // If replica is set, it would be tried in turn when the request fails (connection failure)  // Replica brings additional memory and computational overhead  // If replica is not set, then the request returns directly after failure (connection failure)  Replica uint32 // Number of virtual nodes  // The number of virtual nodes corresponding to each real node  // The higher the value, the higher the memory and computational cost, and the more balanced the load  // When the number of nodes is large, it can be set smaller; conversely, it can be set larger  // It is recommended that the median VirtualFactor * Weight (if Weighted is true) is around 1000, and the load should be well balanced  // Recommended total number of virtual nodes within 2000W (it takes 250ms to build once under 1000W, but it is theoretically fine to build in the background within 3s)  VirtualFactor uint32 // Whether to follow Weight for load balancing  // If false, Weight is ignored for each instance, and VirtualFactor virtual nodes are generated for indiscriminate load balancing  // Weight() * VirtualFactor virtual nodes for each instance  // Note that for instance with weight 0, no virtual nodes will be generated regardless of the VirtualFactor number  // It is recommended to set it to true, but be careful to reduce the VirtualFactor appropriately  Weighted bool // Whether or not to perform expiration processing  // Implementation will cache all keys  // Never expiring will cause memory to keep growing  // Setting expiration will result in additional performance overhead  // Current implementations scan for deletions every minute and delete once when the instance changes rebuild  // It is recommended to always set the value not less than one minute  ExpireDuration time.Duration } Note that if GetKey is nil or VirtualFactor is 0, panic will occur.\nPerformance After testing, with a weight of 10 and a VirtualFactor of 100, the build performance of different instances is as follows:\nBenchmarkNewConsistPicker_NoCache/10ins-16 6565 160670 ns/op 164750 B/op 5 allocs/op BenchmarkNewConsistPicker_NoCache/100ins-16 571 1914666 ns/op 1611803 B/op 6 allocs/op BenchmarkNewConsistPicker_NoCache/1000ins-16 45 23485916 ns/op 16067720 B/op 10 allocs/op BenchmarkNewConsistPicker_NoCache/10000ins-16 4 251160920 ns/op 160405632 B/op 41 allocs/op Therefore, when there are 10,000 instances, each instance weight is 10, and the VirtualFactor is 100 (the total number of virtual nodes is 10,000,000), it takes 251 ms to build once.\nBoth build and request information are cached, so the latency of a normal request (no build is required) has nothing to do with the number of nodes:\nBenchmarkNewConsistPicker/10ins-16 12557137 81.1 ns/op 0 B/op 0 allocs/op BenchmarkNewConsistPicker/100ins-16 13704381 82.3 ns/op 0 B/op 0 allocs/op BenchmarkNewConsistPicker/1000ins-16 14418103 81.3 ns/op 0 B/op 0 allocs/op BenchmarkNewConsistPicker/10000ins-16 13942186 81.0 ns/op 0 B/op 0 allocs/op Note  When the target node changes, the consistent hash result may change, and some keys may change; If there are too many target nodes, the build time may be longer during the first cold start, and if the rpc timeout is short, it could cause a timeout; If the first request fails and Replica is larger than 0, the try will hit the Replica, so the second and subsequent requests will still be sent to the first instance.  The degree of load balance As tested, when the number of target instances is 10, if the VirtualFactor is set to 1 and Weighted is not turned on, the load is very uneven, as follows:\naddr2: 28629 addr7: 13489 addr3: 10469 addr9: 4554 addr0: 21550 addr6: 6516 addr8: 2354 addr4: 9413 addr5: 1793 addr1: 1233 When VirtualFactor is set to 10, the load is as follows:\naddr7: 14426 addr8: 12469 addr3: 8115 addr4: 8165 addr0: 8587 addr1: 7193 addr6: 10512 addr9: 14054 addr2: 9307 addr5: 7172 It can be seen that it is much better than when the VirtualFactor is 1.\nWhen the VirtualFactor is 1000, the load is as follows:\naddr7: 9697 addr5: 9933 addr6: 9955 addr4: 10361 addr8: 9828 addr0: 9729 addr9: 10528 addr2: 10121 addr3: 9888 addr1: 9960 Load is basically balanced at this time.\nLet’s take the situation with Weight. We set the weight of addr0 to 0, the weight of addr1 to 1, the weight of addr2 to 2… and so on.\nSet VirtualFactor to 1000 and get the load result as follows:\naddr4: 8839 addr3: 6624 addr6: 13250 addr1: 2318 addr8: 17769 addr2: 4321 addr5: 11099 addr9: 20065 addr7: 15715 You could see that it is basically consistent with the distribution of weight. There is no addr0 here because weight is 0 and will not be scheduled.\nIn summary, increase VirtualFactor can make the load more balanced, but it will also increase the performance overhead, so you need to make trade-offs.\n","categories":"","description":"","excerpt":"Kitex provides two LoadBalancers officially:\n WeightedRandom …","ref":"/docs/kitex/tutorials/basic-feature/loadbalance/","tags":"","title":"LoadBalancer"},{"body":"Kitex 支持扩展协议，包括整体的 Codec 和 PayloadCodec。通常 RPC 协议中包含应用层传输协议和 Payload 协议，如 HTTP/HTTP2 属于应用层传输协议，基于 HTTP/HTTP2 可以承载不同格式和不同协议的 Payload。\nKitex 默认支持内置的 TTHeader 传输协议，Payload 支持 Thrift、KitexProtobuf、gRPC。另外，Kitex 集成 netpoll-http2 支持 HTTP2，目前主要用于 gRPC，后续会考虑基于 HTTP2 支持 Thrift。\nTTHeader 协议定义如下，通过 TTHeader 可以透传服务信息，便于服务治理。\n* TTHeader Protocol * +-------------2Byte--------------|-------------2Byte-------------+ * +----------------------------------------------------------------+ * | 0| LENGTH | * +----------------------------------------------------------------+ * | 0| HEADER MAGIC | FLAGS | * +----------------------------------------------------------------+ * | SEQUENCE NUMBER | * +----------------------------------------------------------------+ * | 0| Header Size(/32) | ... * +--------------------------------- * * Header is of variable size: * (and starts at offset 14) * * +----------------------------------------------------------------+ * | PROTOCOL ID |NUM TRANSFORMS . |TRANSFORM 0 ID (uint8)| * +----------------------------------------------------------------+ * | TRANSFORM 0 DATA ... * +----------------------------------------------------------------+ * | ... ... | * +----------------------------------------------------------------+ * | INFO 0 ID (uint8) | INFO 0 DATA ... * +----------------------------------------------------------------+ * | ... ... | * +----------------------------------------------------------------+ * | | * | PAYLOAD | * | | * +----------------------------------------------------------------+ Codec 定义 Codec 接口定义如下：\n// Codec is the abstraction of the codec layer of Kitex. type Codec interface { Encode(ctx context.Context, msg Message, out ByteBuffer) error Decode(ctx context.Context, msg Message, in ByteBuffer) error Name() string } Codec 是整体的编解码接口，结合需要支持的传输协议和 Payload 进行扩展，根据协议类型调用 PayloadCodec 接口，其中 Decode 需要进行协议探测判断传输协议和 Payload。Kitex 默认提供 defaultCodec 扩展实现。\nPayloadCodec 定义 PayloadCodec 接口定义如下：\n// PayloadCodec is used to marshal and unmarshal payload. type PayloadCodec interface { Marshal(ctx context.Context, message Message, out ByteBuffer) error Unmarshal(ctx context.Context, message Message, in ByteBuffer) error Name() string } Kitex 默认支持的 Payload 有 Thrift、Kitex Protobuf 以及 gRPC 协议。其中 Kitex Protobuf 是 Kitex 基于 Protobuf 定义的消息协议，协议定义与 Thrift Message 类似。\n特别地，Kitex 的泛化调用也是通过扩展 PayloadCodec 实现：\n默认的 Codec 如果用户不指定 Codec，则使用默认的内置 Codec。\n 指定默认 Codec 的包大小限制，默认无限制 option: codec.NewDefaultCodecWithSizeLimit  maxSizeBytes = 1024 * 1024 * 10 // 10 MB  // server side svr := xxxservice.NewServer(handler, server.WithCodec(codec.NewDefaultCodecWithSizeLimit(maxSizeBytes))) // client side cli, err := xxxservice.NewClient(targetService, client.WithCodec(codec.NewDefaultCodecWithSizeLimit(maxSizeBytes))) 指定自定义 Codec 和 PayloadCodec 通过 option 指定 Codec 和 PayloadCodec。\n 指定 Codec option: WithCodec  // server side svr := xxxservice.NewServer(handler, server.WithCodec(yourCodec)) // client side cli, err := xxxservice.NewClient(targetService, client.WithCodec(yourCodec))  指定 PayloadCodec option: WithPayloadCodec  // server side svr := xxxservice.NewServer(handler, server.WitWithPayloadCodechCodec(yourPayloadCodec)) // client side cli, err := xxxservice.NewClient(targetService, client.WithPayloadCodec(yourPayloadCodec)) ","categories":"","description":"","excerpt":"Kitex 支持扩展协议，包括整体的 Codec 和 PayloadCodec。通常 RPC 协议中包含应用层传输协议和 Payload 协 …","ref":"/zh/docs/kitex/tutorials/framework-exten/codec/","tags":"","title":"编解码 (协议) 扩展"},{"body":"Kitex 默认提供了两种 LoadBalancer（下面简称 lb）：\n WeightedRandom ConsistentHash  Kitex 默认使用的是 WeightedRandom。\nWeightedRandom 顾名思义，这个 lb 使用的是基于权重的随机策略，也是 Kitex 的默认策略。\n这个 lb 会依据实例的权重进行加权随机，并保证每个实例分配到的负载和自己的权重成比例。\n如果所有的实例的权重都一样，Kitex 针对这个场景做了特殊优化，会使用一个纯随机的实现，来避免加权计算的一些额外开销，所以不需要担心这个场景下的性能。\nConsistentHash 简介 一致性哈希主要适用于对上下文（如实例本地缓存）依赖程度高的场景，如希望同一个类型的请求打到同一台机器，则可使用该负载均衡方法。\n如果你不了解什么是一致性哈希，或者不知道带来的副作用，请勿使用一致性哈希。\n使用 如果要使用一致性哈希，可以在初始化 client 的时候传入 client.WithLoadBalancer(loadbalance.NewConsistBalancer(loadbalance.NewConsistentHashOption(keyFunc)))。\nConsistentHashOption 定义如下：\ntype ConsistentHashOption struct { GetKey KeyFunc // 是否使用 replica  // 如果使用，当请求失败（连接失败）后会依次尝试 replica  // 会带来额外内存和计算开销  // 如果不设置，那么请求失败（连接失败）后直接返回  Replica uint32 // 虚拟节点数  // 每个真实节点对应的虚拟节点的数量  // 这个数值越大，内存和计算代价越大，负载越均衡  // 当节点数多时，可以适当设小一些；反之可以适当设大一些  // 推荐 VirtualFactor * Weight（如果 Weighted 为 true）的中位数在 1000 左右，负载应当已经很均衡了  // 推荐 总虚拟节点数 在 2000W 以内（1000W 情况之下 build 一次需要 250ms，不过为后台 build 理论上 3s 内均无问题）  VirtualFactor uint32 // 是否要遵循 Weight 进行负载均衡  // 如果为 false，对于每个 instance 都会忽略 Weight，均生成 VirtualFactor 个虚拟节点，进行无差别负载均衡  // 如果为 true，对于每个 instance 会生成 instance.Weight() * VirtualFactor 个虚拟节点  // 需要注意，对于 weight 为 0 的 instance，无论 VirtualFactor 为多少，均不会生成虚拟节点  // 建议设为 true，不过要注意适当调小 VirtualFactor  Weighted bool // 是否进行过期处理  // 实现会缓存所有的 Key  // 如果永不过期会导致内存一直增长  // 设置过期会导致额外性能开销  // 目前的实现是每分钟扫描删除一次，以及实例发生变动 rebuild 时删除一次  // 建议一定要设置，值不要小于一分钟  ExpireDuration time.Duration } 要注意，如果 GetKey 是 nil 或者 VirtualFactor 是 0，会 panic。\n性能 经过测试，在 weight 为 10、VirtualFactor 为 100 的情况之下，不同 instance 数量的 build 性能如下：\nBenchmarkNewConsistPicker_NoCache/10ins-16 6565 160670 ns/op 164750 B/op 5 allocs/op BenchmarkNewConsistPicker_NoCache/100ins-16 571 1914666 ns/op 1611803 B/op 6 allocs/op BenchmarkNewConsistPicker_NoCache/1000ins-16 45 23485916 ns/op 16067720 B/op 10 allocs/op BenchmarkNewConsistPicker_NoCache/10000ins-16 4 251160920 ns/op 160405632 B/op 41 allocs/op 所以当有 10000 个 instance，每个 instance weight 为 10，VirtualFactor 为 100 的情况之下（总虚拟节点数 1000W），build 一次需要 251 ms。\nbuild 和 请求 信息都会被缓存，所以一次正常请求（不需要 build）的时延和节点多少无关，如下：\nBenchmarkNewConsistPicker/10ins-16 12557137 81.1 ns/op 0 B/op 0 allocs/op BenchmarkNewConsistPicker/100ins-16 13704381 82.3 ns/op 0 B/op 0 allocs/op BenchmarkNewConsistPicker/1000ins-16 14418103 81.3 ns/op 0 B/op 0 allocs/op BenchmarkNewConsistPicker/10000ins-16 13942186 81.0 ns/op 0 B/op 0 allocs/op 注意事项  下游节点发生变动时，一致性哈希结果可能会改变，某些 key 可能会发生变化； 如果下游节点非常多，第一次冷启动时 build 时间可能会较长，如果 rpc 超时短的话可能会导致超时； 如果第一次请求失败，并且 Replica 不为 0，那么会请求到 Replica 上；而第二次及以后仍然会请求 第一个 实例。  负载的均衡度 经过测试，当下游实例为 10 个时，如果 VirtualFactor 设置为 1 并且不开启 Weighted 时，负载非常不均衡，如下：\naddr2: 28629 addr7: 13489 addr3: 10469 addr9: 4554 addr0: 21550 addr6: 6516 addr8: 2354 addr4: 9413 addr5: 1793 addr1: 1233 当 VirtualFactor 设置为 10 时，负载如下：\naddr7: 14426 addr8: 12469 addr3: 8115 addr4: 8165 addr0: 8587 addr1: 7193 addr6: 10512 addr9: 14054 addr2: 9307 addr5: 7172 可以看出比 VirtualFactor 为 1 时要好很多。\n当 VirtualFactor 为 1000 时，负载如下：\naddr7: 9697 addr5: 9933 addr6: 9955 addr4: 10361 addr8: 9828 addr0: 9729 addr9: 10528 addr2: 10121 addr3: 9888 addr1: 9960 可以看出此时负载基本均衡。\n再来看看带 Weight 的情况，我们设置 addr0 的 weight 为 0，addr1 的 weight 为 1，addr2 的 weight 为 2……以此类推。\n设置 VirtualFactor 为 1000，得到负载结果如下：\naddr4: 8839 addr3: 6624 addr6: 13250 addr1: 2318 addr8: 17769 addr2: 4321 addr5: 11099 addr9: 20065 addr7: 15715 可以看到基本是和 weight 的分布一致。在这里没有 addr0 是因为 weight 为 0 是不会被调度到的。\n综上，提高 VirtualFactor，可以使得负载更加均衡，但是也要注意会增加性能开销，需要找个平衡点。\n","categories":"","description":"","excerpt":"Kitex 默认提供了两种 LoadBalancer（下面简称 lb）：\n WeightedRandom ConsistentHash …","ref":"/zh/docs/kitex/tutorials/basic-feature/loadbalance/","tags":"","title":"负载均衡"},{"body":"会议主题 ：CloudWeGo 社区会议 6.30\n参会人 ：GuangmingLuo, welkeyever, YangruiEmma, liu-song, byene0923, Ivnszn, ylck, li-jin-gou, stephenzhang0713, Li Zheming, debug-LiXiwen, joway, yccpt, Yin Xuran, JZK-Keven, Li Weiting, Fan Guangyu, Jacob953, errocks, Huang Xiaolong, towelong, powerxu519, jayantxie, baiyutang, skyenought, yiyun, baize, yunwei37, 834810071, LhdDream\n会前必读 ：http://www.cloudwego.io/；https://github.com/cloudwego\n议程 1 ：新人自我介绍 @GuangmingLuo  新成员名单：stephenzhang0713, yunwei37, errocks, Li Weiting, Huang Xiaolong, towelong, powerxu519, LhdDream, ylck, byene0923 社区新成员分别进行自我介绍，主要包含个人基本情况、开源贡献经历和后续参与社区工作内容。   议程 2 ：Good-first-issue 复盘 @GuangmingLuo  Kitex 单测任务还有两个子任务待完成，希望加快进度。后续会持续放出其它新手任务，希望大家可以保持关注并积极参与。后续针对 PR 可能会有单测覆盖率限制，希望后面每一个提交贡献的同学都能补充相关的单测，提升相关模块的单测覆盖率，保证项目的代码质量。 Hertz 文档建设进展：   先前发布了文档翻译类型的新手任务，英文文档建设是项目非常重要的一部分，新手、字节内部同学、国内熟悉英文的用户都可查看。后续也会将项目进行国际化推广，因而英文文档建设也是很有价值的。 英文文档建设可以锻炼英文翻译能力、对项目技术的理解能力，在翻译时要考虑中英文表达方式的差异，不能只是文字对照翻译。也欢迎大家后续对文档翻译进行持续优化，这也是为社区作出重要贡献的方式之一。   议程 3 ：工程化模板或标准化的讨论 @baiyutang  Issue 地址：https://github.com/cloudwego/kitex/issues/500 背景：用户在做技术选型的时候，对工具化模板是有一定诉求的，比如说怎样快速便捷生成一些基础的业务代码。对比 Go-zero 框架的 API 生成、RPC 生成、Model 生成以及模版的管理四类命令工具，Kitex 有一些生成客户端代码的命令、生成基础 Handler 方法的服务端代码命令等，如果想满足更多用户的诉求，我们可以确定一个 Layout 或者丰富工具化、生成业务代码方面的命令。 相关讨论：   Kitex 和 Hertz 有两套单独的 RPC 命令生成工具，生成 Model 可能需要一个总的工具，因为有很多业务诉求是有共性的，这个问题正在考虑中。 Q：Kitex 和 Hertz 同属一个 Group，命令是否可以相似，如果不去自动指定 Model 的话，可以自动从执行命令最近的文件夹里面找到 Go mod 文件？ A：建议提出 PR，相关同学后续会跟进。  欢迎感兴趣的同学加入讨论！\n 议程 4：Hertz 近期 Roadmap 介绍、实战案例建设、新手任务介绍等等 @welkeyever  Hertz 上周已正式官宣，内部在逐步梳理开源侧的 Roadmap。主库拆成 Hertz 对外提供的各维度的能力：   HTTP2 在内部已经有一个工程实践，内部很多组件用户已经在使用，但是出于成熟度的考量，还没有正式开源。因此首先后期会补充 HTTP2 的能力，对此感兴趣的同学可以一起参与； HTTP3 的 RFC 文档在 6 月份正式发布，这部分也是即将举办的 Byte Camp 的议题，后续的开发工作也会以 Issue 和 PR 的形式直接在主库上展开。也欢迎大家加入到开发过程中； 关于协议，如 ALPN 已经开源，后续希望组织好这些协议，把 ALPN 的能力发挥到极致。协议间无感切换是说在用户在使用 Hertz 时，它能够做到一键切换协议版本； Automatic TLS 在内部不是刚需，主要面对开源用户。其余各维度的能力也在陆续梳理中。 对于 Hz，后续会提供多场景、高定制化、开箱即用等用户自定义能力，通过 Hz 能够直接一键创建出可以快速上线的一整个代码脚架。还会涉及 API 管理以及生成工具提供一些更高层面的抽象能力，包括屏蔽掉 HTTP 协议相关的 Request Response，给用户生成一些基于 IDL、类似于 RPC 方向的开发体验。欢迎感兴趣的同学一起进行 Hz 工具的打磨。  Contrib 仓库为 Hertz 提供全方位的组件能力：   Websocket 已经在内部使用一年多，本质上是基于 Gorilla Websocket 的库做适配，因此没有直接开源。后续可作为新手任务。 反向代理与 Websocket 类似，这个实现也是基于 Golang 原生的实现做的适配，没有直接开源。后续可作为新手任务。 常用中间件（Session/Compress/Cache），每个中间件相对独立，所以希望每个同学单独承接，做独立开发。 服务治理相关能力，与 Automatic TLS 类似，在内部会直接卸载到 Service Mesh 上。服务发现、负载均衡、限流、熔断、超时，都在 Service Mesh上有对应的实现。这一系列的服务治理的相关能力也在开源的 Roadmap 中，后续会逐步地将任务梳理出来。 可观测性（Log/Trace/Metrics），我们现在已经做了一些集成的，日志能够支持具体实现注入，Trace 也有相应的埋点，Example 库也提供了类似于使用 Tracing 能力的示例，这些也在规划中。 云原生（Proxyless/一键部署/CICD），Proxyless 在服务治理能力补齐之后会开始做，直接对接 Istio，Kitex 的这部分已经在进行中，Hertz 后续也会逐步补充。一键部署是指部署到第三方云环境的能力，包括集成 CICD 等等，都是 Contrib 仓库会涵盖的。  希望已经给 Hertz 提供 PR 或 Issue 的同学多使用，帮助框架进一步做性能提升。细节部分可以在 Hertz SIG 讨论，公共事务可以在开发者交流群进行沟通。\n 议程 5：社区 Mentor 机制介绍 @GuangmingLuo  上周社区开发者交流群里进行的问卷调查，是为了有针对性地给群里各位新加入的同学提供学习成长路径和帮助，后续会针对大家的意愿，给各位同学匹配对应的 Mentor，遇到问题可以及时跟 Mentor 交流沟通，方便大家快速地学习以及真正地深入到这个项目的开发中，也帮助大家快速成为社区 Committer。   议程 6：Issue 任务答疑 Q \u0026 A 环节 Q：Hertz 和 Kitex 都要做服务治理，在功能上是不是有些重复？\nA： 二者业务场景不同，整个框架的治理能力是可扩展性的，Kitex 目前在对接 OpenSergo 和 Polaris 等项目，针对服务治理能力做一些集成对接，分别把它们封装成两套不同的服务治理 Suite 进行接入。这部分 Kitex 已经在进行中了，如果后续 Hertz 对应接口扩展性这方面完成准备，也会启动类似这样的集成对接。字节的服务治理能力是由服务网格去实现的，目前 Hertz 框架的服务治理还比较薄弱，因此我们后面会统一对接第三方的服务治理能力，以 Suite 的方法一键集成进来，需要上云的用户就可以一键集成对应不同公务云的通用服务指引能力。有个性化需求的用户可以去做一些集成的对接，但不会对框架的 Core 有侵入。\nQ：Hertz 提供 IDL 生成是不是导致与 Kitex 有重叠？\nA：在 Hertz 中，IDL 主要是用在接口描述上面，在这个接口描述布局下面，生成对应的 HTTP 框架的代码。后续 Kitex 会基于 IDL 做一部分代码生成，但 Hertz 是没有的。本质上其实可以理解为还是 HTTP 协议，跟 RPC 没有任何关系。\nQ：既然支持 Thrift 和 PB 两种语言，那是否可能自己开发出一套语言来进行接口描述？\nA：Thrift 和 PB 都只是在 Hertz 中发挥接口描述的功能。除此之外，我们内部其实都是以 Thrift 的 IDL 做一些描述，包括 RPC 是直接基于 Thrift 生成代码的。其实在 Hertz 这边，我们仅仅只是用了接口描述的能力，如果你想换成自定义协议套都是可以的。后续我们其实也会考虑是否可以设计一套通用的接口描述，撑起整个 Hertz 代码生成逻辑。\nQ：因为 Hertz 和 Kitex 字节内部已经有一些应用实践经验，我们可以不仅仅从产品设计上去考虑，而且还要把它当做技术产品去看待，响应市场的需求。针对不同的人群来讲，第一类是不太了解微服务的概念或者实践的人群，第二类是更关注性能测试的中高级的用户，Hertz 和 Kitex 是否可以给出一些最佳实践文档？\nA： 我们最近也开展了类似的源码解读活动，是面向新手和年轻开发者的活动，之后也在陆续整理一些相关概念和知识介绍。至于偏具体业务场景的使用案例，后续我们希望能有更多同学参与进来。这个其实是一个社区攻坚的过程，我们也会尽可能把字节内部已有的比较好的实践进行输出。希望大家能够就是借着这个框架可以自己去做一些相关领域的实践，我们目前也正在搭一个电商的样例，这个项目会在近期完成，最后我们会把它发布出来放在官网。一些企业用户案例也比较有借鉴意义，我们也希望能够在不同行业，比如电商、证券、游戏和机器学习等做一些企业用户的行业标杆。后续我们也会收集和整理相关企业用户进行落地页实现的案例，放在官网统一的位置做展示，让用户和需要做技术选型的同学能够快速地看到，能够了解这个项目究竟能给业务带来什么价值，能在哪些场景上铺开使用。\n","categories":"","description":"","excerpt":"会议主题 ：CloudWeGo 社区会议 6.30\n参会人 ：GuangmingLuo, welkeyever, YangruiEmma, …","ref":"/zh/community/meeting_notes/2022-06-30/","tags":"","title":"CloudWeGo 社区会议 6.30"},{"body":"What is forward proxy A forward proxy is a special network service that allows a network terminal (usually a client) to make a non-direct connection with another network terminal (usually a server) through this service. Some network devices such as gateways and routers have network proxy functions. Proxy services are generally considered to be beneficial to safeguard the privacy or security of network terminals and prevent attacks.\nA complete proxy request process is that the client first creates a connection to the proxy server, and then requests to create a connection to the target server, or to obtain the specified resources of the target server, according to the proxy protocol used by the proxy server.\nInterface // Proxy struct, which selects the proxy uri to access based on the request type Proxy func(*protocol.Request) (*protocol.URI, error) // ProxyURI is used to generate a Proxy that only returns a fixed proxy uri func ProxyURI(fixedURI *protocol.URI) Proxy // SetProxy is used to set the proxy of the client, after setting, the client will build concatenated requests with the proxy func (c *Client) SetProxy(p protocol.Proxy) Example package main import ( \"context\" \"github.com/cloudwego/hertz/pkg/app/client\" \"github.com/cloudwego/hertz/pkg/protocol\" ) func main() { proxyURL := \"http://\u003c__user_name__\u003e:\u003c__password__\u003e@\u003c__proxy_addr__\u003e:\u003c__proxy_port__\u003e\" // Convert the proxy uri to *protocol.URI  parsedProxyURL := protocol.ParseURI(proxyURL) c, err := client.NewClient() if err != nil { return } // Set proxy  c.SetProxy(protocol.ProxyURI(parsedProxyURL)) upstreamURL := \"http://google.com\" _, body, _ := client.Get(context.Background(), nil, upstreamURL) } ","categories":"","description":"","excerpt":"What is forward proxy A forward proxy is a special network service …","ref":"/docs/hertz/tutorials/basic-feature/forward-proxy/","tags":"","title":"Forward Proxy"},{"body":"何为正向代理 正向代理是一种特殊的网络服务，允许一个网络终端（一般为客户端）通过这个服务与另一个网络终端（一般为服务器）进行非直接的连接。一些网关、路由器等网络设备具备网络代理功能。一般认为代理服务有利于保障网络终端的隐私或安全，防止攻击。\n一个完整的代理请求过程为：客户端（Client）首先与代理服务器创建连接，接着根据代理服务器所使用的代理协议，请求对目标服务器创建连接、或者获得目标服务器的指定资源。\n接口 // Proxy 结构体，根据 request 来选定访问的代理 uri type Proxy func(*protocol.Request) (*protocol.URI, error) // ProxyURI 用来生成只会返回固定代理 uri 的 Proxy func ProxyURI(fixedURI *protocol.URI) Proxy // SetProxy 用来设置 client 的 proxy，设置后 client 会与 proxy 建连发请求 func (c *Client) SetProxy(p protocol.Proxy) 例子 package main import ( \"context\" \"github.com/cloudwego/hertz/pkg/app/client\" \"github.com/cloudwego/hertz/pkg/protocol\" ) func main() { proxyURL := \"http://\u003c__user_name__\u003e:\u003c__password__\u003e@\u003c__proxy_addr__\u003e:\u003c__proxy_port__\u003e\" // 将代理的 uri 转成 *protocol.URI 的形式  parsedProxyURL := protocol.ParseURI(proxyURL) c, err := client.NewClient() if err != nil { return } // 设置代理  c.SetProxy(protocol.ProxyURI(parsedProxyURL)) upstreamURL := \"http://google.com\" _, body, _ := client.Get(context.Background(), nil, upstreamURL) } ","categories":"","description":"","excerpt":"何为正向代理 正向代理是一种特殊的网络服务，允许一个网络终端（一般为客户端）通过这个服务与另一个网络终端（一般为服务器）进行非直接的连接。一 …","ref":"/zh/docs/hertz/tutorials/basic-feature/forward-proxy/","tags":"","title":"正向代理"},{"body":"Kitex provides a default implementation of Circuit Breaker, but it’s disabled by default.\nThe following document will introduce that how to enable circuit breaker and configure the policy.\nHow to use Example // build a new CBSuite cbs := circuitbreak.NewCBSuite(GenServiceCBKeyFunc) // add to the client options opts = append(opts, client.WithCircuitBreaker(cbs)) // init client cli, err := xxxservice.NewClient(targetService, opts) Introduction Kitex provides a set of CBSuite that encapsulates both service-level breaker and instance-level breaker, which are the implementions of Middleware.\n  Service-Level Breaker\nStatistics by service granularity, enabled via WithMiddleware.\nThe specific division of service granularity depends on the Circuit Breaker Key, which is the key for breaker statistics. When initializing the CBSuite, you need to pass it in GenServiceCBKeyFunc. The default key is circuitbreaker.RPCInfo2Key, and the format of RPCInfo2Key is  fromServiceName/toServiceName/method.\n  Instance-Level Breaker\nStatistics by instance granularity, enabled via WithInstanceMW.\nInstance-Level Breaker is used to solve the single-instance exception problem. If it’s triggered, the framework will automatically retry the request.\nNote that the premise of retry is that you need to enable breaker with WithInstanceMW, which will be executed after load balancing.\n  Threshold and Threshold Change\n  The default breaker threshold is ErrRate: 0.5, MinSample: 200, which means it’s triggered by an error rate of 50% and requires the count of requests \u003e 200.\nIf you want to change the threshold, you can modify the UpdateServiceCBConfig and UpdateInstanceCBConfig in CBSuite.\nThe Role of Circuit Breaker When making RPC calls, errors are inevitable for downstream services.\nWhen a downstream has a problem, if the upstream continues to make calls to it, it both prevents the downstream from recovering and wastes the upstream’s resources.\nTo solve this problem, you can set up some dynamic switches that manually shut down calls to the downstream when it goes wrong.\nA better approach, however, is to use Circuit Breaker.\nHere is a more detailed document Circuit Breaker Pattern.\nOne of the famous circuit breakers is hystrix, and here is its design.\nBreaker Strategy The idea of a Circuit Breaker is simple: restrict access to downstream based on successful failures of RPC Calls.\nThe Circuit Breaker is usually divided into three periods: CLOSED, OPEN, and HALFOPEN.\n CLOSED when the RPC is normal. OPEN when RPC errors increase. HALFOPEN after a certain cooling time after OPEN.  HALFOPEN will make some strategic access to the downstream, and then decide whether to become CLOSED or OPEN according to the result.\nIn general, the transition of the three states is roughly as follows:\n [CLOSED] ---\u003e tripped ----\u003e [OPEN]\u003c-------+ ^ | ^ | v + | detect fail | | v | cooling timeout | The timeout for cooling | v +-- detect succeed --\u003c-[HALFOPEN]--\u003e--+ Trigger Strategies Kitex provides three basic fuse triggering strategies by default:\n  Number of consecutive errors reaches threshold (ConsecutiveTripFunc)\n  Error count reaches threshold (ThresholdTripFunc)\n  Error rate reaches the threshold (RateTripFunc)\n  Of course, you can write your own triggering strategy by implementing the TripFunc function.\nCircuitbreaker will call TripFunc when Fail or Timeout happen to decide whether to trigger breaker.\nCooling Strategy After entering the OPEN state, the breaker will cool down for a period of time, the default is 10 seconds which is also configurable (with CoolingTimeout).\nDuring this period, all IsAllowed() requests will be returned false.\nEntering HALFOPEN when cooling is complete.\nHalf-Open Strategy During HALFOPEN, the breaker will let a request go every “interval”, and after a “number” of consecutive successful requests, the breaker will become CLOSED; If any of them fail, it will become OPEN.\nThe process is a gradual-trial process.\nBoth the “interval” (DetectTimeout) and the “number” (DEFAULT_HALFOPEN_SUCCESSES) are configurable.\nStatistics Default Config The breaker counts successes, failures and timeouts within a period of time window(default window size is 10 seconds).\nThe time window can be set by two parameters, but usually you can leave it alone.\nStatistics Implementation The statistics will divide the time window into buckets, each bucket recording data for a fixed period of time.\nFor example, to count data within 10 seconds, you can spread the 10 second time period over 100 buckets, each bucket counting data within a 100ms time period.\nThe BucketTime and BucketNums in Options correspond to the time period of each bucket, and the number of buckets.\nIf BucketTime is set to 100ms, and BucketNums is set to 100, this corresponds to a 10 second time window.\nJitter As time moves, the oldest bucket in the window expires. The jitter occurs when the last bucket expires.\nAs an example:\n  You divide 10 seconds into 10 buckets, bucket 0 corresponds to a time of [0S, 1S), bucket 1 corresponds to [1S, 2S), … , and bucket 9 corresponds to [9S, 10S).\n  At 10.1S, a Succ is executed, and the following operations occur within the circuitbreaker.\n (1) detects that bucket 0 has expired and discards it; (2) creates a new bucket 10, corresponding to [10S, 11S); (3) puts that Succ into bucket 10.    At 10.2S, you execute Successes() to query the number of successes in the window, then you get the actual statistics for [1S, 10.2S), not [0.2S, 10.2S).\n  Such jitter cannot be avoided if you use time-window-bucket statistics. A compromise approach is to increase the number of buckets, which can reduce the impact of jitter.\nIf 2000 buckets are divided, the impact of jitter on the overall data is at most 1/2000. In this package, the default number of buckets is 2000, the bucket time is 5ms, and the time window is 10S.\nThere are various technical solutions to avoid this problem, but they all introduce other problems, so if you have good ideas, please create a issue or PR.\n","categories":"","description":"","excerpt":"Kitex provides a default implementation of Circuit Breaker, but it’s …","ref":"/docs/kitex/tutorials/basic-feature/circuitbreaker/","tags":"","title":"Circuit Breaker"},{"body":"By default, Kitex integrates the self-developed high-performance network library Netpoll. But Kitex is not strongly bound with Netpoll, it also supports users to extend other network libraries and choose one on demand. In addition, Kitex provides ShmIPC to further improve IPC performance, this extension will be open source later.\nExtension APIs The main extension interfaces of transport module are as follows:\ntype TransServer interface {...} type ServerTransHandler interface {...} type ClientTransHandler interface {...} type ByteBuffer interface {...} type Extension interface {...} // ------------------------------------------------------------- // TransServerFactory is used to create TransServer instances. type TransServerFactory interface { NewTransServer(opt *ServerOption, transHdlr ServerTransHandler) TransServer } // ClientTransHandlerFactory to new TransHandler for client type ClientTransHandlerFactory interface { NewTransHandler(opt *ClientOption) (ClientTransHandler, error) } // ServerTransHandlerFactory to new TransHandler for server type ServerTransHandlerFactory interface { NewTransHandler(opt *ServerOption) (ServerTransHandler, error) } TransServer is the startup interface of the server, ServerTransHandler and ClientTransHandler are the message processing interfaces of the server and client respectively, ByteBuffer is the read-write interface. Under the same IO model, the code logic of the TransHandler is usually consistent, so Kitex provides the default implementation of TransHandler for synchronous IO and abstracts the extension interface for different parts. Therefore, in the scenario of synchronous IO, it is not necessary to implement the complete TransHandler interface, just implement the Extension API.\nNetpoll Extension Below figure is Kitex’s extension to netpoll synchronous IO, which implements Extension, ByteBuffer, TransServer interfaces.\nCustomized Transport Module Usage   Server Side\noption: WithTransServerFactory, WithTransHandlerFactory\nvar opts []server.Option opts = append(opts, server.WithTransServerFactory(yourTransServerFactory) opts = append(opts, server.WithTransHandlerFactory(yourTransHandlerFactory) svr := xxxservice.NewServer(handler, opts...)   Client Side\noption: WithTransHandlerFactory\ncli, err := xxxservice.NewClient(targetService, client.WithTransHandlerFactory(yourTransHandlerFactory)   ","categories":"","description":"","excerpt":"By default, Kitex integrates the self-developed high-performance …","ref":"/docs/kitex/tutorials/framework-exten/transport/","tags":"","title":"Extension of Transport Module"},{"body":"Kitex 默认集成了自研的高性能网络库 Netpoll，但没有与 Netpoll 强绑定，同时也支持使用者扩展其他网络库按需选择。Kitex 还提供了 ShmIPC 进一步提升 IPC 性能，该扩展会在后续开源。\n扩展接口 传输模块主要的扩展接口如下：\ntype TransServer interface {...} type ServerTransHandler interface {...} type ClientTransHandler interface {...} type ByteBuffer interface {...} type Extension interface {...} // ------------------------------------------------------------- // TransServerFactory is used to create TransServer instances. type TransServerFactory interface { NewTransServer(opt *ServerOption, transHdlr ServerTransHandler) TransServer } // ClientTransHandlerFactory to new TransHandler for client type ClientTransHandlerFactory interface { NewTransHandler(opt *ClientOption) (ClientTransHandler, error) } // ServerTransHandlerFactory to new TransHandler for server type ServerTransHandlerFactory interface { NewTransHandler(opt *ServerOption) (ServerTransHandler, error) } TransServer 是服务端的启动接口，ServerTransHandler 和 ClientTransHandler 分别是服务端和调用端对消息的处理接口，ByteBuffer 是读写接口。相同的 IO 模型下 TransHandler 的逻辑通常是一致的，Kitex 对同步 IO 提供了默认实现的 TransHandler，针对不一样的地方抽象出了 Extension 接口，所以在同步 IO 的场景下不需要实现完整的 TransHandler 接口，只需实现 Extension 即可。\nNetpoll 的扩展 如下是 Kitex 对 Netpoll 同步 IO 的扩展，分别实现了Extension、ByteBuffer、TransServer 接口。\n指定自定义的传输模块   服务端\noption: WithTransServerFactory, WithTransHandlerFactory\nvar opts []server.Option opts = append(opts, server.WithTransServerFactory(yourTransServerFactory) opts = append(opts, server.WithTransHandlerFactory(yourTransHandlerFactory) svr := xxxservice.NewServer(handler, opts...)   调用端\noption: WithTransHandlerFactory\ncli, err := xxxservice.NewClient(targetService, client.WithTransHandlerFactory(yourTransHandlerFactory)   ","categories":"","description":"","excerpt":"Kitex 默认集成了自研的高性能网络库 Netpoll，但没有与 Netpoll 强绑定，同时也支持使用者扩展其他网络库按需选 …","ref":"/zh/docs/kitex/tutorials/framework-exten/transport/","tags":"","title":"传输模块扩展"},{"body":"Kitex 提供了熔断器的实现，但是没有默认开启，需要用户主动使用。\n下面简单介绍一下如何使用以及 Kitex 熔断器的策略。\n如何使用 使用示例： // build a new CBSuite cbs := circuitbreak.NewCBSuite(GenServiceCBKeyFunc) // add to the client options opts = append(opts, client.WithCircuitBreaker(cbs)) // init client cli, err := xxxservice.NewClient(targetService, opts) 使用说明 Kitex 大部分服务治理模块都是通过 middleware 集成，熔断也是一样。Kitex 提供了一套 CBSuite，封装了服务粒度的熔断器和实例粒度的熔断器。\n  服务粒度熔断\n按照服务粒度进行熔断统计，通过 WithMiddleware 添加。服务粒度的具体划分取决于 Circuit Breaker Key，既熔断统计的 key，初始化 CBSuite 时需要传入 GenServiceCBKeyFunc，默认提供的是 circuitbreaker.RPCInfo2Key ，该 key 的格式是 fromServiceName/toServiceName/method，即按照方法级别的异常做熔断统计。\n  实例粒度熔断\n按照实例粒度进行熔断统计，主要用于解决单实例异常问题，如果触发了实例级别熔断，框架会自动重试。\n注意，框架自动重试的前提是需要通过 WithInstanceMW 添加，WithInstanceMW 添加的 middleware 会在负载均衡后执行。\n  熔断阈值及阈值变更\n默认的熔断阈值是 ErrRate: 0.5, MinSample: 200，错误率达到 50% 触发熔断，同时要求统计量 \u003e200。若要调整阈值，调用 CBSuite 的 UpdateServiceCBConfig 和 UpdateInstanceCBConfig 来更新 Key 的阈值。\n   熔断器作用 在进行 RPC 调用时，下游服务难免会出错；\n当下游出现问题时，如果上游继续对其进行调用，既妨碍了下游的恢复，也浪费了上游的资源；\n为了解决这个问题，你可以设置一些动态开关，当下游出错时，手动的关闭对下游的调用；\n然而更好的办法是使用熔断器，自动化的解决这个问题。\n这里是一篇更详细的熔断器介绍。\n比较出名的熔断器当属 hystrix 了，这里是它的设计文档。\n熔断策略 熔断器的思路很简单：根据 RPC 的成功失败情况，限制对下游的访问；\n通常熔断器分为三个时期： CLOSED、OPEN、HALFOPEN；\nRPC 正常时，为 CLOSED；\n当 RPC 错误增多时，熔断器会被触发，进入 OPEN；\nOPEN 后经过一定的冷却时间，熔断器变为 HALFOPEN；\nHALFOPEN 时会对下游进行一些有策略的访问，然后根据结果决定是变为 CLOSED，还是 OPEN；\n总的来说三个状态的转换大致如下图：\n [CLOSED] ---\u003e tripped ----\u003e [OPEN]\u003c-------+ ^ | ^ | v | + | detect fail | | | | cooling timeout | ^ | ^ | v | +--- detect succeed --\u003c-[HALFOPEN]--\u003e--+ 触发策略 Kitex 默认提供了三个基本的熔断触发策略：\n  连续错误数达到阈值 (ConsecutiveTripFunc)\n  错误数达到阈值 (ThresholdTripFunc)\n  错误率达到阈值 (RateTripFunc)\n  当然，你可以通过实现 TripFunc 函数来写自己的熔断触发策略；\nCircuitbreaker 会在每次 Fail 或者 Timeout 时，去调用 TripFunc，来决定是否触发熔断；\n冷却策略 进入 OPEN 状态后，熔断器会冷却一段时间，默认是 10 秒，当然该参数可配置 (CoolingTimeout)；\n在这段时期内，所有的 IsAllowed() 请求将会被返回 false；\n冷却完毕后进入 HALFOPEN；\n半打开时策略 在 HALFOPEN 时，熔断器每隔 \" 一段时间 \" 便会放过一个请求，当连续成功 \" 若干数目 \" 的请求后，熔断器将变为 CLOSED； 如果其中有任意一个失败，则将变为 OPEN；\n该过程是一个逐渐试探下游，并打开的过程；\n上述的 \" 一段时间 “(DetectTimeout) 和 \" 若干数目 “(DEFAULT_HALFOPEN_SUCCESSES) 都是可以配置的；\n统计 默认参数 熔断器会统计一段时间窗口内的成功，失败和超时，默认窗口大小是 10S；\n时间窗口可以通过两个参数设置，不过通常情况下你可以不用关心 .\n统计方法 统计方法是将该段时间窗口分为若干个桶，每个桶记录一定固定时长内的数据；\n比如统计 10 秒内的数据，于是可以将 10 秒的时间段分散到 100 个桶，每个桶统计 100ms 时间段内的数据；\nOptions 中的 BucketTime 和 BucketNums，就分别对应了每个桶维护的时间段，和桶的个数；\n如将 BucketTime 设置为 100ms，将 BucketNums 设置为 100，则对应了 10 秒的时间窗口；\n抖动 随着时间的移动，窗口内最老的那个桶会过期，当最后那个桶过期时，则会出现了抖动；\n举个例子：\n  你将 10 秒分为了 10 个桶，0 号桶对应了 [0S，1S) 的时间，1 号桶对应 [1S，2S)，…，9 号桶对应 [9S，10S)；\n  在 10.1S 时，执行一次 Succ，则 circuitbreaker 内会发生下述的操作；\n (1) 检测到 0 号桶已经过期，将其丢弃； (2) 创建新的 10 号桶，对应 [10S，11S)； (3) 将该次 Succ 放入 10 号桶内；    在 10.2S 时，你执行 Successes() 查询窗口内成功数，则你得到的实际统计值是 [1S，10.2S) 的数据，而不是 [0.2S，10.2S)；\n  如果使用分桶计数的办法，这样的抖动是无法避免的，比较折中的一个办法是将桶的个数增多，可以降低抖动的影响；\n如划分 2000 个桶，则抖动对整体的数据的影响最多也就 1/2000； 在该包中，默认的桶个数也是 2000，桶时间为 5ms，总体窗口为 10S；\n当时曾想过多种技术办法来避免这种问题，但是都会引入更多其他的问题，如果你有好的思路，请 issue 或者 PR.\n","categories":"","description":"","excerpt":"Kitex 提供了熔断器的实现，但是没有默认开启，需要用户主动使用。\n下面简单介绍一下如何使用以及 Kitex 熔断器的策略。\n如何使用 使 …","ref":"/zh/docs/kitex/tutorials/basic-feature/circuitbreaker/","tags":"","title":"熔断器"},{"body":"会议主题 ：CloudWeGo 社区会议 7.14\n参会人 ：GuangmingLuo, Cheng Guozhu, simon0-o, welkeyever, YangruiEmma, liu-song, Ivnszn, CoderPoet, li-jin-gou, joway, bodhisatan, Fan Guangyu, Jacob953, Wang Yafeng, gova, Huang Xiaolong, Zhang Guiyuan, chenzBin, yccpt, jayantxie, baiyutang, skyenought, yiyun, rogerogers, Zhou Xinyi, baize, LhdDream, Li Congyan, Liu Jia\n会前必读 ：http://www.cloudwego.io/；https://github.com/cloudwego\n议程 1 ：新人自我介绍  新成员名单：@王亚峰 @张桂元 @周鑫宜 @rogerogers 社区新成员分别进行自我介绍，主要包含个人基本情况、开源贡献经历和后续参与社区工作内容。   议程 2 ：Hertz-Contrib/Limiter 组件分享 @LhdDream  介绍PPT：过载保护-限流算法.pptx 相关讨论：   Q： 造成 CPU 负载的因素很多，如何判断这是通过访问量或者高并发请求产生的负载？有时用户的加码程序或者某些在系统上跑的程序也会导致 CPU 负载很高，会不会有限流失误的问题？ A： 如果一个程序出现了问题，CPU 已经负载很高的时候，也没有必要再承担一个请求，因为这个机器的性能已经达到了负荷。  后续补充限流算法相关案例和使用算法的趋势图，方便直观感受使用这个组件带来的收益。   议程 3 ：Hertz-Contrib/Obs-Opentelemetry 设计与应用场景介绍 @CoderPoet  地址：github.com/hertz-contrib/obs-opentelemetry    默认提供开箱即用 OpenTelemetry Provider；\n  对 Hertz 做了一些 Instrumentation，主要有三点：\n Tracing  Support server and client Hertz http tracing Support automatic transparent transmission of peer service through http headers // 基于对端服务信息透传，实现服务拓扑能力   Metrics  Support Hertz http metrics [R.E.D] // 做 http metrics 的埋点，实现一些服务的黄金指标 Support service topology map metrics [Service Topology Map] // 基于 http headers 透传对端服务信息，生成 Service Topology Map Support go runtime metrics   Logging  Extend Hertz logger based on logrus Implement tracing auto associated logs // 拓展 Hertz logger 接口，基于 logrus hook 机制，从 Context 里面提取相应的 trace context 放到日志里，通过这样的模式实现 trace context 和日志的串联      OpenTelemetry 目标是实现 Tracing/Metrics/Logging 三个数据的互联互通，但三者本身的成熟度上不同步，在社区状态中，Tracing 基本都是 Stable，Metrics 只有 API 和协议是 Stable 状态，Logging 是 Draft 状态。相关链接：https://opentelemetry.io/status/ Hertz 并不是把 Logging 的 API 集成起来，而只是把协议里面提到的比如 Log Model、Trace ID 如何定义等规范集成，所以即使 Logging 没有达到一定成熟度，也可以使用。关于使用场景：   如果想要实现全链路观测，可以直接集成该。比如访问 Hertz Server 和 Kitex Server 会有一个简单的链路串联，可以输入一些自定义的属性，并且默认也会帮你输入根据 OpenTelemetry 语法规范做的、协议相关的属性； 如果想自动做请求维度的一些 RED 指标，比如计算 QPS，只要去把数据源导入就可以做相应的面板绘制； Runtime Metrics 也做了自动集成，可以在 Dashboard 里面绘制相应状态； 最新的 Jaeger 已经原生支持 OTLP Protocol 获取协议，相当于我们的库可以直接跟 Jaeger Collector 做集成，不需要用 OpenTelemetry Collector 做数据中转。  使用场景：github.com/cloudwego/hertz-examples/tree/main/opentelemetry\n相关讨论：   Q： 如果在 Hertz 使用 Obs 扩展，比如有一个 Trace ID，想快速找到有问题的请求，有没有可能就是把这个 Trace ID 或者是能够唯一标识这一次链路追踪的 ID 返回到 Response 里面去呢？ A： 目前对于这种错误链路，可以在尾采样中做异常全采，不用借助 Response，可以直接在链路搜索里面找到相应的错误那条 Trace，然后看它上游或者下游哪些地方发生了异常。   议程 4：关于 Hertz-Template 的优化建议与讨论 @skyenought  相关文档：关于 Hertz template 的新 feat  原本如果要定义 Template，所有内容都写在 YAML 文件里，需要转移符号判断文本，这样看起来可读性比较差、耦合度高。解决方案是不使用 Body 关键字，添加 TemplatePath，只描述Template 文件在这个项目中的位置，这样分散开来比较方便修改和浏览。经过逻辑判断，要保证 Body 和 TemplatePath 不能同时使用，这样可能会造成混乱。如果是 Body 就直接读 Body 的值，如果是 TemplatePath 就通过 IO 把内容读进来以后再进行模版分析。优化实现有待进一步讨论。\n相关文档：Hertz 和 Kitex 对于 IDL 的不同处理  Q： CloudWeGo 一个组织中，代码风格却大不相同。Kitex 因为有 Netpoll 存在，只针对 Linux 环境，所以对后缀不做限定。Hertz 在 Go net, windows 和 Linux 环境都可进行开发，它拥有强规定。它们为什么不能统一风格呢？\nA： Apache Thrift 的官方并没有对 Thrift 文件后缀有明确规定，从长期大量的实践来看，有很多用户不会把 Thrift 文件的后缀给改为 .Thrift。在内部，以 HTTP 的 IDL 举例，基本都是以 Thrift 或者 PB 的形式存在，所以说我们没有考虑制定拓展名。\n 议程 5： 关于新增的 Biz-Demo 的后续规划 @GuangmingLuo  地址：https://github.com/cloudwego/biz-demo 新增 Biz-Demo 仓库。第一，存放同时集成 Hertz example 和 Kitex example 的案例；第二，存放各行各业最佳企业落地实践 Examples。正式呼吁感兴趣的同学提交有价值的业务案例！ 提交案例可以帮助同学从新手期向成熟期过渡，同时可以更深入地了解各个技术栈，得到较大的自我提升。后续会将好的 Business Demo 做一些推广，在官网上和公众号上都会有展示。参考案例：https://github.com/cloudwego/kitex-examples/pull/28   议程 6：Hertz 源码解读活动介绍 @yiyun  源码解读活动一期结束，对于 RPC 相关基础知识整理了 1.6 万字，可以在 Community 仓库查看。 源码解读活动二期已经开始，期间有四期直播分享：   了解 HTTP 框架的设计； 上手企业级 HTTP 框架 Hertz 的操作实践； CSG 一期源码解读优秀成员分享如何进行源码解读； 社区 Committer 和 Go 夜读作者分享，如何规划自己的代码学习和提升路径。  欢迎大家关注 CloudWeGo 公众号获取相关信息。\n活动相关资料 Issue 地址：https://github.com/cloudwego/community/issues/33\n第一期直播回顾：https://meetings.feishu.cn/s/1i38ftnck0f18?src_type=3\u0026disable_cross_redirect=true\n第二期直播回顾：https://meetings.feishu.cn/s/1i3fsqit6jchu?src_type=3\n","categories":"","description":"","excerpt":"会议主题 ：CloudWeGo 社区会议 7.14\n参会人 ：GuangmingLuo, Cheng Guozhu, simon0-o, …","ref":"/zh/community/meeting_notes/2022-07-14/","tags":"","title":"CloudWeGo 社区会议 7.14"},{"body":"A good project can’t be built without unit tests. To help users build good projects, hertz of course provides unit testing tools.\nThe principle is similar to that of golang httptest, both of them just execute ServeHTTP without going through the network and return the response after execution.\nExample import ( \"bytes\" \"context\" \"testing\" \"github.com/cloudwego/hertz/pkg/app\" \"github.com/cloudwego/hertz/pkg/common/config\" \"github.com/cloudwego/hertz/pkg/common/test/assert\" \"github.com/cloudwego/hertz/pkg/common/ut\" \"github.com/cloudwego/hertz/pkg/route\" ) func TestPerformRequest(t *testing.T) { router := route.NewEngine(config.NewOptions([]config.Option{})) router.GET(\"/hey/:user\", func(ctx context.Context, c *app.RequestContext) { user := c.Param(\"user\") assert.DeepEqual(t, \"close\", c.Request.Header.Get(\"Connection\")) c.Response.SetConnectionClose() c.JSON(201, map[string]string{\"hi\": user}) }) w := ut.PerformRequest(router, \"GET\", \"/hey/hertz\", \u0026ut.Body{bytes.NewBufferString(\"1\"), 1}, ut.Header{\"Connection\", \"close\"}) resp := w.Result() assert.DeepEqual(t, 201, resp.StatusCode()) assert.DeepEqual(t, \"{\\\"hi\\\":\\\"hertz\\\"}\", string(resp.Body())) } For more examples, refer to the unit test file in pkg/common/ut.\n","categories":"","description":"","excerpt":"A good project can’t be built without unit tests. To help users build …","ref":"/docs/hertz/tutorials/basic-feature/unit-test/","tags":"","title":"Unit Test"},{"body":"一个好的项目的构建离不开单元测试。为了帮助使用者构建出好的项目，hertz 当然也提供了单元测试的工具。\n原理和 golang httptest 类似，都是不经过网络只执行 ServeHTTP 返回执行后的 response。\n例子 import ( \"bytes\" \"context\" \"testing\" \"github.com/cloudwego/hertz/pkg/app\" \"github.com/cloudwego/hertz/pkg/common/config\" \"github.com/cloudwego/hertz/pkg/common/test/assert\" \"github.com/cloudwego/hertz/pkg/common/ut\" \"github.com/cloudwego/hertz/pkg/route\" ) func TestPerformRequest(t *testing.T) { router := route.NewEngine(config.NewOptions([]config.Option{})) router.GET(\"/hey/:user\", func(ctx context.Context, c *app.RequestContext) { user := c.Param(\"user\") assert.DeepEqual(t, \"close\", c.Request.Header.Get(\"Connection\")) c.Response.SetConnectionClose() c.JSON(201, map[string]string{\"hi\": user}) }) w := ut.PerformRequest(router, \"GET\", \"/hey/hertz\", \u0026ut.Body{bytes.NewBufferString(\"1\"), 1}, ut.Header{\"Connection\", \"close\"}) resp := w.Result() assert.DeepEqual(t, 201, resp.StatusCode()) assert.DeepEqual(t, \"{\\\"hi\\\":\\\"hertz\\\"}\", string(resp.Body())) } 更多 examples 参考 pkg/common/ut 中的单测文件。\n","categories":"","description":"","excerpt":"一个好的项目的构建离不开单元测试。为了帮助使用者构建出好的项目，hertz 当然也提供了单元测试的工具。\n原理和 golang …","ref":"/zh/docs/hertz/tutorials/basic-feature/unit-test/","tags":"","title":"单测"},{"body":"Metadata transparently transmission transmits some additional RPC information to the downstream based on the transport protocol, and reads the upstream transparently transmitted information carried by transport protocol. The transparently transmitted field needs to be combined with the internal governance capability. It is recommended that users extend and implement it by themselves.\nExtension API // MetaHandler reads or writes metadata through certain protocol. // The protocol will be a thrift.TProtocol usually. type MetaHandler interface { WriteMeta(ctx context.Context, msg Message) (context.Context, error) ReadMeta(ctx context.Context, msg Message) (context.Context, error) } Extension Example  ClientMetaHandler  package transmeta var\tClientTTHeaderHandler remote.MetaHandler = \u0026clientTTHeaderHandler{} // clientTTHeaderHandler implement remote.MetaHandler type clientTTHeaderHandler struct{} // WriteMeta of clientTTHeaderHandler writes headers of TTHeader protocol to transport func (ch *clientTTHeaderHandler) WriteMeta(ctx context.Context, msg remote.Message) (context.Context, error) { ri := msg.RPCInfo() transInfo := msg.TransInfo() hd := map[uint16]string{ transmeta.FromService: ri.From().ServiceName(), transmeta.FromIDC: ri.From().DefaultTag(consts.IDC, \"\"), transmeta.FromMethod: ri.From().Method(), transmeta.ToService: ri.To().ServiceName(), transmeta.ToMethod: ri.To().Method(), transmeta.MsgType: strconv.Itoa(int(msg.MessageType())), } transInfo.PutTransIntInfo(hd) if metainfo.HasMetaInfo(ctx) { hd := make(map[string]string) metainfo.SaveMetaInfoToMap(ctx, hd) transInfo.PutTransStrInfo(hd) } return ctx, nil } // ReadMeta of clientTTHeaderHandler reads headers of TTHeader protocol from transport func (ch *clientTTHeaderHandler) ReadMeta(ctx context.Context, msg remote.Message) (context.Context, error) { ri := msg.RPCInfo() remote := remoteinfo.AsRemoteInfo(ri.To()) if remote == nil { return ctx, nil } transInfo := msg.TransInfo() strInfo := transInfo.TransStrInfo() ad := strInfo[transmeta.HeaderTransRemoteAddr] if len(ad) \u003e 0 { // when proxy case to get the actual remote address \t_ = remote.SetRemoteAddr(utils.NewNetAddr(\"tcp\", ad)) } return ctx, nil }   Customized ClientMetaHandler Usage\ncli, err := xxxservice.NewClient(targetService, client.WithMetaHandler(transmeta.ClientTTHeaderHandler))   ","categories":"","description":"","excerpt":"Metadata transparently transmission transmits some additional RPC …","ref":"/docs/kitex/tutorials/framework-exten/transmeta/","tags":"","title":"Extension of Metadata Transparent Transmission"},{"body":"Transport Pipeline refers to Netty ChannelPipeline and provides Inbound and Outbound interfaces to support message or I/O event extensions. TLS, Traffic Limit, Transparent Transmission Processing can be extended based on In/OutboundHandler. As shown in the figure below, each BoundHandler is executed in series.\nExtension API // OutboundHandler is used to process write event. type OutboundHandler interface { Write(ctx context.Context, conn net.Conn, send Message) (context.Context, error) } // InboundHandler is used to process read event. type InboundHandler interface { OnActive(ctx context.Context, conn net.Conn) (context.Context, error) OnInactive(ctx context.Context, conn net.Conn) context.Context OnRead(ctx context.Context, conn net.Conn) (context.Context, error) OnMessage(ctx context.Context, args, result Message) (context.Context, error) } Default Extensions   Traffic Limit Handler of Server Side\nKitex supports connection level and request level limiting. The purpose of limiting is to ensure service availability. When the threshold is reached, the request should be limited in time. And the purpose of implementing limit in transport layer is to limit traffic in a timely manner. The implementation is in limiter_inbound.go.\n Limiting of Connection level implements OnActive(), OnInactive() Limiting of Request level implements OnRead()    Metadata Transparent Transmission Handler\nMeta information transparent transmission is to transmit some RPC additional information to the downstream based on the transport protocol, and read the upstream transparent transmission information carried by transport protocol. The implementation is in transmeta_bound.go.\n Write metainfo implements Write() Read metainfo implements OnMessage()  In order to make it more convenient to extend Metadata Transparent Transmission for users, Kitex defines the separately extension API MetaHandler.\n// MetaHandler reads or writes metadata through certain protocol. type MetaHandler interface { WriteMeta(ctx context.Context, msg Message) (context.Context, error) ReadMeta(ctx context.Context, msg Message) (context.Context, error) }   Customized BoundHandler Usage   Server Side\noption: WithBoundHandler\nsvr := xxxservice.NewServer(handler, server.WithBoundHandler(yourBoundHandler))   Client Side\noption: WithBoundHandler\ncli, err := xxxservice.NewClient(targetService, client.WithBoundHandler(yourBoundHandler))   ","categories":"","description":"","excerpt":"Transport Pipeline refers to Netty ChannelPipeline and provides …","ref":"/docs/kitex/tutorials/framework-exten/trans_pipeline/","tags":"","title":"Extension of Transport Pipeline-Bound"},{"body":"Rate limiting is an imperative technique to protect server, which prevents server from overloaded by sudden traffic increase from a client.\nKitex supports max connections limit and max QPS limit. You may add an Option during the server initialization, for example:\nimport \"github.com/cloudwego/kitex/pkg/limit\" func main() { svr := xxxservice.NewServer(handler, server.WithLimit(\u0026limit.Option{MaxConnections: 10000, MaxQPS: 1000})) svr.Run() } Parameter description：\n  MaxConnections: max connections\n  MaxQPS: max QPS (Queries Per Second)\n  UpdateControl: provide the ability to modify the rate limit threshold dynamically, for example:\nimport \"github.com/cloudwego/kitex/pkg/limit\" // define your limiter updater to update limit threshold type MyLimiterUpdater struct { updater limit.Updater } func (lu *MyLimiterUpdater) YourChange() { // your logic: set new option as needed \tnewOpt := \u0026limit.Option{ MaxConnections: 20000, MaxQPS: 2000, } // update limit config \tisUpdated := lu.updater.UpdateLimit(newOpt) // your logic } func (lu *MyLimiterUpdater) UpdateControl(u limit.Updater) { lu.updater = u } //--- init server --- var lu = MyLimiterUpdater{} svr := xxxservice.NewServer(handler, server.WithLimit(\u0026limit.Option{MaxConnections: 10000, MaxQPS: 1000, UpdateControl: lu.UpdateControl}))   Implementation ConcurrencyLimiter and RateLimiter are used respectively to limit max connection and max QPS.\n ConcurrencyLimiter：a simple counter； RateLimiter：token bucket algorithm is used here.  Monitoring Rate limiting defines the LimitReporter interface, which is used by rate limiting status monitoring, e.g. connection overloaded, QPS overloaded, etc.\nUsers may implement this interface and inject this implementation by WithLimitReporter if required.\n// LimitReporter is the interface define to report(metric or print log) when limit happen type LimitReporter interface { ConnOverloadReport() QPSOverloadReport() } ","categories":"","description":"","excerpt":"Rate limiting is an imperative technique to protect server, which …","ref":"/docs/kitex/tutorials/basic-feature/limiting/","tags":"","title":"Rate Limiting"},{"body":"Transport Pipeline 参考 Netty ChannelPipeline，提供 Inbound 和 Outbound 接口，支持对消息或 I/O 事件扩展。基于 In/OutboundHandler 可以扩展实现 TLS、限流、透传信息处理等。如下图所示，各个 BoundHandler 会串行依次执行。\n接口定义 // OutboundHandler is used to process write event. type OutboundHandler interface { Write(ctx context.Context, conn net.Conn, send Message) (context.Context, error) } // InboundHandler is used to process read event. type InboundHandler interface { OnActive(ctx context.Context, conn net.Conn) (context.Context, error) OnInactive(ctx context.Context, conn net.Conn) context.Context OnRead(ctx context.Context, conn net.Conn) (context.Context, error) OnMessage(ctx context.Context, args, result Message) (context.Context, error) } 默认扩展   服务端限流 Handler\nKitex 支持连接级别和请求级别限流，限流是为了保障服务的可用性，当达到阈值应当及时限流，放到 Transport 层可以达到及时限流的目的，实现见 limiter_inbound.go。\n 连接级别限流 OnActive(), OnInactive() 请求级别限流 OnRead()    元信息透传 Handler\n元信息透传是基于传输协议透传一些 RPC 额外的信息给下游，同时读取传输协议中上游透传的信息，实现见 transmeta_bound.go。\n 写入透传信息 Write() 读取透传信息 OnMessage()  为更明确的为使用者元信息透传的扩展能力，Kitex 单独定义了信息透传的处理接口 MetaHandler，这里会执行 MetaHandler 进行透传信息的处理。\n// MetaHandler reads or writes metadata through certain protocol. type MetaHandler interface { WriteMeta(ctx context.Context, msg Message) (context.Context, error) ReadMeta(ctx context.Context, msg Message) (context.Context, error) }   指定自定义的 BoundHandler   服务端\noption: WithBoundHandler\nsvr := xxxservice.NewServer(handler, server.WithBoundHandler(yourBoundHandler))   调用端\noption: WithBoundHandler\ncli, err := xxxservice.NewClient(targetService, client.WithBoundHandler(yourBoundHandler))   ","categories":"","description":"","excerpt":"Transport Pipeline 参考 Netty ChannelPipeline，提供 Inbound 和 Outbound 接口，支 …","ref":"/zh/docs/kitex/tutorials/framework-exten/trans_pipeline/","tags":"","title":"Transport Pipeline-Bound 扩展"},{"body":"元信息透传是基于传输协议透传一些额外的 RPC 信息给下游，同时读取传输协议中上游透传的信息，透传字段需结合内部的治理能力，建议使用者自行扩展实现。\n接口定义 // MetaHandler reads or writes metadata through certain protocol. // The protocol will be a thrift.TProtocol usually. type MetaHandler interface { WriteMeta(ctx context.Context, msg Message) (context.Context, error) ReadMeta(ctx context.Context, msg Message) (context.Context, error) } 扩展示例  ClientMetaHandler  package transmeta var\tClientTTHeaderHandler remote.MetaHandler = \u0026clientTTHeaderHandler{} // clientTTHeaderHandler implement remote.MetaHandler type clientTTHeaderHandler struct{} // WriteMeta of clientTTHeaderHandler writes headers of TTHeader protocol to transport func (ch *clientTTHeaderHandler) WriteMeta(ctx context.Context, msg remote.Message) (context.Context, error) { ri := msg.RPCInfo() transInfo := msg.TransInfo() hd := map[uint16]string{ transmeta.FromService: ri.From().ServiceName(), transmeta.FromIDC: ri.From().DefaultTag(consts.IDC, \"\"), transmeta.FromMethod: ri.From().Method(), transmeta.ToService: ri.To().ServiceName(), transmeta.ToMethod: ri.To().Method(), transmeta.MsgType: strconv.Itoa(int(msg.MessageType())), } transInfo.PutTransIntInfo(hd) if metainfo.HasMetaInfo(ctx) { hd := make(map[string]string) metainfo.SaveMetaInfoToMap(ctx, hd) transInfo.PutTransStrInfo(hd) } return ctx, nil } // ReadMeta of clientTTHeaderHandler reads headers of TTHeader protocol from transport func (ch *clientTTHeaderHandler) ReadMeta(ctx context.Context, msg remote.Message) (context.Context, error) { ri := msg.RPCInfo() remote := remoteinfo.AsRemoteInfo(ri.To()) if remote == nil { return ctx, nil } transInfo := msg.TransInfo() strInfo := transInfo.TransStrInfo() ad := strInfo[transmeta.HeaderTransRemoteAddr] if len(ad) \u003e 0 { // when proxy case to get the actual remote address \t_ = remote.SetRemoteAddr(utils.NewNetAddr(\"tcp\", ad)) } return ctx, nil }   添加该 ClientMetaHandler\ncli, err := xxxservice.NewClient(targetService, client.WithMetaHandler(transmeta.ClientTTHeaderHandler))   ","categories":"","description":"","excerpt":"元信息透传是基于传输协议透传一些额外的 RPC 信息给下游，同时读取传输协议中上游透传的信息，透传字段需结合内部的治理能力，建议使用者自行扩 …","ref":"/zh/docs/kitex/tutorials/framework-exten/transmeta/","tags":"","title":"元信息传递扩展"},{"body":"限流是一种保护 server 的措施，防止上游某个 client 流量突增导致 server 端过载。\n目前 Kitex 支持限制最大连接数和最大 QPS，在初始化 server 的时候，增加一个 Option，举例：\nimport \"github.com/cloudwego/kitex/pkg/limit\" func main() { svr := xxxservice.NewServer(handler, server.WithLimit(\u0026limit.Option{MaxConnections: 10000, MaxQPS: 1000})) svr.Run() } 参数说明：\n  MaxConnections 表示最大连接数\n  MaxQPS 表示最大 QPS\n  UpdateControl 提供动态修改限流阈值的能力，举例：\nimport \"github.com/cloudwego/kitex/pkg/limit\" // define your limiter updater to update limit threshold type MyLimiterUpdater struct { updater limit.Updater } func (lu *MyLimiterUpdater) YourChange() { // your logic: set new option as needed  newOpt := \u0026limit.Option{ MaxConnections: 20000, MaxQPS: 2000, } // update limit config  isUpdated := lu.updater.UpdateLimit(newOpt) // your logic } func (lu *MyLimiterUpdater) UpdateControl(u limit.Updater) { lu.updater = u } //--- init server --- var lu = MyLimiterUpdater{} svr := xxxservice.NewServer(handler, server.WithLimit(\u0026limit.Option{MaxConnections: 10000, MaxQPS: 1000, UpdateControl: lu.UpdateControl}))   实现 分别使用 ConcurrencyLimiter 和 RateLimiter 对最大连接数和最大 QPS 进行限流。\n ConcurrencyLimiter：简单的计数器； RateLimiter：这里的限流算法采用了 \" 令牌桶算法 “。  监控 限流定义了 LimitReporter 接口，用于限流状态监控，例如当前连接数过多、QPS 过大等。\n如有需求，用户需要自行实现该接口，并通过 WithLimitReporter 注入。\n// LimitReporter is the interface define to report(metric or print log) when limit happen type LimitReporter interface { ConnOverloadReport() QPSOverloadReport() } ","categories":"","description":"","excerpt":"限流是一种保护 server 的措施，防止上游某个 client 流量突增导致 server 端过载。\n目前 Kitex 支持限制最大连接数 …","ref":"/zh/docs/kitex/tutorials/basic-feature/limiting/","tags":"","title":"限流"},{"body":"会议主题 ：CloudWeGo 社区会议 7.28\n参会人 ：li-jin-gou, liu-song, GuangmingLuo, pkumza, ag9920, lsjbd, sinnera, welkeyever, YangruiEmma, CoderPoet, stephenzhang0713, joway, Quan Hu, zstone12, Yin Xuran, bodhisatan, Suo Dianjun, Fan Guangyu, Jacob953, Zhang Guiyuan, ppzqh, HeyJavaBean, simon0-o, jayantxie, daidai21, baiyutang, rogerogers, Zhou Xinyi, skyenought, yiyun, cyyolo, baize, Sunxy88\n会前必读 ：http://www.cloudwego.io/；https://github.com/cloudwego\n议程 1：Kitex \u0026 Hertz 对接 sentinel-go 方案和实现介绍 @GuangmingLuo @skyenought  相关文档：ybwflbcn12.feishu.cn/docx/doxcnXcNmOMPaWasGNNQ1dib7xh 基于 CloudWeGo 和 OpenSergo 项目合作的背景下，我们会从开源方面做一些合作对接。Sentinel 会作为 OpenSergo 的具体实现，服务治理的相关标准会沉淀在 OpenSergo 里面，Kitex 与 Hertz 对接 sentinel-go 的 PR 均已经被合入。与 Gin 集成方式保持一致，通过 Middleware 的形式，集成 sentinel-go 的 Entry。 sentinel-go 只提供了一个高度封装的方法，对外只能通过中间件的方式进行。提交到 sentinel-go 仓库里的代码后续维护情况还需进一步讨论。   议程 2：Kitex 定制框架错误处理和规范介绍 @YangruiEmma  Issue 地址：https://github.com/cloudwego/kitex/issues/511 背景：对于用户而言，工程实践里面 RPC 异常分成两大类。   RPC 异常。即 RPC 请求失败，对应超时、协议错误、熔断或者限流等等情况； RPC 层面成功，用户层面异常。用户把请求发到下游，希望根据他的处理逻辑返回状态码给上游，上游可以通过这些状态码做一些额外处理。这种情况在 RPC 层面其实是请求成功，业务错误属于业务逻辑层面。因此服务监控建议对于 RPC 错误上报为请求失败，而业务层面错误，上报为请求成功，但上报 status_code 用于识别错误码。该能力对于工程实践具有一定的价值。  起初，内部要求用户在 Thrift IDL 定义中定义全公司统一的 Base Response 字段，用户通过 Base Response 用户设置业务层面的状态码，我们把这个状态码上报，用户就可以通过监控看到业务层面出现的异常。但是考虑到开源后这套规范并不是很优雅，所以我们想定制一个通用的规范，让用户定义自己的异常。Kitex 本身支持多协议，这一套异常又不能和协议做耦合，因此我们要定义一套通用的接口。\n接口定义：我们会定义一个 bizStatusError 接口。因为 gRPC 用户常用 Status 回传 Error，gRPC 无论是 RPC 真正的框架层面异常，还是用户自定义异常，都使用 Status，其实是没有办法区分的。我们给用户提供的是 gRPC 本身就提供给用户的，即通过 Status 构造 Error，因此我们也要对应地做支持。所以用户可以按照 gRPC 的 Status 实现接口，同时也可以实现 Kitex 定义的这套接口，Kitex 会根据接口判断是否有用户自定义异常，如果是 gRPC 的 Status，我们也会按照 gRPC 的规范通过 HTTPHeader 把错误写到 Header，通过 Header 回传。 用户使用：服务端可以直接通过 bizerror 包构造 bizStatusError。调用端可以通过 bizerror.FromError 方法判断对端返回的是不是 bizerror。 框架实现：Thrift 和 Kitex Protobuf 对于 RPC 层面的异常是放在 Payload 里面编码的，gRPC 是统一放在 HTTPHeader 里面做编码的。因为考虑用户层面的异常，Thrift 和 Kitex Protobuf 放在 Payload 里面编码不是特别合适，所以我们考虑统一在 Header 里面做返回，不再放在 Payload 里面，Payload 里面只写 RPC 层面的异常。 框架处理：具体参见 https://github.com/cloudwego/kitex/issues/511。   议程 3：关于 CloudWeGo 代码生成工具相关建议和方案的讨论 @lsjbd  Issue 地址：https://github.com/cloudwego/kitex/issues/531 李纪昀提了关于 Kitex Tool 的改进建议。解答如下：   问题一：第一，Kitex 默认使用 go path 模式，如果没有指定 -model 参数，会认为当前项目是在 go path 下，之后尝试搜索 go path source 的相对路径，决定代码输出的前缀。现在 gomodule 已经使用比较广泛，我们是否可以默认在 gomodule 文件承载情况下，直接使用当前已知的 gomodule 不要求参数指定？这里的问题是我们内部还有很多项目不使用 gomodule，所以默认行为一旦改变，可能会产生很多 breaking change；第二，gomodule 不一定在当前目录，所以如果实现必须逐层向上搜索，但这可能会达不到预期的效果。 问题二：我们内部可能会使用一些比较奇怪的 IDL 后缀。在它开发的早期，我们其实做了限制，入口的 IDL 必须是 .thrift 或者 .proto 。所以理论上这个是可以做支持的，根据 Thrift 还是 Proto 来确定当前的 Tag，只有在其他情况下才要求它必须指定一个 Tag，所以这是可以实现的。 问题三：我们内部已经在考虑，即使不能合并，是否在两者的页面或者参数做一些统一的功能，此外生成代码的结构体将来是否能够复用也在研究中。 问题四：起初设计 Kitex 也考虑过自定义模板，其实 Kitex 本身支持模版还是比较复杂的，因为 Kitex 并不主导生成代码的过程。它底层有 Protoc 和 Thriftgo 这样两个实际的编译器在做生成代码的工作。所以 Kitex 支持自定义模版还需要考虑两个底层的编译器是否能支持自定义模版的问题。而两个编译器都支持插件，所以自定义模版的功能完全可以用插件的功能实现。   议程 4：2021-2022 Awesome Contributor 评选事宜 @yiyun  Issue 地址：https://github.com/cloudwego/community/issues/36 背景：9 月份 CloudWeGo 开源一周年，一年内除了技术迭代，还收获了 100+ 社区贡献者，以及几百名活动布道者。希望通过 Awesome Contributor 评选，表彰和感谢他们对社区的贡献和支持，共有 100 个名额。活动奖励类型、奖励方式、具体范围、评选标准、评选时间和公示参见 Issue 地址。 8 月 1 日正式开启评选。可以自荐，直接在 Issue 下面评论名单和贡献内容即可。   议程 5： Go through good-first-issue \u0026 QA @GuangmingLuo  Kitex good-first-issue 地址：https://github.com/cloudwego/kitex/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22  一个文档翻译任务待领取，两个单测任务完成情况待审核。\nHertz good-first-issue 地址：https://github.com/cloudwego/hertz/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22  两个任务待认领：https://github.com/cloudwego/hertz/issues/61；https://github.com/cloudwego/hertz/issues/62\n（第二个任务可以考虑和 Kitex 对接远程配置中心设置一个通用方案。）\n业务场景 Business 仓库：https://github.com/cloudwego/biz-demo  欢迎大家提交业务案例！\n 相关资讯 Hertz v0.2.0 发布！\n相关链接：https://mp.weixin.qq.com/s/OOlO-ng4NVgnh32D2dj8Qw\n","categories":"","description":"","excerpt":"会议主题 ：CloudWeGo 社区会议 7.28\n参会人 ：li-jin-gou, liu-song, GuangmingLuo, …","ref":"/zh/community/meeting_notes/2022-07-28/","tags":"","title":"CloudWeGo 社区会议 7.28"},{"body":"Stats Level:\n LevelDisabled, disable all events LevelBase, enable basic events LevelDetailed, enable basic events and detailed events.  Default Stats Level:\n No tracer is available, LevelDisabled by default At least one tracer is available, LevelDetailed by default  Client tracing stats level control:\nimport \"github.com/cloudwego/kitex/client\" import \"github.com/cloudwego/kitex/pkg/stats\" ... baseStats := client.WithStatsLevel(stats.LevelBase) client, err := echo.NewClient(\"echo\", baseStats) if err != nil { log.Fatal(err) } Server tracing stats level control:\nimport \"github.com/cloudwego/kitex/server\" import \"github.com/cloudwego/kitex/pkg/stats\" ... baseStats := server.WithStatsLevel(stats.LevelBase) svr, err := echo.NewServer(baseStats) if err := svr.Run(); err != nil { log.Println(\"server stopped with error:\", err) } else { log.Println(\"server stopped\") } Stats introduction Basic Stats Event:\n RPCStart，（client/server）RPC call start RPCFinish，（client）RPC call finish  Detailed Stats Event(client):\n ClientConnStart, connection establishment start ClientConnFinish，connection establishment finish WriteStart, request send (serialization including) start WriteFinish, request send (serialization including) finish ReadStart, response receive (deserialization including) start WaitReadStart, response stream read start (Fast Codec only) WaitReadFinish, response stream read finish (Fast Codec only) ReadFinish, response receive (deserialization including) finish  Detailed Stats Event(server):\n ReadStart, request receive (deserialization including) start WaitReadStart, request stream read start (Fast Codec only) WaitReadFinish, request stream read finish (Fast Codec only) ReadFinish, request receive (deserialization including) start ServerHandleStart, handler process start ServerHandleFinish, handler process finish WriteStart, response send (serialization including) start WriteFinish, response send (serialization including) start  Timeline:\nclient stats events timeline:\nserver stats events timeline:\n","categories":"","description":"","excerpt":"Stats Level:\n LevelDisabled, disable all events LevelBase, enable …","ref":"/docs/kitex/tutorials/basic-feature/tracing/","tags":"","title":"Instrumentation Control"},{"body":"埋点粒度：\n LevelDisabled 禁用埋点 LevelBase 仅启用基本埋点 LevelDetailed 启用基本埋点和细粒度埋点  默认埋点策略：\n 无 tracer 时，默认 LevelDisabled 有 tracer 时，默认 LevelDetailed  客户端埋点粒度控制：\nimport \"github.com/cloudwego/kitex/client\" import \"github.com/cloudwego/kitex/pkg/stats\" ... baseStats := client.WithStatsLevel(stats.LevelBase) client, err := echo.NewClient(\"echo\", baseStats) if err != nil { log.Fatal(err) } 服务端埋点粒度控制：\nimport \"github.com/cloudwego/kitex/server\" import \"github.com/cloudwego/kitex/pkg/stats\" ... baseStats := server.WithStatsLevel(stats.LevelBase) svr, err := echo.NewServer(baseStats) if err := svr.Run(); err != nil { log.Println(\"server stopped with error:\", err) } else { log.Println(\"server stopped\") } 埋点说明 基本埋点：\n RPCStart，（客户端 / 服务端）RPC 调用开始 RPCFinish，（客户端 / 服务端）RPC 调用结束  细粒度埋点（客户端）：\n ClientConnStart，连接建立开始 ClientConnFinish，连接建立结束 WriteStart，请求发送（含编码）开始 WriteFinish，请求发送（含编码）结束 ReadStart，响应接收（含解码）开始 WaitReadStart，响应二进制读取开始（仅适用于 Fast Codec） WaitReadFinish，响应二进制读取完毕（仅适用于 Fast Codec） ReadFinish，响应接收（含解码）完毕  细粒度埋点（服务端）：\n ReadStart，请求接收（含解码）开始 WaitReadStart，请求二进制读取开始（仅适用于 Fast Codec） WaitReadFinish，请求二进制读取完毕（仅适用于 Fast Codec） ReadFinish，请求接收（含解码）完毕 ServerHandleStart，handler 处理开始 ServerHandleFinish，handler 处理完毕 WriteStart，响应发送（含编码）开始 WriteFinish，响应发送（含编码）结束  时序图：\n客户端埋点时序图\n服务端埋点时序图\n","categories":"","description":"","excerpt":"埋点粒度：\n LevelDisabled 禁用埋点 LevelBase 仅启用基本埋点 LevelDetailed …","ref":"/zh/docs/kitex/tutorials/basic-feature/tracing/","tags":"","title":"埋点粒度"},{"body":"会议主题 ：CloudWeGo 社区会议 8.11\n参会人 ：li-jin-gou, GuangmingLuo, pkumza, ag9920, lsjbd, sinnera, welkeyever, YangruiEmma, CoderPoet, joway, zstone12, Yin Xuran, bodhisatan, Fan Guangyu, Zhang Guiyuan, ppzqh, HeyJavaBean, simon0-o, baiyutang, rogerogers, skyenought, cloudwegoIce, cyyolo, baize, Hchenn, Ivnszn, LemonFish\n会前必读 ：http://www.cloudwego.io/；https://github.com/cloudwego\n议程 1：新人自我介绍  新成员名单：@LemonFish 社区新成员进行自我介绍，主要包含个人基本情况、开源贡献经历和后续参与社区工作内容。   议程 2：对接远程配置中心的方案介绍 @sinnera  Issue 地址：https://github.com/cloudwego/kitex/issues/574 背景：Kitex 开源后一直不支持对接外面的配置中心，收到用户反馈有相关需求，比如框架内的服务治理策略以及自定义的配置都有需求对接配置中心，提供拉取和动态更新等功能。详细内容参见 Issue。 相关讨论：传统配置中心都会有类似的设计，对这套配置每做一次更新，就会发布一个新版本。如果用户想指定发布版本后，在某一个特定的实例上生效，可能就会产生线上同时有不同版本的配置生效的情况。相关问题后续会具体考虑，也可以到 Issue 下参与讨论。   议程 3：fastPB 开源项目简介 @Hchenn  Issue 地址：github.com/cloudwego/fastpb fastPB 项目是用生成代码对 PB 进行序列化和反序列化的仓库。官方的 PB 编辑码是通过反射进行的，这个项目编辑码是把所有的编码和解码具体操作通过生成代码的形式直接进行，这样就规避了反射。项目刚刚完成，具体的性能测试还在进行中。   议程 4：Hertz 服务注册、发现与负载均衡介绍 \u0026 新手任务 @li-jin-gou  Demo 地址：github.com/li-jin-gou/nacos-demo 背景：外部用户对 Hertz 服务注册发现呼声较高，内部的确也有这一套服务发现并且正在使用，所以开源出来。 服务注册：Registry 接口的大部分逻辑是参考 Kitex 的实现，因此接口是一样的。注册和取消注册的逻辑放在 Hertz Hook Function（run hook/shutdown hook）里面，启动时注册，关闭时取消注册。 服务发现：发现是配合 Client 使用的。发现接口分别是 Target/Resolve/Name，Target 就是唯一标识对应服务，这样会返回一个唯一的 Key；Resolve 通过唯一标识获取对应的实例；Name 内部用来和它对应的 Load Balance 做缓存，避免重复创建。 具体使用实例：github.com/li-jin-gou/nacos-demo/tree/main/example 扩展库：github.com/hertz-contrib/registry。对接 Nacos/ZK/ETCD 等其他注册中心的扩展会放到对应仓库，下周初会把文档补齐，会以 good-first-issue 的形式向社区提供。 补充：Hertz Registry 扩展与 Kitex 稍有不同，把子项目都放在了同一个仓库。因为 Hertz-contrib扩展库较多，拆分后维护成本较大。因此我们决定放在一个仓库里面，以不同的子项目形式存放。欢迎社区的同学一起来参与共建！   议程 5：registry-servicecomb 注册中心扩展介绍和演示 @bodhisatan  相关文档：ServiceComb服务注册发现 （附演示视频录屏） 注册关键逻辑：实现了一个 Register 接口。流程是注册服务，再注册服务实例，做一个异步的心跳保活，然后解除注册。 解除注册逻辑：如果 Address 是空，直接注销服务；如果 Address 不是空，先注销实例，然后取消心跳保活，通过 Endpoints 查找实例，查找出来之后用 instanceId 注销 MicroServiceInstances。 服务发现逻辑：调用了 FindMicroServiceInstances 找到下面所有的实例。   议程 6：CloudWeGo 一周年技术沙龙活动预告与介绍 @cloudwegoIce   相关链接：https://mp.weixin.qq.com/s/x0Y7-gn9kwpoDQayS2bo3w\n  背景：2021 年 9 月 CloudWeGo 正式开源，今年 9 月是正式开源一周年。一年内，CloudWeGo收获了 9000+ star，新增许多开源项目，还有即将新开源一个 Rust RPC 框架。我们会在开源一周年 Meetup 上介绍一年以来的开源历程。\n  四个议题：\n 高性能 RPC 框架 Kitex 内外统一的开源实践 大规模企业级 HTTP 框架设计和实践 新一代基于 Rust 语言的高性能 RPC 框架 开源社区的长期主义与新变化 - CloudWeGo 开源社区实践    地点及参与方式：\n 线上：直接报名参与，群里定时放出参与链接。 线下：北京字节跳动的工区，可以联系 cloudwegoIce 或刘佳同学注册报名。    ","categories":"","description":"","excerpt":"会议主题 ：CloudWeGo 社区会议 8.11\n参会人 ：li-jin-gou, GuangmingLuo, pkumza, …","ref":"/zh/community/meeting_notes/2022-08-11/","tags":"","title":"CloudWeGo 社区会议 8.11"},{"body":"The Diagnosis module is used to visualize the information in the service to facilitate troubleshooting and confirm the service status. Kitex defines an interface to register the diagnostic func, and developers can implement this interface to present diagnostic information. Presentation way like: output with log and query display with debug port. The open source version of Kitex does not provide a default extension temporarily, but some information that can be used for diagnosis is registered by default. The developers can also register more information for troubleshooting.\nExtension API // ProbeName is the name of probe. type ProbeName string // ProbeFunc is used to get probe data, it is usually a data dump func. type ProbeFunc func() interface{} // Service is the interface for debug service. type Service interface { // RegisterProbeFunc is used to register ProbeFunc with probe name  // ProbeFunc is usually a dump func that to dump info to do problem diagnosis,  // eg: CBSuite.Dump(), s.RegisterProbeFunc(CircuitInfoKey, cbs.Dump)  RegisterProbeFunc(ProbeName, ProbeFunc) } Register diagnostic information // new diagnosisi service var ds diagnosis.service = NewYourService() // eg: register dump func to get discovery instances. ds.RegisterProbeFunc(\"instances\", dr.Dump) // eg: wrap the config data as probe func, register func to get config info. ds.RegisterProbeFunc(\"config_info\", diagnosis.WrapAsProbeFunc(config)) Default registered diagnostic information in Kitex Kitex registers some diagnostic information for troubleshooting by default, as follows:\nconst ( // Common \tChangeEventsKey ProbeName = \"events\" ServiceInfoKey ProbeName = \"service_info\" OptionsKey ProbeName = \"options\" // Client \tDestServiceKey ProbeName = \"dest_service\" ConnPoolKey ProbeName = \"conn_pool\" RetryPolicyKey ProbeName = \"retry_policy\" ) Integrate into Kitex Specify your own diagnostic service through option: WithDiagnosisService.\n// server side svr := xxxservice.NewServer(handler, server.WithDiagnosisService(yourDiagnosisService)) // client side cli, err := xxxservice.NewClient(targetService, client.WithDiagnosisService(yourDiagnosisService)) ","categories":"","description":"","excerpt":"The Diagnosis module is used to visualize the information in the …","ref":"/docs/kitex/tutorials/framework-exten/diagnosis/","tags":"","title":"Extension of Diagnosis"},{"body":"pkg/klog Kitex defines several interfaces in the package pkg/klog: Logger, CtxLoggerKey and FormatLogger. And it provides a default logger that implements those interfaces and can be accessed by calling klog.DefaultLogger().\nThere are global functions in the package pkg/klog that expose the ability of the default logger, like klog.Info, klog.Errorf and so on.\nNote that the default logger uses the log.Logger from the standard library as its underlying output. So the filename and line number shown in the log messages depend on the setting of call depth. Thus wrapping the implementation of klog.DefaultLogger may cause inaccuracies for these two values.\nInjecting your own logger You can use klog.SetLogger to replace the default logger.\nRedirecting the Output of the Default Logger The klog.SetOutput can be used to redirect the output of the default logger provided by the pkg/klog package.\nFor example, to redirect the output of the default logger to a file name ./output.log under the launch directory, a possible implementation might be:\npackage main import ( \"os\" \"github.com/cloudwego/kitex/pkg/klog\" ) func main() { f, err := os.OpenFile(\"./output.log\", os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0644) if err != nil { panic(err) } defer f.Close() klog.SetOutput(f) ... // continue to set up your server } ","categories":"","description":"","excerpt":"pkg/klog Kitex defines several interfaces in the package pkg/klog: …","ref":"/docs/kitex/tutorials/basic-feature/logging/","tags":"","title":"Logging"},{"body":"pkg/klog Kitex 在 pkg/klog 里定义了 Logger、CtxLogger、FormatLogger 等几个接口，并提供了一个 FormatLogger 的默认实现，可以通过 klog.DefaultLogger() 获取到其实例。\npkg/klog 同时也提供了若干全局函数，例如 klog.Info、klog.Errorf 等，用于调用默认 logger 的相应方法。\n注意，由于默认 logger 底层使用标准库的 log.Logger 实现，其在日志里输出的调用位置依赖于设置的调用深度（call depth），因此封装 klog 提供的实现可能会导致日志内容里文件名和行数不准确。\n注入自己的 logger 实现 可以用 klog.SetLogger 来替换掉默认的 logger 实现。\n重定向默认 logger 的输出 可以使用 klog.SetOutput 来重定向 klog 提供的默认 logger 的输出。\n例如，要把默认 logger 的输出重定向到启动路径下的 ./output.log，可以这样实现：\npackage main import ( \"os\" \"github.com/cloudwego/kitex/pkg/klog\" ) func main() { f, err := os.OpenFile(\"./output.log\", os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0644) if err != nil { panic(err) } defer f.Close() klog.SetOutput(f) ... // continue to set up your server } ","categories":"","description":"","excerpt":"pkg/klog Kitex 在 pkg/klog 里定义了 Logger、CtxLogger、FormatLogger 等几个接口，并提供 …","ref":"/zh/docs/kitex/tutorials/basic-feature/logging/","tags":"","title":"日志"},{"body":"诊断模块是用于将服务中的信息可视化出来，便于问题排查，确认服务状态。Kitex 定义了接口用来注册诊断 func，扩展者可实现该接口来呈现诊断信息。呈现的方式如：输出日志、debug 端口查询展示。Kitex 开源版本暂未提供默认扩展，但默认注册了部分可用于诊断的信息，扩展者也可以注册更多的信息用于问题的排查。\n扩展接口 // ProbeName is the name of probe. type ProbeName string // ProbeFunc is used to get probe data, it is usually a data dump func. type ProbeFunc func() interface{} // Service is the interface for debug service. type Service interface { // RegisterProbeFunc is used to register ProbeFunc with probe name  // ProbeFunc is usually a dump func that to dump info to do problem diagnosis,  // eg: CBSuite.Dump(), s.RegisterProbeFunc(CircuitInfoKey, cbs.Dump)  RegisterProbeFunc(ProbeName, ProbeFunc) } 注册诊断信息 // new diagnosisi service var ds diagnosis.service = NewYourService() // eg: register dump func to get discovery instances. ds.RegisterProbeFunc(\"instances\", dr.Dump) // eg: wrap the config data as probe func, register func to get config info. ds.RegisterProbeFunc(\"config_info\", diagnosis.WrapAsProbeFunc(config)) Kitex 默认注册的诊断信息 Kitex 默认注册了部分诊断信息用于问题排查，具体如下：\nconst ( // Common \tChangeEventsKey ProbeName = \"events\" ServiceInfoKey ProbeName = \"service_info\" OptionsKey ProbeName = \"options\" // Client \tDestServiceKey ProbeName = \"dest_service\" ConnPoolKey ProbeName = \"conn_pool\" RetryPolicyKey ProbeName = \"retry_policy\" ) 集成到 Kitex 通过 option 指定自己的诊断服务，option: WithDiagnosisService。\n// server side svr := xxxservice.NewServer(handler, server.WithDiagnosisService(yourDiagnosisService)) // client side cli, err := xxxservice.NewClient(targetService, client.WithDiagnosisService(yourDiagnosisService)) ","categories":"","description":"","excerpt":"诊断模块是用于将服务中的信息可视化出来，便于问题排查，确认服务状态。Kitex 定义了接口用来注册诊断 func，扩展者可实现该接口来呈现诊 …","ref":"/zh/docs/kitex/tutorials/framework-exten/diagnosis/","tags":"","title":"诊断模块扩展"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/releases/","tags":"","title":"New Releases"},{"body":"Feature  [#124] feat: add option to remove hijackConnPool. [#116] feat: update for template. [#130] feat: add a warning log for invalid character in Cookie.Value. [#143] feat: custom signal to graceful shutdown [#114] feat: release buffer in standard network method. [#112] feat: parse post args in bodystream. [#105] feat: client abstracts hostclient layer. [#92] feat: hz support windows. [#102] feat: client removes default retry logic.  Optimize  [#111] optimize: pre-allocate slice when calling bytesconv.AppendHTTPDate. [#128] optimize: remove useless judgement. [#108] optimize: avoid parsing regular expression repeatedly.  Chore  [#125] Update pr-check.yml.  Fix  [#104] fix: use defer to guarantee that mutex will be unlocked. [#96] fix: ci exec /bin/license-eye: exec format error  Style  [#103] style: fixed the typo “ungzipped” to “gunzipped”. [#90] style: use const var and remove duplicate type conversions.  Refactor  [#94] refactor: use appendCookiePart to simplify code.  Docs  [#97] docs: use comma to separate \u0026\u0026 remove extra space.  ","categories":"","description":"","excerpt":"Feature  [#124] feat: add option to remove hijackConnPool. [#116] …","ref":"/blog/2022/07/22/hertz-release-v0.2.0/","tags":"","title":"Hertz Release v0.2.0"},{"body":"Feature  [#124] feat: 增加参数控制是否使用 hijackConnPool。 [#116] feat: update 也可使用模板更新 handler 及 middleware。 [#130] feat: 如果 Cookie.Value 中存在非法字符，则打印告警日志。 [#143] feat: 增加一个接口支持自定义信号捕捉逻辑，以便根据场景调节优雅退出需要应对的信号类型。 [#114] feat: 标准网络库 Read 方法中调用 connection.Release()，防止在多次少量调用 Read 方法时不回收内存导致的 OOM。 [#112] feat: 修正了 x-www-form-urlencoded 编码下无法读到 bodystream 类型数据。 [#105] feat: client 为 ALPN 和 http2 抽象出协议层 HostClient。client 删除 readbuffersize 和 writebuffersize 配置项。 [#92] feat: hz 命名行工具支持 windows。 [#102] feat: Hertz client 关闭默认的重试逻辑。  Optimize  [#111] optimize: 调用 bytesconv.AppendHTTPDate 时，为切片预分配容量，以防止产生额外的拷贝。 [#128] optimize: 去掉路由树中无用逻辑。 [#108] optimize: 通过提前调用 regexp.MustCompile，避免程序重复解析正则表达式。  Chore  [#125] chore: 更新 license check 方式。  Fix  [#104] fix: cacheLock 可能会因潜在发生的 panic 导致解锁失败。 [#96] fix: ci 可能被调度到 arm 机器上导致报错 exec format error。  Style  [#103] style: 修正不符合语义的错误拼写 “Ungzipped”。 [#90] style: 常量替换和去掉了重复的类型转换。  Refactor  [#94] refactor: 使用 appendCookiePart 函数简化代码。  Docs  [#97] docs: 文档标点符号优化。  ","categories":"","description":"","excerpt":"Feature  [#124] feat: 增加参数控制是否使用 hijackConnPool。 [#116] feat: update 也 …","ref":"/zh/blog/2022/07/22/hertz-v0.2.0-%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/","tags":"","title":"Hertz v0.2.0 版本发布"},{"body":"01 前言 Hertz 是字节跳动服务框架团队研发的超大规模的企业级微服务 HTTP 框架，具有高易用性、易扩展、低时延等特点。在经过了字节跳动内部一年多的使用和迭代后，如今已在 CloudWeGo 正式开源。目前，Hertz 已经成为了字节跳动内部最大的 HTTP 框架，线上接入的服务数量超过 1 万，峰值 QPS 超过 4 千万。除了各个业务线的同学使用外，也服务于内部很多基础组件，如：函数计算平台 FaaS、压测平台、各类网关、Service Mesh 控制面等，均收到不错的使用反馈。在如此大规模的场景下，Hertz 拥有极强的稳定性和性能，在内部实践中某些典型服务，如框架占比较高的服务、网关等服务，迁移 Hertz 后相比 Gin 框架，资源使用显著减少，CPU 使用率随流量大小降低 30%—60%，时延也有明显降低。\nHertz 坚持 内外维护一套代码 ，为开源使用提供了强有力的保障。通过开源， Hertz 也将丰富云原生的 Golang 中间件体系，完善 CloudWeGo 生态矩阵，为更多开发者和企业搭建云原生化的大规模分布式系统，提供一种现代的、资源高效的的技术方案。\n本文将重点关注 Hertz 的架构设计与 功能特性 。\n02 项目缘起 最初，字节跳动内部的 HTTP 框架是对 Gin 框架的封装，具备不错的易用性、生态完善等优点。随着内部业务的不断发展，高性能、多场景的需求日渐强烈。而 Gin 是对 Golang 原生 net/http 进行的二次开发，在按需扩展和性能优化上受到很大局限。因此，为了满足业务需求，更好的服务各大业务线，2020 年初，字节跳动服务框架团队经过内部使用场景和外部主流开源 HTTP 框架 Fasthttp、Gin、Echo 的调研后，开始基于自研网络库 Netpoll 开发内部框架 Hertz，让 Hertz 在面对企业级需求时，有更好的性能及稳定性表现，也能够满足业务发展和应对不断演进的技术需求。\n03 架构设计 Hertz 设计之初调研了大量业界优秀的 HTTP 框架，同时参考了近年来内部实践中积累的经验。为了保证框架整体上满足：1. 极致性能优化的可能性；2. 面对未来不可控需求的扩展能力， Hertz 采用了 4 层分层设计，保证各个层级功能内聚，同时通过层级之间的接口达到灵活扩展的目标。整体架构图如图 1 所示。\n图1 Hertz 架构图 Hertz 从上到下分为：应用层、路由层、协议层和传输层，每一层各司其职，同时公共能力被统一抽象到公共层（Common），做到跨层级复用。另外，同主库一同发布的还有作为子模块的 Hz 脚手架，它能够协助使用者快速搭建出项目核心骨架以及提供实用的构建工具链。\n应用层 应用层是和用户直接交互的一层，提供丰富易用的 API，主要包括 Server、Client 和一些其他通用抽象。Server 提供了注册 HandlerFunc、Binding、Rendering 等能力；Client 提供了调用下游和服务发现等能力；以及抽象一个 HTTP 请求所必须涉及到的请求（Request）、响应（Response）、上下文（RequestContext）、中间件（Middleware）等等。Hertz 的 Server 和 Client 都能够提供中间件这样的扩展能力。\n应用层中一个非常重要的抽象就是对 Server HandlerFunc 的抽象。早期，Hertz 路由的处理函数 （HandlerFunc）中并没有接收标准的 context.Context，我们在大量的实践过程中发现，业务方通常需要一个标准的上下文在 RPC Client 或者日志、Tracing 等组件间传递，但由于请求上下文（RequestContext）生命周期局限于一次 HTTP 请求之内，而以上提到的场景往往存在异步的传递和处理，导致如果直接传递请求上下文，会导致出现一些数据不一致的问题。为此我们做了诸多尝试，但是因为核心原因在于请求上下文（RequestContext）的生命周期无法优雅的按需延长，最终在各种设计权衡下，我们在路由的处理函数签名中增加一个标准的上下文入参，通过分离出生命周期长短各异的两个上下文的方式，从根本上解决各种因为上下文生命周期不一致导致的异常问题，即：\n路由层 路由层负责根据 URI 匹配对应的处理函数。\n起初，Hertz 的路由基于 httprouter 开发，但随着使用的用户越来越多，httprouter 渐渐不能够满足需求，主要体现在 httprouter 不能够同时注册静态路由和参数路由，即 /a/b， /:c/d 这两个路由不能够同时注册；甚至有一些更特殊的需求，如 /a/b、/:c/b ，当匹配 /a/b 路由时，两个路由都能够匹配上。\nHertz 为满足这些需求重新构造了路由树，用户在注册路由时拥有很高的自由度：支持静态路由、参数路由的注册；支持按优先级匹配，如上述例子会优先匹配静态路由 /a/b；支持路由回溯，如注册 /a/b、/:c/d，当匹配 /a/d 时仍然能够匹配上；支持尾斜线重定向，如注册 /a/b，当匹配 /a/b/ 时能够重定向到 /a/b 上。Hertz 提供了丰富的路由能力来满足用户的需求，更多的功能可以参考 Hertz 配置文档。\nHttprouter：https://github.com/julienschmidt/httprouter\nHertz 配置文档： https://www.cloudwego.io/zh/docs/hertz/reference/config/\n协议层 协议层负责不同协议的实现和扩展。\nHertz 支持协议的扩展，用户只需要实现下面的接口便可以按照自己的需求在引擎（Engine） 上扩展协议，同时也支持通过 ALPN 协议协商的方式注册。Hertz 首批只开源了 HTTP1 实现，未来会陆续开源 HTTP2、QUIC 等实现。协议层扩展提供的灵活性甚至可以超越 HTTP 协议的范畴，用户完全可以按需注册任意符合自身需求的协议层实现，并且加入到 Hertz 的引擎中来，同时，也能够无缝享受到传输层带来的极致性能。\n协议扩展： https://www.cloudwego.io/zh/docs/hertz/tutorials/framework-exten/advanced-exten/protocol/\n传输层 传输层负责底层的网络库的抽象和实现。\nHertz 支持底层网络库的扩展。Hertz 原生完美适配 Netpoll，在时延方面有很多深度的优化，非常适合时延敏感的业务接入。Netpoll 对 TLS 能力的支持有待完善，而 TLS 能力又是 HTTP 框架必备能力，为此 Hertz 底层同时支持基于 Golang 标准网络库的实现适配，同时支持网络库的一键切换，用户可根据自己的需求选择合适的网络库进行替换。如果用户有更加高效的网络库或其他网络库需求，也完全可以根据需求自行扩展。\n网络库的扩展： https://www.cloudwego.io/zh/docs/hertz/tutorials/framework-exten/advanced-exten/network-lib/\nHz 脚手架 与 Hertz 一并开源的还有一个易用的命令行工具 Hz，用户只需提供一个 IDL，根据定义好的接口信息，Hz 便可以一键生成项目脚手架，让 Hertz 达到开箱即用的状态；Hz 也支持基于 IDL 的更新能力，能够基于 IDL 变动智能地更新项目代码。目前 Hz 支持了 Thrift 和 Protobuf 两种 IDL 定义。命令行工具内置丰富的选项，可以根据自己的需求使用。同时它底层依赖 Protobuf 官方的编译器和自研的 Thriftgo 的编译器，两者都支持自定义的生成代码插件。如果默认模板不能够满足需求，完全能够按需定义。\n未来，我们将继续迭代 Hz，持续集成各种常用的中间件，提供更高层面的模块化构建能力。给 Hertz 的用户提供按需调整的能力，通过灵活的自定义配置打造一套满足自身开发需求的脚手架。\nCommon 组件 Common 组件主要存放一些公共的能力，比如错误处理、单元测试能力、可观测性相关能力（Log、Trace、Metrics 等）。对于服务可观测性的能力，Hertz 提供了默认的实现，用户可以按需装配；如果用户有特殊的需求，也可以通过 Hertz 提供的接口注入。比如对于 Trace 能力，Hertz 提供了默认的实现，也提供了将 Hertz 和 Kitex 串起来的 Example。如果想注入自己的实现，也可以实现下面的接口：\nExample： https://github.com/cloudwego/hertz-examples/blob/main/tracer/README.md\n04 功能特性 中间件 Hertz 除了提供 Server 的中间件能力，还提供了 Client 中间件能力。用户可以使用中间件能力将通用逻辑（如：日志记录、性能统计、异常处理、鉴权逻辑等等）和业务逻辑区分开，让用户更加专注于业务代码。Server 和 Client 中间件使用方式相同，使用 Use 方法注册中间件，中间件执行顺序和注册顺序相同，同时支持预处理和后处理逻辑。\nServer 和 Client 的中间件实现方式并不相同。对于 Server 来说，我们希望减少栈的深度，同时也希望中间件能够默认的执行下一个，用户需要手动终止中间件的执行。因此，我们将 Server 的中间件分成了两种类型，即不在同一个函数调用栈（该中间件调用完后返回，由上一个中间件调用下一个中间件，如图 2 中 B 和 C）和在同一个函数调用栈的中间件（该中间件调用完后由该中间件继续调用下一个中间件，如图 2 中 C 和 Business Handler）。\n图2 中间件链路 其核心是需要一个地方存下当前的调用位置 index，并始终保持其递增。恰好 RequestContext 就是一个存储 index 合适的位置。但是对于 Client，由于没有合适的地方存储 index，我们只能退而求其次，抛弃 index 的实现，将所有的中间件构造在同一调用链上，需要用户手动调用下一个中间件。\n流式处理 Hertz 提供 Server 和 Client 的流式处理能力。HTTP 的文件场景是十分常见的场景，除了 Server 侧的上传场景之外，Client 的下载场景也十分常见。为此，Hertz 支持了 Server 和 Client 的流式处理。在内部网关场景中，从 Gin 迁移到 Hertz 后，CPU 使用量随流量大小不同可节省 30%—60% 不等，服务压力越大，收益越大。Hertz 开启流式功能的方式也很容易，只需要在 Server 上或 Client 上添加一个配置即可，可参考 CloudWeGo 官网 Hertz 文档的流式处理部分。\n由于 Netpoll 采用 LT 的触发模式，由网络库主动将将数据从 TCP 缓冲区读到用户态，并存储到 buffer 中，否则 epoll 事件会持续触发。因此 Server 在超大请求的场景下，由于 Netpoll 持续将数据读到用户态内存中，可能会有 OOM 的风险。HTTP 文件上传场景就是一个典型的场景，但 HTTP 上传服务又是很常见的场景，因此我们支持标准网络库 go net，并针对 Hertz 做了特殊优化，暴露出 Read() 接口，防止 OOM 发生。\n对于 Client，情况并不相同。流式场景下会将连接封装成 Reader 暴露给用户，而Client 有连接池管理，那这样连接就多了一种状态，何时关连接，何时复用连接成了一个问题。由于框架侧并不知道该连接何时会用完，框架侧复用该连接不现实，会导致串包问题。由于 GC 会关闭连接，因此我们起初设想流式场景下的连接交由用户后，由 GC 负责关闭，这样也不会导致资源泄漏。但是在测试后发现，由于 GC 存在一定时间间隔，另外 TCP 中主动关闭连接的一方需要等待 2RTT，在高并发场景下会导致 fd 被打满的情况。最终我们提供了复用连接的接口，对于性能有场要求用户，在使用完连接后可以将连接重新放入连接池中复用。\n流式处理：https://www.cloudwego.io/zh/docs/hertz/tutorials/basic-feature/stream/\n05 性能表现 Hertz 使用字节跳动自研高性能网络库 Netpoll，在提高网络库效率方面有诸多实践，参考已发布文章 字节跳动在 Go 网络库上的实践 。除此之外，Netpoll 还针对 HTTP 场景进行优化，通过减少拷贝和系统调用次数提高吞吐以及降低时延。为了衡量 Hertz 性能指标，我们选取了社区中有代表性的框架 Gin（net/http）和 Fasthttp 作为对比，如图3所示。可以看到，Hertz 的极限吞吐、TP99 等指标均处于业界领先水平。未来，Hertz 还将继续和 Netpoll 深度配合，探索 HTTP 框架性能的极限。\n图3 Hertz 和其他框架性能对比 06 一个Demo 下面简单演示一下 Hertz 是如何开发一个服务的。\n 首先，定义 IDL，这里使用 Thrift 作为 IDL 的定义（也支持使用 Protobuf 定义的 IDL），编写一个名为 Demo 的 service。这个服务有一个 API: Hello，它的请求参数是一个 query，响应是一个包含一个 RespBody 字段的 Json。  接下来我们使用 Hz 生成代码，并整理和拉取依赖。  填充业务逻辑，比如我们返回 hello，${Name}，那我们在 biz/handler/example/hello_service.go 中添加以下代码即可。  编译并运行项目。  到现在一个简单的 Hertz 项目已经生成，下面我们来测试一下。\n以上 Demo 可以在 Hertz-Examples 中查看，之后就可以愉快地构建自己的项目了。\n07 后记 希望以上的分享能够让大家对 Hertz 有一个整体上的认识。同时，我们也在不断地迭代 Hertz、完善CloudWeGo 整体生态。欢迎各位感兴趣的同学们加入我们，共同建设 CloudWeGo。\n08 参考资料 Hertz：https://github.com/cloudwego/hertz\nHertz Doc：https://www.cloudwego.io/zh/docs/hertz/\n官网文章：字节跳动在 Go 网络库上的实践 \n","categories":"","description":"","excerpt":"01 前言 Hertz 是字节跳动服务框架团队研发的超大规模的企业级微服务 HTTP 框架，具有高易用性、易扩展、低时延等特点。在经过了字节 …","ref":"/zh/blog/2022/06/21/%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%BC%80%E6%BA%90-go-http-%E6%A1%86%E6%9E%B6-hertz-%E8%AE%BE%E8%AE%A1%E5%AE%9E%E8%B7%B5/","tags":"","title":"字节跳动开源 Go HTTP 框架 Hertz 设计实践"},{"body":"今天，经过了字节跳动内部一年多的使用和迭代，高性能企业级 HTTP 框架—— Hertz，已在 CloudWeGo 正式开源啦！Hertz 已经成为了字节跳动内部最大的 HTTP 框架，线上接入的服务数量超过 1 万，峰值 QPS 超过 4 千万，具有 高易用性 、 易扩展 、低时延的特点。对于字节跳动服务框架团队和 CloudWeGo 而言，Hertz 将不仅仅是一个开源项目，它也是一个真实的超大规模企业级实践。\n项目地址：https://github.com/cloudwego/hertz\n未来，字节跳动基础架构团队将以 Hertz 开源库为主进行迭代，坚持内外维护一套代码，统一进行迭代演进，为用户提供更好的体验。\n01 Hertz 概述 Hertz 是一个超大规模的企业级微服务 HTTP 框架，具有高易用性、易扩展、低时延等特点。最初，字节跳动内部的 HTTP 框架是对 Gin 框架的封装，具备不错的易用性、生态完善等优点。随着内部业务的不断发展，对高性能、多场景的需求日渐强烈。而 Gin 是对 Golang 原生 net/http 进行的二次开发，在按需扩展和性能优化上受到很大局限。因此，为了满足业务需求，更好的服务各大业务线，2020 年初，字节跳动服务框架团队经过内部对使用场景和外部主流开源 HTTP 框架 Fasthttp、Gin、Echo 的调研后，开始基于自研网络库 Netpoll 开发内部框架 Hertz，让 Hertz 在面对企业级需求时，有更好的性能及稳定性表现，也能够满足业务发展和应对不断演进的技术需求。2021 年 7 月，Hertz 正式上线 1.0 版本。\n在经历了字节跳动内部一年多的使用后，Hertz 框架成为了字节跳动内部最大的 HTTP 框架，线上接入的服务数量超过 1 万，峰值 QPS 超过 4 千万。Hertz 除了业务线的同学使用外，也服务于内部很多基础组件，如：函数计算平台 FaaS、压测平台、各类网关、Service Mesh 控制面等，均收到不错的使用反馈。在如此大规模的场景下，Hertz 拥有极强的稳定性和性能，bug 和 kernel case 也几乎暴露无遗并进行修复。同时 Hertz 坚持的内外维护一套代码，也为开源出去的 Hertz 框架提供了强有力的保障。\n下面是 Hertz 的一些特性：\n 稳定性 Hertz 在如此大规模的场景下，每一个 PR 的合入、每一次发版都要慎之又慎，稍有不慎便可能造成千万甚至更多的损失。我们制定了规范的 PR、发版流程，每次合入代码需要由有经验的同学审核。即便如此，为了降低风险，我们也搭建了各种测试场景，包括兼容性、高并发、大小包等场景，每次的 PR、发版都需要测试一段时间，充分测试，将每次发版的风险减少到最低。 高易用性 在开发过程中，快速写出正确的代码往往是重要的。Hertz 在设计 API 时，考虑到用户的使用习惯，参考业界主流框架使用 API 的方式，并加以优化。在 Hertz 在迭代过程中，积极听取用户意见，持续打磨框架，比如很多用户希望 Client 也有 Trace 的能力，为此，Hertz Client 支持了中间件能力；在代理场景中，Hertz Client 也支持了流式处理。在做中间件和流式处理设计时，也考虑到用户实际使用习惯，帮助用户更快地写出正确的代码。Hertz 也提供了命令行工具，一键生成代码，提高框架的易用性。 易扩展 Hertz 采用了分层设计，提供了较多的接口以及默认的扩展实现，用户也可以自行扩展，详情可参考 CloudWeGo 官网的 Hertz 扩展部分。同时得益于框架的分层设计，框架的扩展性也会大很多。目前仅将稳定的能力开源给社区，更多的规划参考 RoadMap。 低延时 Hertz 默认使用自研的高性能网络库 Netpoll，在一些特殊场景中，相较于 go net，Hertz 在 QPS、时延上均具有一定优势。关于性能数据，可参考下图 Echo 数据。  在内部实践中，某些典型服务，如框架占比较高的服务、网关等服务，迁移 Hertz 后相比 Gin 框架，资源使用显著减少，CPU 使用率随流量大小降低 30%—60%。\n关于详细的性能数据，可参考：https://github.com/cloudwego/hertz-benchmark。\n 命令行工具 Hertz 提供了一个简单易用的命令行工具 Hz，用户只需提供一个 IDL，根据定义好的接口信息，Hz 便可以一键生成项目脚手架，开箱即用使用 Hertz；Hz 也提供更新能力，用户的 IDL 如果发生改变，Hz 可以更新脚手架。目前 Hz 支持了 Thrift 和 Protobuf 两种 IDL 定义。命令行工具内置丰富的选项，可以根据自己的需求使用。同时它底层依赖 Protobuf 官方的编译器和自研的 Thriftgo 的编译器，两者都支持自定义的生成代码插件。如果觉得默认模板不能够满足的需求，可以自定义生成的模板。  自 Hertz 发布以来，内部反响优异。在内部，除最常见的前后端通信场景外，还涉及网关、上传、下载、代理等场景；所用到的交互模式除 ping-pong 外，还有 streaming、chunk 等；使用的协议除 HTTP1 外，还有 HTTP2、Websocket 等。这些复杂的交互场景和交互模式都对 Hertz 的 Server 和 Client 的可用性和稳定性提出了不小的挑战。为此，Hertz 快速响应用户需求；搭建稳定性测试服务尽可能模拟线上真实复杂场景；较高的单测覆盖率保证代码逻辑正常。\n02 内外版本维护 字节跳动内部有着完善的微服务体系，团队非常重视开源建设和承诺，Hertz 和 CloudWeGo 中的开源项目 Kitex 相同，保持内外一致，项目的核心能力均迁移至开源库中，在内部仅封装一层壳帮助企业内无感升级，以保证对开源长期维护的承诺，并且所有开源特性，都会在内部的稳定性验证后才会开源出来。\n后续，团队将持续以 Hertz 开源库为主进行迭代，及时响应社区需求与问题，为用户提供更好的体验和使用保障。\n对于 Hertz 的开发者来说，Hertz 同样支持对框架进行灵活的扩展，以适应业务需求。我们也欢迎外部的开发者将自己的贡献提交到社区当中，在社区进行开源共建，共同打造一款有着完善生态、极致性能和高易用性的 HTTP 框架。\n03 RoadMap 对于基础架构团队而言，Hertz 不仅仅是一个开源项目，它也是一个真实的超大规模企业级实践项目。通过开源，我们希望 Hertz 能丰富云原生社区的 Golang 中间件体系，完善 CloudWeGo 生态矩阵，为更多开发者和企业搭建云原生化的大规模分布式系统，提供一种现代的、资源高效的的技术方案。\n如前文所述，目前 Hertz 只开源了内部经过稳定性验证的部分，未来，我们会进一步推动其走向完善：\n 云原生能力支持。支持 xDS API，从 Istio 动态获取服务配置。 多协议的支持。Hertz 目前只开源了 HTTP1 的部分，未来我们还会开源其他协议，如：HTTP2、Websocket、ALPN 等，为开发者提供更多场景的微服务需求支持。如果有需求也可以提交 issue 告诉我们，让我们知道您的需求以便快速支持。 更好用的命令行工具。我们将继续迭代 Hz，持续集成各种常用的中间件，提供模块化构建能力，用户可以按需选择所需组件。 更完善的生态支持。由于 Hertz 没有采用 go net 的数据结构，需要更多的生态支持。第一批开源我们只开源了 CORS、Trace、Metrics 等生态。未来我们还将支持包括反向代理、Session 等生态。 结合内外部用户需求，持续迭代。项目开源后，我们也会根据开发者需求开展迭代。  欢迎大家向 Hertz 提交 issue 和 PR 共建 Hertz。\n我们诚心期待更多的开发者加入，也期待 Hertz 助力越来越多的企业快速构建云原生架构。我们也真诚欢迎企业用户迁移使用，我们会提供专项技术支持和交流，欢迎入群咨询。\n04 相关链接 项目地址：https://github.com/cloudwego/hertz\n项目官网：https://www.cloudwego.io\n","categories":"","description":"","excerpt":"今天，经过了字节跳动内部一年多的使用和迭代，高性能企业级 HTTP 框架—— Hertz，已在 CloudWeGo 正式开源啦！Hertz  …","ref":"/zh/blog/2022/06/21/%E8%B6%85%E5%A4%A7%E8%A7%84%E6%A8%A1%E7%9A%84%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BE%AE%E6%9C%8D%E5%8A%A1-http-%E6%A1%86%E6%9E%B6-hertz-%E6%AD%A3%E5%BC%8F%E5%BC%80%E6%BA%90/","tags":"","title":"超大规模的企业级微服务 HTTP 框架 — Hertz 正式开源！"},{"body":"Feature  [#31] feat: close connection after responding to the short-connection request. [#44] feat: add the VisitAllCustomHeader method. [#59] feat: support windows. [#70] feat: add code generator hz. [#64] feat: add adaptor for Hertz Request \u0026 Response to net/http Request \u0026 ResponseWriter. [#45] feat: add ctx.Body().  Optimize  [#57] optimize: use http.TimeFormat as layout for http date, which can avoid more copying. [#58] optimize: add remote address to the error log when server processes the error. [#41] optimize: use CtxErrorf instead of ‘Errorf’ when server panic.  Refactor  [#37] refactor: unify the entry of setting request options to prevent options uninitialized from causing panic. [#52] refactor: omit redundant nil check around loop. [#33] refactor: simplify code in AddMissingPort. [#27] refactor: use errors.NewPublic rather than fmt.Errorf. [#34] refactor: remove fshandler and related tests.  Style  [#29] style(*): fix typos.  Docs  [#60] docs: add icon in README.md and README_cn.md. [#54] docs: Update README.md.  ","categories":"","description":"","excerpt":"Feature  [#31] feat: close connection after responding to the …","ref":"/blog/2022/06/20/hertz-release-v0.1.0/","tags":"","title":"Hertz Release v0.1.0"},{"body":"Feature  [#70] feat: 增加 hz 脚手架。 [#64] feat: 增加 Hertz Request \u0026 Response 到 net/http Request \u0026 ResponseWriter 的适配器。 [#45] feat: 添加 ctx.Body() 方法。 [#44] feat: 在 request header 上添加 VisitAllCustomHeader 方法，使得传入的函数 f 只作用在用户自定义的 header 上（除了 cookie, host, content-length, content-type, user-agent 和 connection 以外的 header）。 [#59] feat: 支持 windows 开发环境。  Refactor   [#37] refactor: 统一设置 request options 的入口，防止 options 未初始化导致 panic。\n  [#52] refactor: 去掉 for 循环中多余的判空。\n  [#33] refactor: 当子串长度确定为 1 时，可以直接调用 strings.IndexByte 函数而不是像 strings.Index 一样先调用 len() 判断子串长度后再调用 strings.IndexByte 函数； 为省去整型数字转字符串的工作，可以将相关变量直接定义成 string 类型而不是 int 类型； net 包下的 JoinHostPort 函数会再次判断 ‘:’ 是否在 addr 中，如果不在则将 host 与 port 相关字符串连接起来。然而在 AddingMissingPort 函数中调用 net.JoinHostPort 时，':' 应不在 addr 中。所以在此可以不调用 net.JoinHostPort，而是直接连接 host 和 port 信息。\n  [#27] refactor: 当字符串不需要格式化时，使用 hertz 的 errors.NewPublic 创建 error 而不是使用 fmt.Errorf。\n  Style  [#29] style(*): 修正拼写错误。  Optimize  [#57] optimize: 使用 http.TimeFormat 格式化 HTTP 中的 Date 信息，避免产生更多的复制。 [#58] optimize: 服务端错误日志中添加对端地址。 [#41] optimize(recovery): 使用 ‘CtxErrorf’ 代替 ‘Errorf’ 当服务 panic。  Docs  [#60] docs: readme 文件中添加 icon。  ","categories":"","description":"","excerpt":"Feature  [#70] feat: 增加 hz 脚手架。 [#64] feat: 增加 Hertz Request \u0026 …","ref":"/zh/blog/2022/06/20/hertz-v0.1.0-%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/","tags":"","title":"Hertz v0.1.0 版本发布"},{"body":"Feature  [#473] feat(grpc): support short connection for gRPC unary. [#431] feat(limiter): extend outside limiter implementation and fix problems of rate limiter of multiplexed server.  Optimize  [#465] optimize(ttheader): set remote address for client-side after decoding TTHeader. [#466] optimize(mux): wrap ErrReadTimeout with ErrRPCTimeout in mux scenario. [#425] optimize(limiter): promise tokens of the first second don’t exceed limit significantly.  Bugfix  [#485] fix(grpc): fix the incorrect integer conversion. [#474] fix(trans): fix detection handler panic when conn inactive early. [#445] fix(retry): race problems of callTimes in retry and some fields of rpcStats. [#471] fix(retry): callCosts race in backup request.  Test  [#404] test: add unit test for pkg/retry. [#439, #472] test: add unit test for pkg/remote/remotecli. [#462, #457] test: add unit test for pkg/remote/trans/nphttp2/grpc. [#420] test: add ut for pkg/remote/trans/nphttp2.  Refactor  [#464] refactor(ttheader): change protocol id of Kitex Protobuf in TTHeader and promise the change is compatible with the old version.  Chore  [#453, #475] chore: upgrade netpoll and bytedance/gopkg. [#458] chore: fix ci reviewdog and pr ut didn’t run. [#454] chore: use self-hosted ci to optimize speed. [#449] chore: fix github issue template.  Style  [#486] style(trans): add comment for detection trans handler.  Docs  [#482] docs: update FAQ of readme.  Dependency Change  github.com/cloudwego/netpoll: v0.2.2 -\u003e v0.2.4  ","categories":"","description":"","excerpt":"Feature  [#473] feat(grpc): support short connection for gRPC unary. …","ref":"/blog/2022/06/02/kitex-release-v0.3.2/","tags":"","title":"Kitex Release v0.3.2"},{"body":"Feature  [#473] 功能 (grpc): 为 Kitex gRPC unary 模式增加短连接功能。 [#431] 功能 (limiter):  支持自定义的限流实现，接口增加了请求参数的传递； 修复多路复用场景下 Server 的 QPS 限流器问题，添加基于 OnMessage 的限流； 调整默认的限流生效时机，只有使用框架 QPS 限流且非多路复用的场景下，才使用基于 OnRead 的限流。    Optimize  [#465] 优化 (ttheader): Client 端在 TTHeader 解码结束后赋值 Remote Address (用于 Proxy 场景请求失败时获取对端地址)。 [#466] 优化 (mux): 连接多路复用场景的 ErrReadTimeout 用 ErrRPCTimeout 封装返回。Proxy 场景请求失败时获取对端地址)。 [#425] 优化 (limiter): 优化限流实现，保证第一秒的 Tokens 不会大幅超过限制。  Bugfix  [#485] 修复 (grpc): 修复 grpc 内不恰当的 int 类型转换。 [#474] 修复 (trans): 在 detection handler 中增加检测。当 OnInactive 比 OnActive 先发生，或者 OnActive 返回 error 时，防止空指针 panic。 [#445] 修复 (retry):  修复重试中 callTimes 字段的 race 问题； 修复 rpcStats 中一些字段的 race 问题。   [#471] 修复 (retry): 修复在 backup request 中的一个 race 问题。  Test  [#404] test: 增加 pkg/retry 的单测。 [#439, #472] test: 增加 pkg/remote/remotecli 的单测。 [#462, #457] test: 增加 pkg/remote/trans/nphttp2/grpc 的单测。 [#420] test: 增加 pkg/remote/trans/nphttp2 的单测。  Refactor  [#464] refactor (ttheader): 修改 Kitex Protobuf 在 TTHeader 中的 protocolID，同时保证该变更与低版本的兼容性。  Chore  [#453, #475] chore: 更新 netpoll 和 bytedance/gopkg 的版本。 [#458] chore: 修复了 reviewdog 失效的问题与 fork pr 单测的问题。 [#454] chore: 现在的 CI 受限于 github runner 的性能经常会失败，尝试改成 self-hosted runner 来提升性能。 [#449] chore: 更新 issue template，修改为更适合 Kitex 项目的问题模板。  Style  [#486] style (trans): 为 detection trans handler 增加注释信息。  Docs  [#482] docs: 在 Readme 中增加 FAQ 链接。  Dependency Change  github.com/cloudwego/netpoll: v0.2.2 -\u003e v0.2.4  ","categories":"","description":"","excerpt":"Feature  [#473] 功能 (grpc): 为 Kitex gRPC unary 模式增加短连接功能。 [#431] …","ref":"/zh/blog/2022/06/02/kitex-v0.3.2-%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/","tags":"","title":"Kitex v0.3.2 版本发布"},{"body":"从 CloudWeGo 谈云原生时代的微服务与开源  本文整理自罗广明在 DIVE 全球基础软件创新大会 2022 的演讲分享，主题为《从 CloudWeGo 谈云原生时代的微服务与开源》。\n 01 项目创造的思考与哲学 我们团队经常会被人问到，你们为什么创造一个新的项目？我认为这是一个哲学问题。\n纵观整个开源社区，每个时间段都会有各种各样的项目被重复地创造出来，这其中的大部分项目很快便销声匿迹了，只有一部分项目能够存活下来。当旁观者看到这样一番景象时，渐渐地，越来越多的人停留于项目搜寻，而放弃了成为项目创作者的机会。久而久之，我们开始忧虑下一代是否还会有新的项目可以使用？难道未来在同一领域，一个项目就能统一整个市场？\n其实，在程序员的世界里，参考旧的项目来创造新的项目一点都不可耻。创造不仅意味着思考、权衡与设计，更需要我们贡献项目的特殊与差异。这其中涌现了很多后起之秀，正是他们促成了开源社区的多样性。“每一行代码都是一次精心的设计”是我们对优秀创造者的最佳赞誉。而一项优秀的代码设计往往包含两个最基本的特性：正确性和可维护性。同时，这两种特性恰恰又对应了两种不同的人格。\n第一种人格，设计者与实现者，其驾驭是相对简单的，只要功能实现，通过测试，运行正确就算完成了。然而，第二种人格，阅读者和维护者，却要求更高的代码质量，更明晰的代码结构和更好的扩展性。只有同时具备这两种人格，开发者才能游刃有余地创造出一个优秀的项目。\n优秀的项目被创造出来意味着什么呢？千千万万的用户可以评估并且使用它。这也从侧面表明了开源本身可以避免更多项目被重复地创造出来。\n02 CloudWeGo 简介 CloudWeGo 是字节跳动基础架构团队开源出来的项目，它是一套可快速构建企业级云原生架构的中间件集合，它专注于微服务通信与治理，具备高性能、可扩展、高可靠的特点，且关注易用性。\nCloudWeGo 在第一阶段开源了四个项目：\n Kitex：高性能、强可扩展的 Golang RPC 框架 Netpoll：高性能、I/O 非阻塞、专注于 RPC 场景的网络框架 Thriftgo：Golang 实现的 thrift 编译器，支持插件机制和语义检查 Netpoll-http2：基于 Netpoll 的 HTTP/2 实现  除了这几个主要项目外，CloudWeGo 紧随其后陆续开源了 Kitex-benchmark、Netpoll-benchmark、Thrift-gen-validator、Kitex-examples 、Netpoll-examples等项目。\n鉴于文章篇幅有限，下文将重点介绍 CloudWeGo 核心项目 Kitex。\n从演进历史来看，2014 年，字节跳动技术团队引入 Golang 解决长连接推送业务面临的高并发问题，两年后，内部技术团队基于 Golang 推出了一个名为 Kite 的框架，同时对开源项目 Gin 做了一层很薄的封装，推出了 Ginex。这两个框架极大推动了 Golang 在公司内部的应用。此后，围绕性能和可扩展性设计，字节跳动重构 Kite，并在次年 10 月完成并发布Kitex，投入到内部应用中。据悉，截至 2021 年 9 月，线上有 3w+ 微服务使用 Kitex，大部分服务迁移新框架后可以收获 CPU 和延迟上的收益。\n从架构上看，Kitex 主要分为两部分。其中 Kitex Core 是它的的主干逻辑，定义了框架的层次结构、接口，还有接口的默认实现。最上面 Client 和 Server 是对用户暴露的，包含 Option 配置以及其初始化的逻辑；中间的 Modules 模块是框架治理层面的功能模块和交互元信息，而 Remote 模块是与对端交互的模块，包括编解码和网络通信。另一部分 Kitex Tool 则是对应生成代码相关的实现，生成代码工具就是编译这个包得到的，里面包括 IDL 解析、校验、代码生成、插件支持、自更新等。\n从功能与特性这两个角度来看，主要可以分为以下七个方面：\n 高性能：网络传输模块 Kitex 默认集成了自研的网络库 Netpoll，性能相较使用 go net 有显著优势；除了网络库带来的性能收益，Kitex 对 Thrift 编解码也做了深度优化。关于性能数据可参考 kitex-benchmark。 扩展性：Kitex 设计上做了模块划分，提供了较多的扩展接口以及默认的扩展实现，使用者也可以根据需要自行定制扩展，更多扩展能力参见 CloudWeGo 官网文档。Kitex 也并未耦合 Netpoll，开发者也可以选择其它网络库扩展使用。 消息协议：RPC 消息协议默认支持 Thrift、Kitex Protobuf、gRPC。Thrift 支持 Buffered 和 Framed 二进制协议；Kitex Protobuf 是 Kitex 自定义的 Protobuf 消息协议，协议格式类似 Thrift；gRPC 是对 gRPC 消息协议的支持，可以与 gRPC 互通。除此之外，使用者也可以扩展自己的消息协议。 传输协议：传输协议封装消息协议进行 RPC 互通，传输协议可以额外透传元信息，用于服务治理，Kitex 支持的传输协议有 TTHeader、HTTP2。TTHeader 可以和 Thrift、Kitex Protobuf 结合使用；HTTP2 目前主要是结合 gRPC 协议使用，后续也会支持 Thrift。 多消息类型：支持 PingPong、Oneway、双向 Streaming。其中 Oneway 目前只对 Thrift 协议支持，双向 Streaming 只对 gRPC 支持，后续会考虑支持 Thrift 的双向 Streaming。 服务治理：支持服务注册/发现、负载均衡、熔断、限流、重试、监控、链路跟踪、日志、诊断等服务治理模块，大部分均已提供默认扩展，使用者可选择集成。 Kitex 内置代码生成工具，可支持生成 Thrift、Protobuf 以及脚手架代码。原生的 Thrift 代码由本次一起开源的 Thriftgo 生成，Kitex 对 Thrift 的优化由 Kitex Tool 作为插件支持。Protobuf 代码由 Kitex 作为官方 protoc 插件生成 ，目前暂未单独支持 Protobuf IDL 的解析和代码生成。  简单总结一下，CloudWeGo 不仅仅是一个开源的项目，也是一个真实的、超大规模的企业级最佳实践。它源自企业，所以天生就适合在企业内部落地；它源自开源，最终也拥抱了开源，从 Go 基础库，到 Go 网络库和 Thrift 编译器，再到上层的服务框架，以及框架拥有的所有企业级治理能力，均对外开放开源。\n03 CloudWeGo 的微服务治理 微服务架构是当前软件开发领域的技术热点。大系统终究会拆解成小系统，“合久必分，分而治之”，传统行业的系统架构大多都是庞大的单体架构，微服务是架构发展过程中一个非常自然的演变状态。\n那么，什么是微服务治理呢？众说纷纭，业界没有达成一个共识。广义上，服务治理关注服务生命周期相关要素，包括服务的架构设计、应用发布、注册发现、流量管理，监控与可观测性、故障定位、安全性等；又或将其分为架构治理、研发治理、测试治理、运维治理、管理治理。狭义上，服务治理技术包括服务注册与发现、可观测性、流量管理、安全、控制。后续主要是从狭义上服务治理的角度出发，展开介绍 CloudWeGo-Kitex 相关的思考和探索。\n服务注册与发现 Kitex 并不提供默认的服务注册发现，体现了框架的中立特征。Kitex 支持自定义注册模块和发现模块，使用者可自行扩展集成其他注册中心和服务发现实现，该扩展分别定义在 Pkg/Registry 和 Pkg/Discovery 下。\nKitex 服务注册扩展接口如下所示，更多详情可以查看官网框架扩展 -\u003e 服务注册扩展。\nKitex 服务发现扩展接口如下所示，更多详情可以查看官网框架扩展 -\u003e 服务发现扩展。\n截止日前，Kitex 已经通过社区开发者的支持，完成了 ETCD、ZooKeeper、Eureka、Consul、Nacos、Polaris 多种服务发现模式，当然也支持 DNS 解析以及 Static IP 直连访问模式，建立起了强大且完备的社区生态，供用户按需灵活选用。\n特别鸣谢 @li-jin-gou @liu-song @baiyutang @duduainankai @horizonzy @Hanson 等几位社区贡献者对上述服务发现扩展库的实现与支持。更多代码详情可以查看 https://github.com/kitex-contrib 。\n熔断 前面介绍了 Kitex 服务注册与发现机制，这一点对于业务接入框架非常重要，缺少这一环节微服务之间无法实现互通。那么熔断对于微服务有什么作用呢？\n在微服务进行 RPC 调用时，下游服务难免会出错，当下游出现问题时，如果上游继续对其进行调用，既妨碍了下游的恢复，也浪费了上游的资源。为了解决这个问题，可以设置一些动态开关，当下游出错时，手动的关闭对下游的调用，然而更好的办法是使用熔断器，自动解决这个问题。\nKitex 提供了熔断器的实现，但是没有默认开启，需要用户主动开启后即可使用。\nKitex 大部分服务治理模块都是通过 Middleware 集成，熔断也是一样。Kitex 提供了一套 CBSuite，封装了服务粒度的熔断器和实例粒度的熔断器。\n 服务粒度熔断：按照服务粒度进行熔断统计，通过 WithMiddleware 添加。服务粒度的具体划分取决于 Circuit Breaker Key，即熔断统计的 Key，初始化 CBSuite 时需要传入 GenServiceCBKeyFunc。默认提供的是 circuitbreaker.RPCInfo2Key，该 Key 的格式是 fromServiceName/toServiceName/method，即按照方法级别的异常做熔断统计。 实例粒度熔断：按照实例粒度进行熔断统计，主要用于解决单实例异常问题，如果触发了实例级别熔断，框架会自动重试。  熔断器的思路很简单根据 RPC 成功或失败的情况，限制对下游的访问。通常熔断器分为三个时期：CLOSED、OPEN、HALFOPEN。当RPC 正常时，为 CLOSED；当 RPC 错误增多时，熔断器会被触发，进入 OPEN；OPEN 后经过一定的冷却时间，熔断器变为 HALFOPEN；HALFOPEN 时会对下游进行一些有策略的访问，然后根据结果决定是变为 CLOSED，还是 OPEN。总的来说三个状态的转换大致如下图：\n关于 Kitex 熔断器实现的更多细节和原理，可以查看官网基本特性 -\u003e 熔断器章节。\n限流 如果说熔断是从客户端出发保护调用链，以防止系统雪崩，那么限流则是一种保护服务端的措施，防止上游某个 Client 流量突增导致 Server 过载。\nKitex 支持限制最大连接数和最大 QPS。在初始化 Server 的时候，增加一个 Option：\n其中 MaxConnections 表示最大连接数，MaxQPS` 表示最大 QPS，此外，Kitex 还提供了动态修改限流阈值的能力。\nKitex 分别使用了 ConcurrencyLimiter 和 RateLimiter 对最大连接数和最大 QPS 进行限流，其中 ConcurrencyLimiter 采用了简单的计数器算法，RateLimiter 采用了“令牌桶算法”。\n限流状态的监控也是重要的一环，Kitex 定义了 LimitReporter 接口，用于限流状态监控，例如当前连接数过多、QPS 过大等。如有需求，用户需要自行实现该接口，并通过 WithLimitReporter 注入。\n请求重试 Kitex 提供三类重试：超时重试、Backup Request，建连失败重试。其中建连失败是网络层面问题，由于请求未发出，框架会默认重试，下面重点介绍前两类重试的使用。需要注意的是，因为很多的业务请求不具有幂等性，这两类重试不会作为默认策略，用户需要按需开启。\n 超时重试：错误重试的一种，即客户端收到超时错误的时候，发起重试请求。 Backup Request：客户端在一段时间内还没收到返回，发起重试请求，任一请求成功即算成功。Backup Request 的等待时间 RetryDelay 建议配置为 TP99，一般远小于配置的超时时间 Timeout。  服务中的长尾请求增加了服务的整体延迟，而长尾请求占比很低，如上图所示，一个真实服务的延迟分布，能明显看出长尾现象，最大延迟 60ms，而 99% 服务可以在 13ms 返回。当请求延迟达到 13ms 的时候就已经进入长尾请求，这个时候我们可以再发出一条请求，这条请求大概率会在 13ms 内返回，任意一次请求返回我们就认为请求成功，即通过增加适当的负载，大大减少了响应时间的波动。关于超时重试和 Backup Request 的优缺点以及适用场景，可见下表：\n负载均衡 Kitex 默认提供了两种负载均衡算法实现：\n WeightedRandom：这个算法使用的是基于权重的随机策略，也是 Kitex 的默认策略。它会依据实例的权重进行加权随机，并保证每个实例分配到的负载和自己的权重成比例。 ConsistentHash：一致性哈希主要适用于对上下文（如实例本地缓存）依赖程度高的场景，如希望同一个类型的请求打到同一台机器，则可使用该负载均衡方法。  ConsistentHash 在使用时，需要注意如下事项：\n 下游节点发生变动时，一致性哈希结果可能会改变，某些 Key 可能会发生变化； 如果下游节点非常多，第一次冷启动时 Build 时间可能会较长，如果 RPC 超时短的话可能会导致超时； 如果第一次请求失败，并且 Replica 不为 0，那么会请求到 Replica 上；而第二次及以后仍然会请求第一个实例。  可观测性 框架自身不提供监控打点实现，提供了 Tracer 接口，用户可以根据需求实现该接口，并通过 WithTracer Option 注入到框架中。\nKitex 的监控打点、Metrics 上报以及链路追踪，都可以通过上述接口进行扩展。\n目前 kitex-contrib 组织下提供了 Prometheus 的监控扩展，OpenTracing 的链路追踪扩展，以及 OpenTelemetry 可观测性全家桶（Metrics + Tracing + Logging）扩展实现，用户可以按需接入相应的扩展。\n微服务框架与服务网格 服务框架是传统微服务技术的核心所在。早期微服务技术中的服务注册、发现、调用、治理、观测都离不开服务框架。这也带来了一些问题，比如业务研发者需要感知并使用服务框架的服务治理能力，框架版本升级困难，框架越来越重难于维护等等。\n服务网格（Service Mesh） 是将无侵入服务治理定义的更为深入的微服务架构方案，被称为第二代微服务架构。通过将微服务治理能力以独立组件（Sidecar）整合并下沉到基础设施，服务网格可以实现应用业务逻辑与服务治理逻辑完全分离，这也使支持多语言、热升级等高阶特性变得顺理成章。\n进入云原生时代，随着服务网格技术的逐步发展，我们也要用发展的眼光进行架构规划和设计，微服务框架和服务网格未来必定会是并存的，统一组成服务治理体系。在字节跳动，服务治理体系就是由服务框架和服务治理组成。以 Golang 服务为例，CloudWeGo 提供业务强相关、强侵入的服务治理，字节 Service Mesh 提供业务弱相关、弱侵入的服务治理，相互搭配，相互协商，既解决了业务开发所需的脚手架和开发模式，又让服务治理的接入更加容易。\n与此同时，在服务网格和服务框架同时使用的场景下，服务框架必须要支持灵活卸载治理能力，服务网格也需要保证功能的稳定性。在未来技术的演进方向上，服务框架也主要专注于编解码、通信效率、多协议支持等方面，而服务网格则可以深入更多无侵入的服务治理功能研发中。\n此外，在大规模场景下，针对服务治理新功能的研发需求决策，我们往往还需要考虑以下因素：\n 性能: 大部分业务很在意，也是团队一直努力的重点； 普遍性:需要评估是不是所有业务都需要的能力； 简洁: 通俗说，我们不太希望引入太多的线上问题或者太复杂的使用说明文档； ROI：功能迭代、产品升级需要考虑整体投资回报率。  04 CloudWeGo 的开源之路 字节内部版本的 Kitex 是依赖于开源版本的 Kitex，因此可以理解为 Kitex 内外同源，不存在两个 Kitex。\n开源的原因 回到开篇的问题，为什么要创造一个新的项目，并且开源 CloudWeGo 呢？\n首先，CloudWeGo 里面的项目都是在字节内部经过大规模落地实践验证的，开源后每个功能的迭代也都是第一时间在内部使用验证过的，是一个真正的企业级落地项目，开源用户和字节内部业务使用的是同一套服务框架；其次，CloudWeGo 提供的功能，尤其是协议支持和服务治理，都是能解决真实业务痛点的，每一行代码优化都能实实在在地提升用户服务的性能；最后，CloudWeGo 的研发也借鉴了一些知名开源项目的设计思路，同时也依赖一些开源项目的实现，我们把 CloudWeGo 开源出去也是为了回馈社区，给开源社区贡献一份力量。\nCloudWeGo 在设计之初，就同时考虑了正确性和可维护性，除了代码逻辑的正确性，高质量的代码、明晰的代码结构和优良的扩展性一直都是 CloudWeGo 追求的方向和实践的信条。\nCloudWeGo 服务于用户、需求驱动，为用户提供开箱即用的服务框架及相关中间件，希望可以服务于更多企业和独立开发者，避免用户重复创造。\n开源的历程 CloudWeGo 自 2021 年 9 月 8 日正式对外官宣，主要子项目 Kitex 先后发布 v0.1.0 和 v0.2.0，支持了许多新的功能，对性能、代码、文档也相继做了许多优化。截止到 2022 年 4 月，距离首次官宣 7 个月，仅 CloudWeGo-Kitex 就收获了 4000 个 Star，累计近 50 个 Contributors，达到了一个新的里程碑，这很有趣，并且十分振奋人心，不是吗？\nCloudWeGo 团队自开源之初就非常重视社区建设，“Community Over Code” 也是 CloudWeGo 社区所遵循的文化和目标。\n从搭建用户群，建设官网和文档，积极维护项目 Issue，及时处理新的 PR，再到我们与贡献者的深入沟通和对他们的培养，每一个动作都体现我们的决心。为了推进社区建设规范化和标准化，CloudWeGo 团队先后创建了 Community 仓库用来定义社区成员晋升机制以及存档社区材料。\n为了践行公开透明和开源开放的开源文化，搭建开放的对话与交流平台，CloudWeGo 组织了社区双周例会，在例会上同步社区近期计划并积极听取社区成员的建议，与社区贡献者讨论相关技术方案实现。\n截止目前，通过社区 Maintainer 的培养、Contributor 的主动申请、社区管理委员会的投票审批，已经正式通过了 5 位 Committer 的加入申请，极大地壮大了 CloudWeGo 社区核心力量，他们为社区的发展作出了重大贡献。\n后续的规划 CloudWeGo 在 2021 年底收录进入 CNCF Landscape，丰富了 CNCF 在 RPC 领域的生态，给全球用户在做技术选型时提供了一套新的选择。\n尽管取得了一些小小的成绩，但是 CloudWeGo 仍旧还是一个年轻的项目，开源贵在持之以恒、长期建设，CloudWeGo 团队也会持续完善，继续向前。\n从社区建设方面来看，CloudWeGo 团队将继续提供更多新人友好的 Good-first-issue，坚持组织社区例会，定期举办开源技术沙龙，提供更易于理解的技术文档，另外也将继续欢迎更多新的开发者参与到社区建设中来。\n从开源规划来看，HTTP 框架 Hertz 开源在即，还有更多中间件小工具、扩展库也都在持续开源中。此外，CloudWeGo 主创团队还研发了一套 Rust RPC 框架，正在内部落地实践验证中，在不久的将来，也将对外开源。\n从功能研发计划来看，以 CloudWeGo-Kitex 为例，将继续以内外部用户需求为驱动力，持续开发新的功能并迭代完善已有的功能。其中，包括支持连接预热、自定义异常重试、对 Protobuf 支持的性能优化，支持 xDS 协议等。\n从开源生态来看，目前 CloudWeGo-Kitex 已经完成了诸多开源项目的对接，未来也将会按需支持更多开源生态。此外，CloudWeGo 也在和国内外主流公有云厂商进行合作对接，提供开箱即用、稳定可靠的微服务托管与治理产品的基座；CloudWeGo 也积极与国内外软件基金会开展合作和交流，探索新的合作模式。\nCloudWeGo 未来可期，我们期待更多用户使用我们的项目，也期待有更多开发者可以加入共建 CloudWeGo 社区，共同见证云原生时代一个初生但了不起的微服务中间件和开源项目。\n","categories":"","description":"","excerpt":"从 CloudWeGo 谈云原生时代的微服务与开源  本文整理自罗广明在 DIVE 全球基础软件创新大会 2022 的演讲分享，主题为《 …","ref":"/zh/blog/2022/05/26/%E4%BB%8E-cloudwego-%E8%B0%88%E4%BA%91%E5%8E%9F%E7%94%9F%E6%97%B6%E4%BB%A3%E7%9A%84%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%B8%8E%E5%BC%80%E6%BA%90/","tags":"","title":"从 CloudWeGo 谈云原生时代的微服务与开源"},{"body":"字节微服务框架的挑战和演进 2014 年以来，字节跳动内部业务的快速发展，推动了长连接推送服务，它们面临着高并发的业务需求问题，对性能和开发效率都有很高要求。当时的业务，大部分都是由 Python 开发，难以应对新出现的问题。项目负责人在一众现存的技术栈中选择了 Golang 这一门新兴的编程语言，快速解决了性能和开发效率的问题。随后，字节跳动内部开始逐渐推广使用 Golang 进行服务开发。\n2016 年， 第一代 Golang RPC 框架 Kite 正式发布。Kite 是一个基于 Apache Thrift 进行包装的 RPC 框架，它在 Facebook 开源的 Thrift 之上提供了结合字节跳动内部基础设施的治理功能，同时还提供了一套简单易用的生成工具。随着 Kite 的发展，业务开始大规模使用 Golang。然而，在业务发展的过程中，由于研发专注于实现业务需求，对于框架的可维护性考量不足，Kite 逐渐背上了一些技术包袱，越来越难以满足业务在高性能和新特性方面的需求。因此我们决定对 Kite 进行重新设计，于是出现了 Kitex。\n2020 年，Kitex 在内部发布了 v1.0.0，并且直接接入了 1,000+ 服务。由于 Kitex 的优秀性能和易用性，Kitex 在内部得到了大规模发展。直到 2021 年年中，字节跳动内部已有 2w+ 服务使用了 Kitex。因此，我们决定全面优化 Kitex，将其实践成果进行开源，反馈给开源社区。\n字节跳动 Golang RPC 框架的演进 Kite 的缺陷 Kite 作为字节跳动第一代 Golang RPC 框架，主要存在以下缺陷：\n Kite 为了快速支持业务发展需求，不可避免地耦合了部分中台业务的功能； Kite 对 Go modules 支持不友好（Go modules 在 2019 年才进入语言核心）； Kite 自身的代码拆分成多仓库，版本更新时推动业务升级困难； Kite 强耦合了早期版本的 Apache Thrift，协议和功能拓展困难； Kite 的生成代码逻辑与框架接口强耦合，成为了性能优化的天花板。  因此，业务的快速发展和需求场景的多样化，催生了新一代 Golang RPC 框架 Kitex。\nKitex Kitex 的架构主要包括四个部分：Kitex Tool、Kitex Core、Kitex Byted、Second Party Pkg。\n Kitex Core 是一个携带了一套微服务治理功能的 RPC 框架，它是 Kitex 的核心部分。 Kitex Byted 是一套结合了字节跳动内部基础设施的拓展集合。通过这一套拓展集合，Kitex 能够在内部支持业务的发展。 Kitex Tool 是一个命令行工具，能够在命令行生成我们的代码以及服务的脚手架，可以提供非常便捷的开发体验。 Second Party Pkg，例如 Netpoll， Netpoll-http2，是 Kitex 底层的网络库，这两个库也开源在 CloudWeGo 组织中。  Kitex 的架构设计 总的来说， Kitex 主要有五个特点：面向开源、功能丰富、灵活可拓展、支持多协议、高性能。\n面向开源 由于之前已经体验过了 Kite 维护的各种问题，我们在立项之初就考虑到了未来可能会开源 Kitex。因此，我们设计的第一个宗旨就是不将 Kitex 和公司内部的基础设施进行强耦合或者硬编码绑定。Kitex Core 是一个非常简洁的框架，公司内部的所有基础设施都以拓展的方式注入到 Kitex Core 里。即使我们现在已经开源了，它也以这种形式存在。公司内部基础设施的更新换代，和 Kitex 自身的迭代是相互独立的，这对于业务来说是非常好的体验。同时，在 Kitex 的接口设计上，我们使用了 Golang 经典的 Option 模式，它是可变参数，通过 Option 能够提供各种各样的功能，这为我们的开发和业务的使用都带来了非常大的灵活性。\nKitex 的功能特性 治理能力 Kitex 内置了丰富的服务治理能力，例如超时熔断、重试、负载均衡、泛化调用、数据透传等功能。业务或者外部的用户使用 Kitex 都是可以开箱即用的。如果你有非常特殊的需求，你也可以通过我们的注入点去进行定制化操作，比如你可以自定义中间件去过滤或者拦截请求，定义跟踪器去注入日志、去注入服务发现等。在 Kitex 中，几乎一切跟策略相关的东西都是可以定制的。\n以服务发现为例，Kitex 的核心库里定义了一个 Resolver interface 。任何一个实现了这四个方法的类型都可以作为一个服务发现的组件，然后注入到 Kitex 来取代 Kitex 的服务发现功能。在使用时，客户端只需要创建一个 Resolver 的对象，然后通过 client.WithResolver 注入客户端，就可以使用自己开发的服务发现组件。\nKitex 的一个创新之处是使用 Suite 来打包自定义的功能，提供一键配置基础依赖的体验。\n它能在什么地方起作用呢？例如，一个外部企业想要启用或者接入 Kitex， 它不可能拥有字节跳动内部的所有基础设施。那么企业在使用的时候肯定需要定制化，他可能需要定义自己的注册中心、负载均衡、连接池等等。如果业务方要使用这些功能的话，就需要加入非常非常多的参数。而 Suite 可以通过一个简单的类一次性包装这些功能，由此，业务方使用时，仍然是以单一的参数的方式添加，十分方便。又例如，我现在开发一个叫 mysuite 的东西，我可能提供一个特殊的服务发现功能，提供了一个拦截的中间件，还有负载均衡功能等。业务方使用时，不需要感知很多东西去配置，只需要添加一个 Suite 就足够了，这点非常方便一些中台方或者第三方去做定制。\n示例 多协议 Kitex 网络层基于高性能网络库 Netpoll 实现。在 Netpoll 上，我们构建了 Thrift 和 Netpoll-http2；在 Thrift 上，我们还做了一些特殊的定制，例如，支持 Thrift 的泛化调用，还有基于 Thrift 的连接多路复用。\n多协议 代码生成工具 和 Kitex 一同出现的，还有我们开发的一个简单易用的命令行工具。如果我们写了一个 IDL， 只需要提供一个 module 参数和一个服务名称，Kitex 就会为你生成服务代码脚手架。\n目前 Kitex 支持了 Protobuf 和 Thrift 这两种 IDL 的定义。命令行工具内置丰富的选项，可以进行项目代码定制；同时，它底层依赖 Protobuf 官方的编译器，和我们自研的 Thriftgo 的编译器，两者都支持自定义的生成代码插件。\nKitex 的性能表现 字节跳动内部 RPC 框架使用的协议主要都是基于 Thrift，所以我们在 Thrift 上深耕已久。结合自研的 netpoll 能力，它可以直接暴露底层连接的 buffer。在此基础上，我们设计出了 FastRead/FastWrite 编解码实现，测试发现它具有远超过 apache thrift 生成代码的性能。整体而言，Kitex 的性能相当不错，今年 1 月份的数据如下图所示，可以看到，Kitex 在使用 Thrift 作为 Payload 的情况下，性能优于官方 gRPC，吞吐接近 gRPC 的两倍；此外，在 Kitex 使用定制的 Protobuf 协议时，性能也优于 gRPC。\nKitex/gRPC 性能对比（2022 年 1 月数据） Kitex：一个 demo 下面简单演示一下 Kitex 是如何开发一个服务的。\n首先，定义 IDL。这里使用 Thrift 作为 IDL 的定义，编写一个名为 Demo 的 service。方法 Test 的参数是 String，它的返回也是 String。编写完这个 demo.thrift 文件之后，就可以使用 Kitex 在命令行生成指定的生成代码。如图所示，只需要传入 module name，service name 和目标 IDL 就行了。\n定义 IDL 随后，我们需要填充业务逻辑。文件中除了第 12 行，全部代码都是 Kitex 命令行工具生成的。通常一个 RPC 方法需要返回一个 Response，例如这里需要返回一个字符串，那么我们给 Response 赋值即可。接下来需要通过 go mod tidy 把依赖拉下来，然后用 build.sh 构建，就可以启动服务了。Kitex 默认的接听端口是 8888。\n定义 Handler 方法 编译、运行 对于刚刚启动的服务端，我们可以写一个简单的客户端去调用它。服务端写完之后，写客户端也是非常方便的。这里同样是 import 刚刚生成的生成代码，创建 Client、指定服务名字、构成相应的参数，填上“ Hello，word！” ，然后就可以调用了。\n编写 Client Kitex 在字节内部的落地 与内部基础设施的集成 谈到落地，第一步就是 Kitex 和字节跳动内部的基础设施进行结合。字节跳动内部的所有基础设施都是以依赖的方式注入到 Kitex 的。我们将日志、监控、tracing 都定义为 tracer，然后通过 WithTracer 这个 Option 将其注入到 Kitex 里；服务发现是 WithResolver；Service Mesh 则是 WithProxy 等。字节跳动内部的基础设施都是通过 Option 被注入到 Kitex 的，而且所有的 Option 都是通过前面说的 Suite 打包，简单地添加到业务的代码里完成。\n与内部基础设施的集成 内部落地的经典案例：合并部署 这里介绍一个内部落地的经典案例：合并部署。其背景是，在开发微服务时，由于业务拆分和业务场景的多样化，微服务容易出现过微的情况。当服务数量越来越多，网络传输和序列化开销就会越来越大，变得不可忽视。因此，Kitex 框架需要考虑如何减小网络传输和序列化的开销。\n字节跳动基础架构经过一系列的探索和实践，最终推出了合并部署的机制。它的思路是：将有强依赖关系的服务进行同机部署，减少它们之间的调用开销。理论上说起来比较简单，实际过程中需要非常多的组件进行配合。\nKitex 的做法是：首先，它会依赖一套中心化的部署调度和流量控制；其次，我们开发了一套基于共享内存的通信协议，它可以使得我们两个不同的服务在同一台机器部署时，不需要通过网络进行数据传输，直接通过共享内存，减少额外的数据拷贝。\n在服务合并部署的模式下，我们需要特殊的服务发现和连接池的实现、定制化的服务启动和监听逻辑。这些在 Kitex 框架里都是通过依赖注入的方式给添加进来的。Kitex 服务在启动过程中会感知到我们 PaaS 平台提供的指定的环境变量。当它察觉到自己需要按合并部署的方式启动之后，就会启动一个预先注入的特定 Suite，随后将相应的功能全都添加进来再启动，就可以执行我们的合并部署。\n那么，它的效果如何呢？在 2021 年的实践过程中，我们对抖音的某个服务约 30% 的流量进行了合并，服务端的 CPU 的消耗减少了 19%， TP99 延迟下降到 29%，效果相当显著。\n内部落地的经典案例：合并部署 微服务框架推进的痛点  升级慢  大家可能好奇 Kitex 在字节跳动内部推广是不是很顺畅？其实并不是。作为一个相对而言比较新的框架， Kitex 和其它新生项目一样，在推广的过程中都会遇到同样的问题。特别是， Kitex 作为一个 RPC 框架，我们提供给用户的其实是一个代码的 SDK, 我们的更新是需要业务方的用户去感知、升级、部署上线，才能最终体现在他们的服务逻辑里，因此具有升级慢的问题。\n 召回慢  同时，因为代码都是由研发人员编写，如果代码出现了 bug，我们就需要及时地去感知定位问题，通知负责人去更新版本。因此，会有召回慢的问题。\n 问题排查困难  业务方的用户在写代码时，他们其实往往关注的是自己的业务逻辑，他们不会深入理解一个框架内部的实现。所以如果出现问题，他们往往会不知所措，需要依赖我们的业务同学才能进行相应的问题排查。所以会有问题排查困难的问题。\n针对升级慢，我们有两个操作。一是，代码生成工具支持自动更新：当用户在使用时，我们会检查最新版本，然后直接将我们的版本更新到最新版本，这样可以及时把我们的框架新 feature、bug fix 直接推送到业务方；二是，用户群发版周知：我们有一个几千人的用户群，当有了新版本，我们会在用群里周知，可以最大范围的覆盖到我们的目标用户。\n针对召回慢，我们有三个操作。一是，我们在线上建立完整的版本分布统计，监控所有服务上线部署的框架的版本；二是，我们会跟 PaaS 平台合作，在服务上线时进行卡点操作，检查它们使用的框架版本是不是有 bug，是否需要拦截；三是，针对有问题的版本，我们会及时封禁，及时推动用户更新。\n针对问题排查困难，我们有两个操作。一是，我们积累了非常丰富的 Wiki 和问题排查手册，例如超时问题、 协议解析问题等。二是，如果遇到难以解决的问题，我们在线上服务默认开启了 Debug 端口，保证框架开发同学可以第一时间赶到现场去排查。\nKitex 在字节内部的发展 数据显示，在 2020 年，v1.0 版本发布的初始阶段，用户的接受度比较低。直到 2020 年 6 月，线上接受 Kitex 的数量还不到 1000。随后进入快速发展的阶段，到 2021 年年初，累积接近 1w+ 的服务开始使用 Kitex。2021 年底，4w+服务使用 Kitex。\nKitex 的开源实践 开源工作主要包括代码、文档和社区运营三个层面。\n代码层面\n 代码拆分、脱敏； 内部仓库引用开源仓库，避免内外多副本同时维护； 在开源过程中确保内部用户平滑切换、体验无损；  文档层面\n 重新梳理用户文档，覆盖方方面面； 建立详尽的用例仓库(CloudWeGo/Kitex-examples)。  社区运营\n 官网建设； 组建用户群，进行答疑解惑； 飞书机器人对接 Github 的 Issue 管理、PR 管理之类的业务，可以快速响应； 对优秀贡献者进行奖励。  在以上努力下，CloudWeGo/Kitex 仓库目前收获了 4.1k+ stars；Kitex-Contrib 获得多个外部用户贡献的仓库；CloudWeGo 飞书用户群近 950 个用户……\n未来展望 首先，我们仍然会持续向开源社区反馈最新的技术进展。例如在 Thrift 协议上，虽然对 Thrift 的编解码已经做到非常极致的优化了，我们还在探索利用 JIT 手段来提供更多的性能提升；在 Protobuf 上，我们会补足短板，将在 Thrift 方面的优化经验迁移到 Protobuf 上，对 Protobuf 的生成代码和编解码进行优化；Kitex 后续也会进一步融入云原生社区，所以也在考虑支持 xDS 协议。其次，我们会去拓展更多的开源组件，去对接现存的云原生社区的各种常用的或者热门组件。最后，我们也会尝试去对接更多的公有云基础设施，使得用户在公有云上使用 Kitex 时能够拥有愉悦的体验。\n","categories":"","description":"","excerpt":"字节微服务框架的挑战和演进 2014 年以来，字节跳动内部业务的快速发展，推动了长连接推送服务，它们面临着高并发的业务需求问题，对性能和开发 …","ref":"/zh/blog/2022/05/19/%E5%AD%97%E8%8A%82%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%A1%86%E6%9E%B6%E7%9A%84%E6%8C%91%E6%88%98%E5%92%8C%E6%BC%94%E8%BF%9B/","tags":"","title":"字节微服务框架的挑战和演进"},{"body":"Overview Fixed Panic when “peer Close \u0026 local user Close” occurs simultaneously\nSeverity Level Middle\nDescription Fixed concurrency issues in extreme scenarios.\nScenario: “peer Close \u0026 local user Close” occurs at the same time, and C.onClose executes lock(user), and Poller also executes p.detaches (all three conditions are met)\nWhen two goroutines execute op.Control(PollDetach) at the same time, op.unused will be executed twice.\nWhen the program is idle, it is possible that c.onclose has completed the close callback, freeop has been reused, and the state becomes inuse again. That’s when the op.unused in p.detaches is executed; This will incorrectly set the operator of a new connection to 0, causing all subsequent op.do to fail in an infinite loop.\nSolution Poller no longer executes detach asynchronously to ensure that it does not run concurrently with C.onclose and the changes are performance-neutral.\nAffected Components netpoll-v0.2.2\nkitex-v0.3.0\nCVE None\nReferences  https://github.com/cloudwego/netpoll/issues/149 https://github.com/cloudwego/netpoll/pull/142  ","categories":"","description":"Netpoll Panic","excerpt":"Netpoll Panic","ref":"/security/safety-bulletin/detail/cloudwego-sa-2022-2/","tags":"","title":"CloudWeGo-SA-2022-1"},{"body":"简介 修复当“对端关闭 \u0026 本端 user Close” 同时发生时出现 Panic 的问题\n严重级别 Middle\n描述 修复极端场景下的并发问题：\n场景：“对端关闭 \u0026 本端 user Close” 同时发生，并且 c.onClose 执行的是 lock(User)，同时 poller 也执行到了 p.detaches (三个条件同时满足)\n这时候会有两个 goroutine 同时执行 op.Control(PollDetach)，因此 op.unused 会被执行两次。\n当程序比较闲时，有可能 c.onClose 已经执行完 close callback 了，freeop 并且已经被复用，又变成 inuse 状态；而这时候 p.detaches 里的 op.unused 才开始执行；这样就会把一个新连接的 operator 错误的置成 0 ，导致后续的 op.do 全失败，变成死循环。\n解决办法 poller 不再异步执行 detach，以保证不会和 c.onClose 并发，改动是性能无损的。\n影响组件 netpoll-v0.2.2\nkitex-v0.3.0\nCVE 无\n参考链接  https://github.com/cloudwego/netpoll/issues/149 https://github.com/cloudwego/netpoll/pull/142  ","categories":"","description":"Netpoll Panic","excerpt":"Netpoll Panic","ref":"/zh/security/safety-bulletin/detail/cloudwego-sa-2022-2/","tags":"","title":"CloudWeGo-SA-2022-2"},{"body":"Feature  [#366, #426 ] feat(client): support warming-up for kitex client [#395 ] feat(mux): support gracefully shutdown in connection multiplexing [#399 ] feat(protobuf): add fastpb protocol API and support it in the pkg/remote/codec module  Optimise  [#402 ] optimize(connpool): export getCommonReporter in pkg/remote/connpool [#389 ] optimize(rpcinfo): fill TransportProtocol, PackageName fields into RPCInfo of the server side after decoding  Bugfix  [#413 ] fix(mux): set PayloadCodec for sendMsg in NetpollMux trans handler to fix generic request codec error.issue #411 [#406 ] fix(grpc): fix the sending and receiving logic about http2 framer, such as preventing the peer unable to receive the framer in time [#398 ] fix(utils): fix the bug that Dump() func in ring.go can’t dump the accurate data in ring shards [#428 ] fix(trans): close connection when flush data fails to avoid memory leak  Tool  [#340 ] tool(protobuf): redesign and implement new protobuf gen code called fastpb which doesn’t use reflection and only supports proto3  Chore  [#396 ] chore: replace cespare/xxhash with xxhash3 from bytedance/gopkg [#400 ] chore: upgrade go version of workflow to 1.18 [#407 ] chore: add a separate file to declare the use of gRPC source code  Test  [#401 ] test: add ut for kitex/server package [#393 ] test: add ut for pkg/remote/bound package [#403 ] test: add netpollmux unit test [#401 ] test: add klog unit test [#392 ] test(utils): add UT for utils and fix inaccurate err throw in json.go [#373, #432, #434 ] test(grpc): add gRPC transport unit tests, promoting the coverage to 76% [#424 ] test(transmeta): supplementary of unit tests for http2 and ttheader implementations of MetaHandler/StreamingMetaHandler in pkg/transmeta.  Dependency Change  github.com/cloudwego/netpoll: v0.2.0 -\u003e v0.2.2 github.com/bytedance/gopkg: 20210910103821-e4efae9c17c3 -\u003e 20220413063733-65bf48ffb3a7  ","categories":"","description":"","excerpt":"Feature  [#366, #426 ] feat(client): support warming-up for kitex …","ref":"/blog/2022/04/29/kitex-release-v0.3.0/","tags":"","title":"Kitex Release v0.3.0"},{"body":"Feature  [#366, #426 ] 功能(client): 客户端支持预热操作 [#395 ] 功能(mux): 连接多路复用支持优雅关闭 [#399 ] 功能(protobuf): 定义 fastpb protocol API 并在编解码模块对应支持  Optimise  [#402 ] 优化(connpool): 导出 pkg/remote/connpool 里的 getCommonReporter [#389 ] 优化(rpcinfo)：填充由 defaultCodec 解码得到的 rpcinfo 中缺失的 Invocation().PackageName, Invocation().ServiceName and Config().TransportProtocol 字段  Bugfix  [#413 ] 修复(mux): 在 NetpollMux transHandler 中设置 sendMsg的PayloadCodec，以修复泛化请求编码报错问题issue #411 [#406 ] 修复(grpc): 修复 http2 framer 的读写逻辑，例如避免对端无法及时收到 framer [#398 ] 修复(utils)：修复了 Dump() 接口无法 dump 出 ring 里所有数据的 bug [#428 ] 修复(trans)：当写入失败时，关闭连接以避免内存泄漏  Tool  [#340 ] tool(protobuf): 重新设计并实现 Protobuf 生成代码，不使用反射完成编解码，当前仅支持 proto3  Chore  [#396 ] chore: 用 bytedance/gopkg 里的 xxhash3 替换掉 cespare/xxhash [#400 ] chore: 升级 workflow 的 go 版本到 1.18 [#407 ] chore: 单独增加文件对 grpc 源码使用做声明  Test  [#401 ] test: 补充 kitex/server 的单测 [#393 ] test: 补充 pkg/remote/bound package 单测 [#403 ] test: 补充 netpollmux package 单测 [#401 ] test: 补充 klog package 单测 [#392 ] test: 补充 utils package 单测 [#373, #432, #434 ] test: 补充 gRPC transport 部分的单测，单测覆盖率提升到 76% [#424 ] test: 补充 transmeta 实现 handler 的单元测试  Dependency Change  github.com/cloudwego/netpoll: v0.2.0 -\u003e v0.2.2 github.com/bytedance/gopkg: 20210910103821-e4efae9c17c3 -\u003e 20220413063733-65bf48ffb3a7  ","categories":"","description":"","excerpt":"Feature  [#366, #426 ] 功能(client): 客户端支持预热操作 [#395 ] 功能(mux): 连接多路复用支持 …","ref":"/zh/blog/2022/04/29/kitex-v0.3.0-%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/","tags":"","title":"Kitex v0.3.0 版本发布"},{"body":"Improvement  Fix: reduce costs of SetNumLoops Chore: update mcache and ci Feat: recycle caches when LinkBuffer is closed  Fix  Fix: send\u0026close ignored by OnRequest Fix: fill lost some data when read io.EOF Fix: check is active when flush  Doc  Doc: update guide.md Doc: restate the definition of Reader.Slice Doc: fix replace examples url  Revert  Revert “feat: change default number of loops policy (#31)”  ","categories":"","description":"","excerpt":"Improvement  Fix: reduce costs of SetNumLoops Chore: update mcache and …","ref":"/blog/2022/04/28/netpoll-release-v0.2.2/","tags":"","title":"Netpoll Release v0.2.2"},{"body":"Improvement  Fix: Loops 缩容不再全部重置 Chore: mcache bsr 计算使用 math/bits.Len 代替，以提升性能。 Feat: 修复 LinkBuffer Close 时没有回收 caches 的问题（不是内存泄漏）  Fix  Fix: 修复短链接 send\u0026close 场景无法触发 OnRequest 回调的问题 Fix: 修复 zcReader 读到 io.EOF 后丢失部分数据的问题 Fix: 修复 flush 没有检查连接关闭的问题  Doc  Doc: 更新了用户文档 Doc: 增加了 Reader.Slice 的定义描述 Doc: 修复了 examples 中的死链  Revert  Revert: 重置了 loops 初始化数量  ","categories":"","description":"","excerpt":"Improvement  Fix: Loops 缩容不再全部重置 Chore: mcache bsr 计算使用 math/bits.Len  …","ref":"/zh/blog/2022/04/28/netpoll-v0.2.2-%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/","tags":"","title":"Netpoll v0.2.2 版本发布"},{"body":" 导语：2022 年 3 月，NextArch 基金会正式成立微服务技术小组，致力于推动微服务技术和开源生态的持续发展，根据各个企业在微服务生产实践中遇到的问题，针对不同行业和应用场景输出标准化解决方案，并且联合 PolarisMesh、TARS、go-zero、GoFrame、CloudWeGo 和 Spring Cloud Tencent 等开源社区提供开箱即用的实现，降低终端用户的使用门槛。来自腾讯、字节跳动、七牛云、快手、BIGO、好未来和蓝色光标等多家企业的技术专家已经加入技术小组，欢迎更多企业和开源社区加入。\n 2021 年 11 月，Linux 基金会正式成立 NextArch 基金会，共计 40 余家企业或单位联合参与了该基金会的筹建工作，并作为首批共建和支持单位加入，目前已增至 53 家企业。NextArch 基金会致力于在异构基础设施、多元化技术栈和混合云场景下的构建下一代技术架构，始终秉承一个开放中立的治理模式，发展适合企业数字化转型的开源生态。\n微服务是下一代架构的关键部分，越来越多企业采用微服务架构。市场调研表明，随着企业数字化转型持续深入，2023 年微服务云市场的规模达到 18.8 亿美元，从 2018 到 2023 年的复合年增长率达到 22.4%。众所周知，微服务相比于传统架构具有诸多优势，但是，我们在微服务实施的各个环节中都可能面临问题。\n为了降低微服务架构的落地成本，来自腾讯、快手、字节跳动、好未来、七牛云和蓝色光标等多家企业的技术专家在 NextArch 基金会成立微服务技术小组，共同探讨各自企业在微服务领域中遇到的问题，分享大家在生产过程中的实践经验，并且面向不同的应用场景和终端用户，联合相关开源社区输出标准化的解决方案。\n在采用微服务架构之前，我们需要思考为什么采用微服务架构，并不是所有的开发团队和发展阶段都适合采用微服务架构。通常，采用微服务架构可以解决以下问题：首先，开发团队具有一定的规模，所有成员共同开发一个单体应用的内耗太高，如果采用微服务架构，每个服务可以由单个或者少数成员独立负责。第二，业务系统的功能模块很多，耦合在一起会增加测试和部署的成本，任何一个模块故障也会导致整个系统故障。第三，功能模块之间的负载无法隔离，容易互相影响，没有办法针对热点模块的计算层或者存储层进行扩容。\n如果我们采用微服务架构，单个服务是⾮常简单的，但是，分布式服务之间的功能调用远⽐单体应用内部更加复杂。在单体应用中，⼀个函数可以调⽤其他任何一个公共函数。在微服务架构中，一个函数只可以调⽤同⼀个微服务的函数。如何实现分布式服务之间的通信是微服务架构的首要问题，构建高性能、高可用的远程调用能力并不容易。值得庆幸的是，已经有 grpc、thrift、tars、go-zero、GoFrame、cloudwego/kitex 和 spring cloud 等大量开源的分布式服务开发框架，这些框架可以帮助终端用户快速地构建微服务。不幸的是，仅仅把服务开发出来并且跑通是不够的，保障大规模服务的稳定运营还需要考虑诸多问题，例如：在分布式架构中如何处理基础设施以及应用层的各种异常、如何实现大规模服务的无损发布和流量调度，如何定位和分析复杂调用链路中出现的问题等。对于中大型企业来说，还存在异构的开发技术栈和运行时环境，存在跨地域和混合云的架构要求，如何在更加复杂的应用场景中解决上述问题，面临更多的挑战。\n目前，这个方向还没有开箱即用的解决方案，终端用户必须在不同的基础设施和适当的工具之间做出抉择，才能解决各种问题。近日，NextArch 微服务技术小组向基金会提交了首个提案，根据各自企业在分布式或者微服务生产实践中的经验和痛点，面向多语言、多框架和异构基础设施，针对不同行业和应用场景输出微服务落地的标准化方案，并且依托相关开源社区提供推荐实现，方便终端用户落地。我们也期待更多企业和开源社区加入 NextArch 基金会，共同探讨分布式或者微服务治理的标准化方案。\n部分 NextArch Microservice SIG 成员引文： PolarisMesh 单家骏\n腾讯云专家工程师，具备 10 年以上中间件研发经验。北极星开源社区（PolarisMesh）联合发起人，负责开源项目的技术规划、代码开发和社区运营等工作。\n自分布式架构发展至今，微服务成为了复杂业务系统的首选模式，在企业得到了充分的生产落地，然而各个微服务框架及工具链，对于微服务治理体系的理解存在差异性，使得业务系统在实现微服务治理上存在较大的成本，同时也不利于微服务技术的沉淀及长期发展。北极星是腾讯自研和开源的微服务治理框架，覆盖了腾讯内部 90% 以上的业务，解决了业务系统因多语言、多框架以及业务差异性所带来的服务治理不一致的问题，在腾讯内部完成了服务发现和治理的标准化。我们期望通过加入 NextArch 基金会这样一个中立组织，可以讨论业界微服务治理的相关实践及解决方案，沉淀出标准化的服务治理体系，促进微服务生态的进一步发展，也期望北极星开源社区可以推动并承载微服务治理标准体系的落地。\ngo-zero 万俊峰\n万俊峰，七牛云技术副总裁，go-zero 开源社区/go-zero 作者。负责 go-zero 框架的规划、代码编写、代码 review、工具链规划、社区建设、开源推广\n微服务在发展了这么多年之后，已经呈现出百花齐放的状态，各种微服务框架和治理能力在很多公司都得到了充分的落地，并带来了巨大的业务价值。但当前的现状也没有形成足够的技术共识和规范，我们需要进一步提炼和抽象微服务的能力，并加以标准化。这样可以更好的沉淀经验，并将各语言的微服务框架提供规划化对接，从而推动微服务技术的进一步发展。同时也期望在 SIG 组织能够更多的讨论微服务落地的各种最佳实践，也期望能够通过 go-zero 开源社区帮助推动共识的微服务治理标准落地。\nGoFrame 郭强\n腾讯高级工程师，GoFrame 开源框架项目发起人及主要贡献者，负责 GoFrame 框架发展规划、社区建设维护、核心代码开发。GoFrame 是一款模块化、高性能、企业级的 Go 基础开发框架。\n微服务是一种架构设计思想，目的是为了有效解决业务复杂度提高带来的项目架构问题。微服务需要解决的不仅是技术问题，也是项目协作问题。在\"微服务化\"过后，项目架构将引入更多的痛点：职责边界界定、服务高效通信、分布事务处理、微服务化治理、服务版本管理、项目迭代协作等等。微服务思想发展至今，这些痛点的解决方案已比较成熟，并且大同小异。NextArch 微服务 SIG 需要做的是在这些方案之上分析共性之处，形成统一化和规范化的解决方案。以帮助企业更快速地实现微服务化，同时，也需要提供一些最佳实践，帮助企业提高在服务化后的项目管理手段。80% 的解决方案抽象，20% 的最佳实践沉淀。\nCloudWeGo 罗广明\n字节跳动微服务架构师，CloudWeGo 开源负责人。CloudWeGo 是一套由字节跳动开源的、可快速构建企业级云原生架构的中间件集合，专注于解决微服务通信与治理的难题，具备高性能、可扩展、高可靠的特点。\n微服务技术发展至今，业界涌现出一大批微服务开发框架、技术和最佳实践。这个多样化是不可避免的，没有一个微服务开发框架能够统一所有的语言，但是微服务架构里面所涉及的服务治理体系，却可以做到统一和规范化。NextArch 微服务 SIG 正是在这样的背景下诞生了，旨在提供统一服务治理体系，解决共性问题，将促进微服务框架和技术的进一步演进和发展。\n","categories":"","description":"","excerpt":" 导语：2022 年 3 月，NextArch 基金会正式成立微服务技术小组，致力于推动微服务技术和开源生态的持续发展，根据各个企业在微服务 …","ref":"/zh/blog/2022/04/01/cloudwego-%E5%8A%A9-nextarch-%E5%9F%BA%E9%87%91%E4%BC%9A%E6%8E%A8%E5%8A%A8%E6%A0%87%E5%87%86%E5%8C%96%E5%BB%BA%E8%AE%BE/","tags":"","title":"CloudWeGo 助 NextArch 基金会推动标准化建设"},{"body":"云原生时代，各行各业的基础架构都在经历微服务架构转型，研发效率和稳定性是所有互联网公司需要考虑的问题。开发者想要搭建微服务，离不开配套的微服务治理，如治理平台、监控、链路跟踪、注册/发现、配置中心、服务网格等。随着 Golang 逐渐成为云原生时代的主要编程语言，基于 Golang 的微服务中间件在开源社区有着较强的诉求。\n字节跳动也同样面临这些问题。2014 年，字节跳动引入 Golang 解决长连接推送业务面临的高并发问题，两年后，内部技术团队基于 Golang 推出了一个名为 Kite 的框架，同时对开源项目 Gin 做了一层很薄的封装，推出了 Ginex。字节跳动基础架构/服务框架团队负责人成国柱在 QCon 2021 的分享中表示，这两个原始框架的推出，极大推动了 Golang 在公司内部的应用。\n但是由于关联技术迭代和业务诉求增加，深度耦合的 Kite 和 Thrift ，很难从网络模型或编解码层面改造优化，继续支持新特性势必会造成代码臃肿、迭代受阻问题。2019 年下半年，字节跳动技术团队开始重新设计 Golang RPC 框架，同时为了在网络通信上有更好的性能并能支持连接多路复用、感知连接状态，自研了网络库 Netpoll。\n字节跳动重构 Kite 为 Kitex ，围绕性能和可扩展性设计，并在次年 10 月完成发布，投入到内部应用中。据悉，截至 2021 年 9 月，线上有 3w+ 微服务使用 Kitex，大部分服务迁移新框架后可以收获 CPU 和延迟上的收益。\n“在 Kitex 得到内部广泛使用后，我们决定围绕微服务逐步把我们的实践开源出去，并且对外保持统一。”字节跳动 CloudWeGo 技术专家谈道，“但微服务相关的项目较多，每个项目单独开源对外部用户并不友好，因此我们以 CloudWeGo 作为项目名，逐步将内部整个微服务体系开源，内外统一使用开源库，各项目以开源库为主进行迭代。”\n2021 年 9 月 8 日，字节跳动宣布正式开源 CloudWeGo。\n中间件“工具箱”CloudWeGo CloudWeGo 是一套字节跳动内部微服务中间件集合，具备高性能、强扩展性和稳定性的特点，专注于解决微服务通信与治理的难题，满足不同业务在不同场景的诉求。此外，CloudWeGo 也重视与云原生生态的集成，支持对接 K8s 注册中心、Prometheus 监控以及 OpenTracing 链路追踪等。\n目前，CloudWeGo 第一批开源了四个项目：Kitex、Netpoll、Thriftgo 和 netpoll-http2，以 RPC 框架 Kitex 和网络库 Netpoll 为主。Kitex 内置了部分治理策略以及丰富的扩展接口，便于融入微服务体系中；Netpoll 主要面向对高性能有诉求的场景。\nCloudWeGo 的每一个组件都可以单独使用。“很多人担心 Kitex 是一个很重的框架，其实 Kitex 没有耦合任何其他组件包括 Netpoll，Kitex 内置的一些治理能力，用户也可以选择性集成。Netpoll 作为一个网络库，其他 RPC 框架、HTTP 框架都可以单独接入使用。Thriftgo 是 Thrift IDL 解析和代码生成器，也是独立的工具，并且提供插件机制，用户可定制生成代码。”字节跳动 CloudWeGo 技术专家表示，“我们会继续开源其他内部项目，如 HTTP 框架 Hertz、基于共享内存的 IPC 通信库 ShmIPC 等，提供更多场景的微服务需求支持。”\nCloudWeGo 的优势和局限 微服务中间件和业务紧密联系，是整个业务架构的基础，在进行技术选型时必须慎重。业内公认的选型标准关键在于两方面：\n 能解决实际业务问题和上生产抗流量，且易用性高、可治理、成熟稳定 技术是开源的，且开源项目的 star 数、项目活跃度（Issue\u0026PR）、文档更新频率、发版周期稳定可靠  CloudWeGo 的优势在于，已经在字节跳动经过大规模生产流量验证，有可以参考的稳定性和可靠性实际案例。“CloudWeGo 的特点之一是高性能，但实际上在开发之初它经常遇到性能瓶颈，于是内部专门进行了网络库、Thrift 序列化的专项优化，优化的过程会比较漫长，一个瓶颈点要花很长时间反复测试调整实现，我们也发过两篇文章《字节跳动 Go RPC 框架 Kitex 性能优化实践》和《字节跳动在 Go 网络库上的实践》分享了优化实践。”字节跳动 CloudWeGo 技术专家表示。\n据悉，与同类型项目相比，CloudWeGo 开发团队不仅考虑了高性能、强扩展性，还考虑到了易用性。“以 Kitex 为例，目前从治理功能的多样性上不及一些开源框架，从性能、扩展性、使用体验多维度综合来看，Kitex 具有一定的优势。Kitex 支持多协议，由于内部以 Thrift 为主，Kitex 对 Thrift 支持也做了性能优化，如果使用 Thrift，Kitex 将是最佳的选择。”字节跳动 CloudWeGo 技术专家告诉 InfoQ。\n此外，为了遵守长期投入承诺，内外维护一套代码、统一迭代，字节跳动已经将与内部生态没有耦合的项目直接迁移到 CloudWeGo 开源库，并将内部依赖调整为开源库。而对于需要集成治理能力融入微服务体系的 Kitex，开源团队则对内外部代码做了拆分，把 Kitex 的核心代码迁移到开源库，内部库封装一层壳保证用户无感知升级，而集成内部治理特性的模块则作为 Kitex 的扩展保留在内部库。未来，字节跳动也会持续把已经在内部经过稳定性验证的新特性，迁移到开源库。\n在字节跳动内部，为了便于 Kitex 融入内部的治理体系，Kitex 面向内部提供了 Byted Suite 扩展，集成内部的注册中心、配置中心、监控等，内部 ServiceMesh 已经得到了大规模落地，Kitex 会根据服务的信息判断是否是 ServiceMesh 模式，若是，Kitex 则会卸载治理组件，治理能力下沉至 Mesh 中。为了提高与 ServiceMesh 通信的性能，Kitex 单独扩展 TransHandler 模块集成内部实现的 ShmIPC，与 ServiceMesh 通信走 ShmIPC ，后续 Kitex 对 ShmIPC 的扩展以及 ShmIPC 库也会开源出来。\n不过 CloudWeGo 依然有自己的局限性。字节跳动 CloudWeGo 技术专家告诉 InfoQ：CloudWeGo 功能的丰富度和多样性还不够，还需要进一步完善，字节跳动技术团队会收集外部用户的需求，评估排期支持，期待更多的开发者加入。目前 Kitex Server 性能优势明显，但 Client 相比 Server 性能表现不佳，后续会重点对 Client 进行优化。此外，基于不同的语言框架，默认场景必须能兼容互通而非性能最佳。“刚开源时得到大家的关注，看到一些压测对比显示 Kitex 性能表现一般，主要是压测场景未对齐，后续我们也会考虑面向开源尽量提供性能较优的策略。”\n“开源”不是为了“完成 KPI ” 目前，CloudWeGo 在社区中也比较有活力。据悉，在未被正式宣布开源前，一个月内 Kitex 收获了 1.2k stars，Netpoll 收获了 700+ stars。9 月 8 日，字节跳动正式宣布开源 CloudWeGo 后，截至 10 月初，项目整体 star 数已经超过 4800，且已被收录进 CNCF landscape。\n字节跳动 CloudWeGo 技术专家表示：“我们收到了来自社区的大量反馈，如很多用户对 Protobuf 的诉求较为强烈，我们已经针对这个问题，计划开展 Kitex 对 Protobuf 支持的性能优化。欢迎大家向 CloudWeGo 提交 issue 和 PR，共建 CloudWeGo。我们也为企业和组织使用 Kitex 和 Netpoll 设置了专项支持，希望 CloudWeGo 将来能真正成为通用的、可落地的微服务通信与治理开源方案。”\n关于开源，字节跳动 CloudWeGo 技术专家的观点旗帜鲜明：“完成 KPI 不是这个项目开源的目的。健康的开源模式注重开放共享，共同成长和长期主义。CloudWeGo 认同个体参与、社区价值以及开源共同体带来的归属感。”\n“字节跳动作为开源项目的受益者、参与者，也希望成为开源项目的推动者、主导者，将内部优秀的最佳实践反馈给开源社区，与社区共同建设、丰富基础架构领域开源生态，为广大开发者和企业在技术选型时提供更多更优的选择。”字节跳动 CloudWeGo 技术专家谈道，“我们拥抱开源的文化，倾听社区的反馈，积极响应用户的需求，并且提供友好的中英文文档和快速开发 guideline，为社区开发者快速深入了解 CloudWeGo 以及参与贡献提供便利与支持。”\n项目地址：https://github.com/cloudwego\n受访嘉宾: 字节跳动 CloudWeGo 技术专家罗广明、杨芮、马子昂\n原文链接: https://www.infoq.cn/article/9ixlu4kjapg3ufhymm3j\n","categories":"","description":"","excerpt":"云原生时代，各行各业的基础架构都在经历微服务架构转型，研发效率和稳定性是所有互联网公司需要考虑的问题。开发者想要搭建微服务，离不开配套的微服 …","ref":"/zh/blog/2022/03/28/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%B8%AD%E9%97%B4%E4%BB%B6-cloudwego/","tags":"","title":"一文了解字节跳动微服务中间件 CloudWeGo"},{"body":"In the cloud native era, infrastructures across all industries are undergoing a microservice architecture transformation, and all internet companies are concerned about R\u0026D efficiency and stability. Developers who want to build microservices can never forgo supporting microservice governance, such as governance platforms, monitoring, tracing, registration/discovery, configuration centers, service mesh, etc. With Golang gradually becoming the dominating programming language in the cloud native era, there is a strong demand for Golang-based microservices middleware in the open source community.\nByteDance also faces these problems. In 2014, ByteDance introduced Golang to solve the high concurrency problem faced by push services with persistent connections; and two years later, the technical team launched a framework called Kite based on Golang, and rolled out Ginex after lightly encapsulating the open-source project Gin. In QCon 2021, Guozhu Cheng, the head of ByteDance’s infrastructure/service framework team, said that the launch of these two original frameworks has greatly promoted Golang’s adoption within the company.\nHowever, due to evolutions of related technologies and demanding business requirements, the deeply coupled Kite and Thrift are difficult to be transformed and improved on the network model or codec level, and therefore continuous feature delivery will inevitably cause code bloat and blocked iterations. In the second half of 2019, the ByteDance technical team began to redesign the Golang RPC framework. At the same time, in order to get better performance in network communication, support connection multiplexing and sense connection status, they developed their own network library Netpoll.\nByteDance refactored Kite as Kitex, designed revolving around performance and scalability, and released it in October 2020 for internal use. As of September 2021, there are 30, 000+ online microservices using Kitex, most of which can benefit from boosted CPU and alleviated latency after using the new framework.\n“After Kitex became widely used within ByteDance, we decided to gradually entail our practice open-source revolving around microservices and keep it in line with the outside.” ByteDance CloudWeGo technical experts said, “But there are many microservice-related projects, and each project is open-source alone, which is not friendly to external users. So we named the project as CloudWeGo and gradually enabled the entire internal microservice system to be open-source, using open-source libraries internally and externally. Each project iterates mainly with open-source libraries.”\nOn September 8, 2021, ByteDance officially announced the open source CloudWeGo.\nCloudWeGo CloudWeGo is a set of microservice middleware developed by ByteDance with high performance, strong scalability and stability. It focuses on microservice communication and governance, and meets the demands of different businesses in various scenarios. CloudWeGo also focuses on integration with the cloud native ecology, supporting K8s registry, Prometheus, and OpenTracing.\nCloudWeGo currently has 4 repositories: Kitex, Netpoll, Thriftgo and netpoll-http2, featuring the RPC framework – Kitex and the network library – Netpoll. Kitex is equipped with built-in governance strategies and expansion interfaces for frictionless integrations into the microservice system. Netpoll is aimed at scenarios where demand high performance.\nEach component of CloudWeGo can be used separately. “Many people worry that Kitex would be a heavy-weight framework. In fact, Kitex does not couple any other component including Netpoll. Users can also optionally integrate some of Kitex’s built-in governance functions. Netpoll is a network library that can work separately with other RPC frameworks and HTTP frameworks. Thriftgo is a Thrift IDL parser and code generator. It is also a stand-alone tool and provides a customizable, plug-in mechanism for the code generation.” ByteDance CloudWeGo technical experts said, “We will continue to move all other internal projects to the open-source track, such as HTTP framework Hertz, shared memory-based IPC communication library ShmIPC, etc., to provide microservices support for wider scenarios.”\nAdvantages\u0026disadvantages The close connection between microservice middleware and business is the foundation of the entire business architecture; so the selection of technology requires special care. Our selection criteria mainly depend on two aspects：\n It can address practical business problems and is ready for production with massive traffic, and is easy to use, governable, mature and stable. The technology is open-source; and the number of Stars, project activity (Issues \u0026 PRs), document update frequency, and release cycle of the open-source project are stable and reliable.  The advantage of CloudWeGo is that it has already been tested with massive traffic amid the real production deployment in ByteDance. Providing a practical example that can be referred to attest for its stability and reliability. “One of the characteristics of CloudWeGo is high performance, but at the beginning of the development, we often confront performance bottlenecks. So we improved network library and Thrift serialization specifically. The optimization process was prolonged, with a bottleneck taking a long time to test and fine-tune repetitively. We have also published two articles “ByteDance Go RPC Framework Kitex Performance Optimization Practice” and “ByteDance on the Go Network Library Practice” to share our optimization practices.” ByteDance CloudWeGo technical experts said.\nCompared to similar projects, the CloudWeGo R\u0026D team considered not only its performance and strong scalability, but also ease of use. “Taking Kitex as an example, it is currently inferior to some open-source frameworks in terms of the diversity of governance functions. But from the perspective of performance, scalability, and user experience, Kitex showcases the following advantages. Kitex supports a variety of protocols, because it mainly applies Thrift. Kitex has also made performance improvements for Thrift support. If using Thrift, Kitex will be the best choice.” ByteDance CloudWeGo technical experts remark on the benefits of using CloudWeGo.\nIn addition, in order to uphold a key principle of maintaining one set of code internally and externally, iterating them as a whole, ByteDance has directly migrated projects without coupling the internal ecology to the CloudWeGo open-source library, and adapt the internal dependency for the open-source library. For Kitex, which requires integrated governance functions into the microservice system, the open-source team split the internal and external code, migrating Kitex’s core code to the open source library. The internal library encapsulates a shell to ensure that updates are transparent to users. And the modules that integrate internal governance features are retained in the internal library as extensions of Kitex. In the future, ByteDance will continue to migrate new features that have been internally validated for stability to open-source libraries.\nInside ByteDance, in order to facilitate Kitex’s integration into the internal governance system, Kitex provides a Byted Suite extension, integrating the internal registry, configuration center, monitoring, etc. Internal Service Mesh has been implemented on a large scale. Kitex determines whether it is a Service Mesh mode based on the information of the service, if so, Kitex will uninstall the governance components, and the governance functions will sink into Service Mesh. As an attempt to speed up the performance of communication with Service Mesh, Kitex separately extends the TransHandler module to integrate the self-developed ShmIPC, and communicates with Service Mesh through ShmIPC. Subsequently, Kitex’s extension to ShmIPC and the ShmIPC library will also be open-source.\nHowever, CloudWeGo has its own limitations. ByteDance CloudWeGo technical experts told InfoQ: The richness and diversity of CloudWeGo functions are not enough, pending further improvement. ByteDance’s technical team will solicit the needs of external users, provide support, and welcome more developers to contribute. At present, the performance advantages of Kitex Server are obvious, but the performance of the Client side with Server, and we will focus on improving the Client in the future. The primary goal is to make default scenarios compatible with each other, with negligible performance overhead. “The launch of the open source has attracted public attention, and we observed some stress test comparisons showing that Kitex performance was mediocre, mainly because the stress test scenario was not aligned. We will consider providing better performance strategies for the open-source community.”\nOpen Source is Not About Completing KPIs At present, CloudWeGo is dynamic in the community. Before the official announcement of open source, Kitex gained 1.2k stars and Netpoll gathered 700+ stars within one month. After ByteDance officially announced the open source CloudWeGo on September 8, as of early October, the overall number of stars in the project has exceeded 4,800 and has been included in the CNCF landscape. The overall star number of the project has exceeded 4800, and it has been included in the CNCF landscape.\nByteDance CloudWeGo technical experts said, “We have received a lot of feedback from the community. For example, many users call for Protobuf. In response to this feedback, we plan to implement Kitex performance optimizations for Protobuf support. We welcome you to submit issues and PRs to CloudWeGo. We’ve also set up customized support for enterprises and organizations to use Kitex and Netpoll, and hope that CloudWeGo will truly become a universal, available open-source solution to microservices communication and governance in the future.”\nRegarding “open source”, ByteDance CloudWeGo technology experts have a clear vision: “Completing KPIs is not the purpose of this open source project.” A healthy open-source model focuses on open sharing, co-growth, and long-termism. CloudWeGo recognizes individual participation, community values, and the sense of belonging brought by open source community.\"\n“As a beneficiary and participant of the open source project, ByteDance also hopes to become a promoter and leader of the open source project. It hopes to gift excellent internal practices to the open source community, build and enrich the open source ecosystem in the infrastructure field together with the community, and provide wider and better choices for developers and enterprises for their technology selection.” ByteDance CloudWeGo technology experts said, “We embrace the open source culture, listen to community feedback, actively meet user’s needs, provide Chinese and English documentations, and develop guidelines quickly, to facilitate and support community developers to understand CloudWeGo and participate in contributions.”\nProject address: https://github.com/cloudwego\nInterviewees: ByteDance CloudWeGo technical experts (Guangming Luo, Rui Yang, Ziang Ma).\nReference link: https://www.infoq.cn/article/9ixlu4kjapg3ufhymm3j\n","categories":"","description":"","excerpt":"In the cloud native era, infrastructures across all industries are …","ref":"/blog/2022/03/25/an-article-to-learn-about-bytedance-microservices-middleware-cloudwego/","tags":"","title":"An Article to Learn About ByteDance Microservices Middleware CloudWeGo"},{"body":"Bugfix  [#383 ] fix(generic): detect circular dependency in thrift IDL when using generic call. [#359 ] fix(tool): fix streaming import missing in protobuf combine service. [#363 ] fix(client): fix a bug that sequence ID of oneway requests are not encoded and lower the loss rate of oneway requests. [#367 ] fix(generic/tool): combine services may have duplicate loading of the same service.  Optimise  [#362 ] optimize(diagnosis): lbcache is global, it doesn’t need register ProbeFunc for diagnosis. [#374 ] optimize(rpcinfo): RPCInfo.To().Tag() use instance tag instead of remoteinfo tag firstly. [#355 ] optimize(connpool): adjust minMaxIdleTimeout to 2s. [#354 ] optimize(hook): adding locks to onServerStart and onShutdown, acquire the corresponding lock when doing some read and write operations like RegisterStartHook and range in server.Run(). [#331 ] optimize(discovery): add error definition ErrInstanceNotFound which is used in the service discovery module.  Refactor  [#352 ] refactor(event): delete additional atomic operations and replace them with a normal operation. [#343 ] refactor(loadbalancer): merge BuildWeightedVirtualNodes function into buildVirtualNodes function, make it easier to maintain.  Chore  [#376 ] chore: upgrade choleraehyq/pid for Go 1.18.  Docs  [#364 ] docs: update readme with new blog.  ","categories":"","description":"","excerpt":"Bugfix  [#383 ] fix(generic): detect circular dependency in thrift IDL …","ref":"/blog/2022/03/24/kitex-release-v0.2.1/","tags":"","title":"Kitex Release v0.2.1"},{"body":"Bugfix  [#383 ] 修复(generic)：在泛化调用的时候检查 IDL 是否有循环依赖。 [#359 ] 修复(tool)：修复 protobuf CombineService 缺失 streaming 引用的问题。 [#363 ] 修复(client)：修复 oneway 请求的 sequence ID 没有被编码的问题以及降低 oneway 调用的丢包率。 [#367 ] 修复(generic/tool)：修复 CombineServices 可能存在多次加载同一个 service 问题。  Optimise  [#362 ] 优化(diagnosis)：lbcaches 是全局的，无需为每个 client 注册 ProbeFunc 用于诊断查询。 [#374 ] 优化(rpcinfo)：RPCInfo.To().Tag() 优先使用服务发现的 instance tag 而不是 remoteinfo tag。 [#355 ] 优化(连接池)：修改默认的连接池最小空闲等待时间为 2s。 [#354 ] 优化(hook)：为 onServerStart和 onShutdown添加资源锁，当做一些如RegisterStartHook和 server.Run中的 range之类的读写操作时请求对应的资源锁。 [#331 ] 优化(discovery)：增加「实例不存在」错误定义。  Refactor  [#352 ] 重构(event)：删除额外的原子操作并用普通赋值操作替换。 [#343 ] 重构(loadbalancer)：将 buildWeightedVirtualNodes 函数合入 buildVirtualNodes 函数中，成为一个函数。  Chore  [#376 ] 升级依赖 choleraehyq/pid 以兼容Go 1.18。  Docs  [#364 ] 更新 README 到新博客的链接。  ","categories":"","description":"","excerpt":"Bugfix  [#383 ] 修复(generic)：在泛化调用的时候检查 IDL 是否有循环依赖。 [#359 ] 修复(tool)：修 …","ref":"/zh/blog/2022/03/24/kitex-v0.2.1-%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/","tags":"","title":"Kitex v0.2.1 版本发布"},{"body":"Feature  Feat(grpc): support options to set the internal params of gRPC Feat(kerror): add new func WithCauseAndExtraMsg for basicError Feat(rpcinfo): add FreezeRPCInfo to support asynchronous context usage Feat(codec): default codec supports size limit  Bugfix  Fix(remotecli): fix bug that released connections may be reused Fix(generic): generic call supports extended services Fix(generic): fix generic call oneway flag  Optimise  Optimize(retry): improve retry success rate when do failure retry  Chore  Chore: upgrade netpoll to v0.2.0 Chore:add third party license  ","categories":"","description":"","excerpt":"Feature  Feat(grpc): support options to set the internal params of …","ref":"/blog/2022/02/24/kitex-release-v0.2.0/","tags":"","title":"Kitex Release v0.2.0"},{"body":"Feature  Feat(grpc): gRPC 相关配置支持通过 options 来设置，并且为了兼容旧版本默认窗口大小调整为 64K Feat(kerror): 为 basicError 添加新的 error 封装 func WithCauseAndExtraMsg Feat(rpcinfo): 添加 FreezeRPCInfo 以支持异步 context 使用 Feat(codec): 默认编解码器支持限定包体积大小  Bugfix  Fix(remotecli): 修复重置的连接可能被复用的问题 Fix(generic): 修复泛化调用的客户端不能使用继承的 service 的方法的问题 Fix(generic): 修复泛化调用 client 侧判断 Oneway 不准确的问题  Optimise  Optimize(retry): 提高异常重试的重试成功率   如果超时的请求先于重试的请求返回，可能会导致重试请求也失败；同时也可以避免超时请求不必要的解码处理。\n Chore  Chore: 升级 netpoll 的版本至 v0.2.0 Chore: 添加第三方库的license  ","categories":"","description":"","excerpt":"Feature  Feat(grpc): gRPC 相关配置支持通过 options 来设置，并且为了兼容旧版本默认窗口大小调整为 64K …","ref":"/zh/blog/2022/02/24/kitex-v0.2.0-%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/","tags":"","title":"Kitex v0.2.0 版本发布"},{"body":"Improvement  Feat: on connect callback Feat: new conn api - Until Feat: support dialing without timeout  Fix  Fix: trigger close callback if only set the onConnect callback Fix: add max node size to prevent OOM Fix: FDOperator.reset() not reset op.OnWrite Fix: Write panic when conn Close Fix: unit tests may fail  Chore  docs: update readme  ","categories":"","description":"","excerpt":"Improvement  Feat: on connect callback Feat: new conn api - Until …","ref":"/blog/2022/02/22/netpoll-release-v0.2.0/","tags":"","title":"Netpoll Release v0.2.0"},{"body":"Improvement  Feat: 添加 OnConnect 回调 Feat: 新增 Until API Feat: 支持不带 timeout 的 dial  Fix  Fix: 修复当只设置了 onConnect 回调时，不会触发 close callback 的 bug Fix: 添加最大节点限制，避免异常情况下的 OOM 问题 Fix: 修复 reset operator 时，没有 reset OnWrite 的问题 Fix: 修复连接关闭时，写 panic 的问题 Fix: 修复单测失败问题  Chore  docs: 更新 readme  ","categories":"","description":"","excerpt":"Improvement  Feat: 添加 OnConnect 回调 Feat: 新增 Until API Feat: …","ref":"/zh/blog/2022/02/22/netpoll-v0.2.0-%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/","tags":"","title":"Netpoll v0.2.0 版本发布"},{"body":"Improvement  Optimize(log): don’t print timeout log in rpctimeout middleware Optimize(log): adjust default log level to info Optimize(gRPC): lock the sendAt avoid grpc bdp data race  Bugfix  Fix(client-connection): fix a connection leaking bug that happens when clients fail at Send Fix(timeout): fix TimeoutAdjust won’t work when set in middleware builder  Tool  Fix(tool): fix protobuf handler arguments name   kitex will generate a stream type named “{{.ServiceName}}{{.Name}}Server” for each streaming server, but in handler.go kitex use “{{.ServiceName}}{{.RawName}}Server” as stream name.\n Chore  Style: remove unnecessary type conversions  ","categories":"","description":"","excerpt":"Improvement  Optimize(log): don’t print timeout log in rpctimeout …","ref":"/blog/2022/01/18/kitex-release-v0.1.4/","tags":"","title":"Kitex Release v0.1.4"},{"body":"功能优化  在 rpctimeout 的 middleware 的输出日志中过滤掉超时日志 调整默认日志级别为 Info 给 sentAt 变量加锁，避免单测出现 DATA RACE，实际上不会有并发问题  Bug 修复  修复客户端编码失败时连接会泄漏的问题 修复 middleware builder 中设置 TimeoutAdjust 不生效的问题  工具  修复 protobuf 的 handler 参数名   kitex 会给每个 stream server 生成一个名为 “{{.ServiceName}}{{.Name}}Server” 的 stream 类型， 但是在 handler.go 中使用的是 “{{.ServiceName}}{{.RawName}}Server\n Chore  删除不必要的类型转换  ","categories":"","description":"","excerpt":"功能优化  在 rpctimeout 的 middleware 的输出日志中过滤掉超时日志 调整默认日志级别为 Info 给 sentAt  …","ref":"/zh/blog/2022/01/18/kitex-v0.1.4-%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/","tags":"","title":"Kitex v0.1.4 版本发布"},{"body":"Overview Fixed connection leak when client encoding failed\nSeverity Level Low\nDescription Fixed connection leak when client encoding failed\nSolution When client encoding fails, the connection is released\nAffected Components kitex-v0.1.3\nCVE None\nReferences  https://github.com/cloudwego/kitex/pull/315  ","categories":"","description":"Connection Leaking","excerpt":"Connection Leaking","ref":"/security/safety-bulletin/detail/cloudwego-sa-2022-1/","tags":"","title":"CloudWeGo-SA-2022-1"},{"body":"简介 修复客户端编码失败时连接会泄漏的问题\n严重级别 Low\n描述 修复客户端编码失败时连接会泄漏的问题\n解决办法 当客户端编码失败时，释放连接\n影响组件 kitex-v0.1.3\nCVE 无\n参考链接  https://github.com/cloudwego/kitex/pull/315  ","categories":"","description":"Connection Leaking","excerpt":"Connection Leaking","ref":"/zh/security/safety-bulletin/detail/cloudwego-sa-2022-1/","tags":"","title":"CloudWeGo-SA-2022-1"},{"body":"Feature  Transmit the Base from client to server for getting the caller info in JSON generic  Bugfix  Fix(grpc): fix metric missing method tag in streaming Fix(generic): fix the incompatible modification about base64 binary in the JSON and HTTP generic Fix(grpc): fix the bug of grpc flow control, which brings the problem of continuous timeout  CI  Add scenario tests  Chore  update the ROADMAP  ","categories":"","description":"","excerpt":"Feature  Transmit the Base from client to server for getting the …","ref":"/blog/2021/12/30/kitex-release-v0.1.3/","tags":"","title":"Kitex Release v0.1.3"},{"body":"功能优化  JSON 泛化调用场景，向服务端传递 Base 信息，从而服务端可获取 Caller 等信息  Bug 修复  修复 streaming 的 metric 上报（server侧）丢失 method 信息的问题 修复 JSON 和 HTTP 泛化中 base64 和 binary 的不兼容改动 修复 gRPC 流控相关的问题，该问题会导致 client 侧出现持续超时  CI  增加场景测试  Chore  更新了 ROADMAP  ","categories":"","description":"","excerpt":"功能优化  JSON 泛化调用场景，向服务端传递 Base 信息，从而服务端可获取 Caller 等信息  Bug …","ref":"/zh/blog/2021/12/30/kitex-v0.1.3-%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/","tags":"","title":"Kitex v0.1.3 版本发布"},{"body":"Hotfix  Fix some gRPC request bugs which are involved by v0.1.0 Fix mistake gRPC method path when no package definition in IDL  Dependency Change  Chore: upgrade netpoll-http2 to fix the problem about large request package (\u003e4K) in streaming  Chore  Chore: use GitHub’s PULL_REQUEST_TEMPLATE to create a PR  ","categories":"","description":"","excerpt":"Hotfix  Fix some gRPC request bugs which are involved by v0.1.0 Fix …","ref":"/blog/2021/12/22/kitex-release-v0.1.2/","tags":"","title":"Kitex Release v0.1.2"},{"body":"Hotfix  修复 v0.1.0 gRPC 请求优化引入的部分问题 修复 IDL 中未定义 package 时，gRPC 的方法信息错误问题  依赖更新  更新 netpoll-http2 依赖，解决 streaming 场景下大包（\u003e4K）请求报错的问题  杂项  使用 GitHub 的 PR 模板，强制开发者提交 PR 时填写相关描述  ","categories":"","description":"","excerpt":"Hotfix  修复 v0.1.0 gRPC 请求优化引入的部分问题 修复 IDL 中未定义 package 时，gRPC 的方法信息错误问 …","ref":"/zh/blog/2021/12/22/kitex-v0.1.2-%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/","tags":"","title":"Kitex v0.1.2 版本发布"},{"body":"Feature Generic Call  Support combined services Export SetSeqID and add GetSeqID for binary generic call of server side Support close generic client to avoid memory leak  Log  Use key=value style in log messages Use klog as global log in some logs Use the global default logger across kitex Print detail loginfo by ctx Pass service info to go func which is used to output for troubleshooting  Option  Add NewThriftCodecDisableFastMode to disable FastWrite/Read Add server option - WithReusePort Default rpc timeout = 0  Proxy  Proxy add ContextHandler interface to support passing initialization context to mwBuilder Register Dump in lbcache to diagnosis Pass RPCConfig to proxy.Config  Improvement  Reduce heap allocation Optimize mux performance Recycle grpc codec buffer by close linkbuffer Distinguish ErrRPCFinish in cost info of backup request Move mux.ShardQueue to netpoll, rename sharedMap to shardMap Add container length encoding guard in fast api  Bugfix  Enable server error handle middleware Adjust Balancer initialization in lbcache Init TraceCtl when it is nil (only affect unit test) Set default rpctimeout and disable timeout logic if rpctimeout == 0 Defaultlogger wrong calldepth Rename BackwardProxy to ReverseProxy Avoid nil panic in grpc keepalive Fix hidden dangers about grpc Fix exception missing in void method Fix mistake dump info when instances change.  Docs  Fix link in readme_zh Remove docs; maintain cloudwego.io only  Netpoll API Change  Adapt netpoll.Writer.Append API  Dependency Change  github.com/cloudwego/netpoll: v0.0.4 -\u003e v0.1.2  ","categories":"","description":"","excerpt":"Feature Generic Call  Support combined services Export SetSeqID and …","ref":"/blog/2021/12/13/kitex-release-v0.1.0/","tags":"","title":"Kitex Release v0.1.0"},{"body":"功能 泛化调用  IDL 解析支持多 Service 暴露 SetSeqID 方法便于二进制泛化场景 server 侧使用 泛化 client 支持关闭，规避内存泄漏问题  日志  修改日志风格，使用 “key=value” 列出信息 使用 klog 作为全局的日志输出工具 使用全局的 default logger 日志打印更多 context 信息，例如 logId，方便问题排查 go func 传入服务信息用于 recover panic 后输出关键信息方便问题排查  Option  增加 NewThriftCodecDisableFastMode 方法，来关闭 FastWrite 和 FastRead Kitex server 支持端口复用 默认 RPC 超时设置为 0（在后续 PR 中，revert 了该变更）  Proxy  Proxy 增加 ContextHandler 接口用于传递初始化ctx给 mwbuilder 注册 lbcache 的 Dump 给 diagnosis，用于问题诊断 将 PRCConfig 传递给 proxy.Config  优化  减少了对象的堆分配 优化多路复用性能 优化 grpc 编解码性能，通过 Release 时释放(Close) LinkBuffer 在计算 backup request 的消耗(cost)时，区分 ErrRPCFinish 多路复用分片队列逻辑移动至 netpoll/mux，并重命名分片字典 优化Fast api中容器类型的长度编码逻辑  Bug 修复  修复 server 端 WithErrorHandler 配置不生效问题 调整 lbcache 中的 Balancer 初始化逻辑 修复 TraceCtl 可能为 nil 的问题(仅影响单测) 设置默认的 rpc timeout, 并支持设置 WithRPCTimeout(0) 来关闭超时中间件 修复 default logger 使用错误的 call depth 重命名 BackwardProxy 为 ReverseProxy 修复 grpc 场景下的 panic 修复 grpc 场景下的潜在风险（keepalive 超时导致 panic） 修复 void 方法中的异常缺失 修复实例变更时 dump 信息不正确问题。  文档  修复失效的中文链接 将全部 doc 移至官网 cloudwego.io  Netpoll API Change:  适应 netpoll.Writer.Append 的 API 改动，返回值从 2个 变为 1个  依赖变化  github.com/cloudwego/netpoll: v0.0.4 -\u003e v0.1.2  ","categories":"","description":"","excerpt":"功能 泛化调用  IDL 解析支持多 Service 暴露 SetSeqID 方法便于二进制泛化场景 server …","ref":"/zh/blog/2021/12/13/kitex-v0.1.0-%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/","tags":"","title":"Kitex v0.1.0 版本发布"},{"body":"Hotfix  check args in LinkBuffer API  ","categories":"","description":"","excerpt":"Hotfix  check args in LinkBuffer API  ","ref":"/blog/2021/12/13/netpoll-release-v0.1.2/","tags":"","title":"Netpoll Release v0.1.2"},{"body":"Bug 修复:  LinkBuffer 增加了空值校验  ","categories":"","description":"","excerpt":"Bug 修复:  LinkBuffer 增加了空值校验  ","ref":"/zh/blog/2021/12/13/netpoll-v0.1.2-%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/","tags":"","title":"Netpoll v0.1.2 版本发布"},{"body":"Improvement  enhance mux shard queue  Bugfix  book never reset readnode  Chore  update readme  ","categories":"","description":"","excerpt":"Improvement  enhance mux shard queue  Bugfix  book never reset …","ref":"/blog/2021/12/09/netpoll-release-v0.1.1/","tags":"","title":"Netpoll Release v0.1.1"},{"body":"优化:  优化了多路复用下，分片队列的性能  Bug 修复:  修复了 book 方法在多路复用下的 bug  文档  修正了一些大小写和语法问题，并更新了链接  ","categories":"","description":"","excerpt":"优化:  优化了多路复用下，分片队列的性能  Bug 修复:  修复了 book 方法在多路复用下的 bug  文档  修正了一些大小写和语 …","ref":"/zh/blog/2021/12/09/netpoll-v0.1.1-%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/","tags":"","title":"Netpoll v0.1.1 版本发布"},{"body":"Improvement  add mux.ShardQueue to support connection multiplexing input at a single LinkBuffer Node to improve performance fix waitReadSize logic bug and enhance input trigger reduce timeout issues when waitRead and inputAck have competition unify and simplify conn locks  Bugfix  ensure EventLoop object will not be finalized before serve return  Chore  update readme update issue templates  Breaking Change  remove WriteBuffer() returned parameter n  ","categories":"","description":"","excerpt":"Improvement  add mux.ShardQueue to support connection multiplexing …","ref":"/blog/2021/12/01/netpoll-release-v0.1.0/","tags":"","title":"Netpoll Release v0.1.0"},{"body":"功能:  增加了分片队列，用于支持连接多路复用 优化方案：尽可能的维护单节点 LinkBuffer 来减少 copy 优化方案：修复了 waitReadSize 的 bug，并优化了 input trigger 效率 优化方案：减少了 waitRead 和 inputAck 冲突时产生的超时错误 逻辑简化：简化了连接状态机  Bug 修复:  修复了 eventLoop 提前 GC 的 bug  文档  更新 README，将 Performance 部分移动至 netpoll-benchmark 项目 更新了 reference，添加了官网信息，移除了 change log  重大变更  WriteBuffer 返回值由 (n int, err error) 改为 (err error)  ","categories":"","description":"","excerpt":"功能:  增加了分片队列，用于支持连接多路复用 优化方案：尽可能的维护单节点 LinkBuffer 来减少 copy 优化方案： …","ref":"/zh/blog/2021/12/01/netpoll-v0.1.0-%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/","tags":"","title":"Netpoll v0.1.0 版本发布"},{"body":" On September 8, 2021, ByteDance announced the launch of CloudWeGo open source project. CloudWeGo is a set of microservice middleware developed by ByteDance with high performance, strong scalability and stability. It focuses on microservice communication and governance, and meets the demands of different services in various scenarios. CloudWeGo currently has 4 Repos: Kitex, Netpoll, Thriftgo and netpoll-http2, featuring the RPC framework – Kitex and the network library – Netpoll.\n Recently, ByteDance Service Framework Team officially announced the open source of CloudWeGo. It includes the Golang microservice RPC framework – Kitex, which has been deeply used in Douyin and Toutiao.\nThis article aims to share the scenarios and technical issues that developers need to know when stress testing Kitex. These guides will help users adjust and optimize Kitex to better match their business needs, and maximize Kitex’s performance in real RPC scenarios. Users can also refer to the official stress test project – kitex-benchmark for more details.\nThe Characteristics of Microservice Scenario Kitex was born in ByteDance’s large-scale microservices architecture practice. The scenario it is aimed at is naturally a microservices scenario. Therefore, the following will first introduce the characteristics of microservices, so that developers can understand Kitex’s design thinking in depth.\n RPC Communication Model  The communication between microservices is usually based on PingPong model. So, in addition to the conventional throughput performance index, developers also need to consider the average latency of each RPC.\n Complex Call Chain  An RPC call often requires multiple microservices to collaborate, and downstream services have their own dependencies, so the entire call chain will be a complex network structure.\nIn this kind of complex call chains, the latency fluctuation of one intermediate node may be transmitted to the entire chain，resulting in an overall timeout. When there are many nodes on the chain, even if the fluctuation probability of each node is very low, the timeout probability that eventually converges on the chain will be magnified. Therefore, the latency fluctuation of a single service, notably P99, is also a key indicator that has a significant impact on online services.\n Size of Data Package  Although the size of transmitted data packages depends on the actual business scenario, the internal statistics of ByteDance found that most online requests are small packages (\u003c2KB). So we focused on optimizing the performance in the small data package scenarios while taking the large package scenarios into account.\nStress Test for Microservice Scenarios Determine Stress Test Objects Measuring the performance of an RPC framework requires consideration from two perspectives: Client and Server. In large-scale business architectures, upstream clients are not necessarily using the same frameworks as downstream, and same goes to the downstream services scheduled by developers. The situation becomes more complicated when Service Mesh is involved.\nSome stress test projects often generate performance data for the entire framework by mixing Client and Server processes, which is likely to be inconsistent with the actual online operation.\nIf you want to stress test Server, you should give Client as many resources as possible to push Server to its limit, and vice versa. If both Client and Server are only provided 4-core CPUs for stress tests, it will be impossible for developers to determine the performance data is referring to either Client or Server. Thus, the test result will not have practical value for online services.\nAlignment of Connection Model Conventional RPCs have three major connection models:\n Short connection: Each request creates a new connection and closes the connection immediately after the return is received. Persistent connection pool: A single connection can process only one complete request \u0026 return at once. Connection multiplexing: A single connection can process multiple requests \u0026 returns asynchronously at the same time.  Each type of connection model is not absolutely good or bad, it depends on the actual usage scenario. Although connection multiplexing generally performs the best, the application must rely on the protocol being able to support package serial numbers, and some older framework services may not support multiplexing calls.\nIn order to ensure maximum compatibility, Kitex initially used short connections on the Client side by default, while other mainstream open source frameworks used connection multiplexing by default. It resulted in large performance data deviations for some users when stress testing with default configuration.\nLater, in order to accommodate the common scenario of open source users, Kitex supported persistent connection by default in v0.0.2.\nAlignment of Serialization Strategy For RPC frameworks, regardless of service governance, the computation overhead is mainly generated in serialization and deserialization.\nKitex uses the Go protobuf library to serialize Protobuf. And for serialization of Thrift, Kitex has specific performance optimization, which is introduced in the blog post on our official web.\nMost of the current open source frameworks support Protobuf in preference, and some built-in Protobuf are actually gogo/protobuf versions with performance optimizations. However, gogo/protobuf is currently at risk of maintenance absence. Therefore, due to maintainability concerns, we decided to use the official protobuf library only. Certainly, we will plan to optimize Protobuf in the future.\nUse Exclusive CPU Although multiple processes would usually utilize the CPU capability at the same time for online applications. But in stress test scenarios, both Client and Server processes are extremely busy. Sharing the CPU will result in a large number of context switching, which makes the output data less reliable and prone to large fluctuations.\nTherefore, we recommend that the Client and Server processes should be isolated on different CPUs or different exclusive machines. If you want to further avoid the impact of other processes, you can add the nice -n -20 command to adjust the scheduling priority of the stress testing process.\nIn addition, if possible, using physical machines makes the test results more precise and reproducible compared to using virtual machines on cloud platforms.\nPerformance Data Demonstration On the premise of meeting the above requirements, we compared the stress test results of multiple frameworks using Protobuf. The stress test source code can be found in kitex-benchmark repo. When Server is fully loaded, P99 Latency of Kitex in connection pool mode is the lowest of all frameworks. In multiplexing mode, Kitex also performs well in each indicator.\nConfiguration\n Client 16 CPUs，Server 4 CPUs 1KB Request Package Size, Echo Scenario  Reference Data\n KITEX: Connection Pool Model (Default Setting) KITEX-MUX: Connection Multiplexing Connection Multiplexing for all other Frameworks  Summary Each mainstream Golang open source RPC framework actually has its own focus in terms of design goals: some focus on generality, some on scenarios with light business logic like Redis, some on throughput performance, and some on P99 latency.\nIn the daily iteration of ByteDance’s business, it is common for a feature to cause one indicator to rise and another indicator to decline. Therefore, Kitex was more inclined to solve various problems in large-scale microservice scenarios at the beginning of its design.\nSince the launch of Kitex, we have received a large amount of self-testing data from our users. We appreciate the community for their attention and support. We also encourage developers to use the testing guide provided in this article, and select appropriate tools for their own scenarios. For more questions, please make an Issue on GitHub.\nPertinent Links  CloudWeGo Official Website: https://www.cloudwego.io Kitex: https://github.com/cloudwego/kitex Netpoll: https://github.com/cloudwego/netpoll kitex-benchmark: https://github.com/cloudwego/kitex-benchmark netpoll-benchmark: https://github.com/cloudwego/netpoll-benchmark Go Protobuf Library: https://github.com/golang/protobuf Thriftgo：https://github.com/cloudwego/thriftgo  ","categories":"","description":"","excerpt":" On September 8, 2021, ByteDance announced the launch of CloudWeGo …","ref":"/blog/2021/11/24/getting-started-with-kitexs-practice-performance-testing-guide/","tags":"","title":"Getting Started With Kitex's Practice: Performance Testing Guide"},{"body":" 2021 年 9 月 8 日，字节跳动宣布正式开源 CloudWeGo。CloudWeGo 是一套字节跳动内部微服务中间件集合，具备高性能、强扩展性和稳定性的特点，专注于解决微服务通信与治理的难题，满足不同业务在不同场景的诉求。CloudWeGo 第一批开源了四个项目：Kitex、Netpoll、Thriftgo 和 netpoll-http2，以 RPC 框架 Kitex 和网络库 Netpoll 为主。\n 日前，字节跳动服务框架团队正式开源 CloudWeGo ，在抖音、今日头条均有深度应用的 Golang 微服务 RPC 框架 Kitex 也包含在其中。\n本文旨在分享开发者在压测 Kitex 时需要了解的场景和技术问题。这些建议有助于用户更好地结合真实 RPC 场景对 Kitex 进行调优，使之更贴合业务需要、发挥最佳性能。用户也可以参考官方提供的压测项目 kitex-benchmark 了解更多细节。\n微服务场景的特点 Kitex 诞生于字节跳动大规模微服务架构实践，面向的场景自然是微服务场景，因此下面会先介绍微服务的特点，方便开发者深入理解 Kitex 在其中的设计思考。\n  RPC 通信模型\n微服务间的通信通常以 PingPong 模型为主，所以除了常规的吞吐性能指标外，每次 RPC 的平均时延也是开发者需要考虑的点。\n  复杂的调用链路\n一次 RPC 调用往往需要多个微服务协作完成，而下游服务又会有其自身依赖，所以整个调用链路会是一个复杂的网状结构。\n在这种复杂调用关系中，某个中间节点出现的延迟波动可能会传导到整个链路上，导致整体超时。当链路上的节点足够多时，即便每个节点的波动概率很低，最终汇聚到链路上的超时概率也会被放大。所以单一服务的延迟波动 —— 即 P99 延迟指标，也是一个会对线上服务产生重大影响的关键指标。\n  包体积大小\n虽然一个服务通信包的大小取决于实际业务场景，但在字节跳动的内部统计中，我们发现线上请求大多以小包（\u003c2KB）为主，所以在兼顾大包场景的同时，也重点优化了小包场景下的性能。\n  针对微服务场景进行压测 确定压测对象 衡量一个 RPC 框架的性能需要从两个视角分别去思考：Client 视角与 Server 视角。在大规模的业务架构中，上游 Client 不见得使用的也是下游的框架，而开发者调用的下游服务也同样如此，如果再考虑到 Service Mesh 的情况就更复杂了。\n一些压测项目通常会把 Client 和 Server 进程混部进行压测，然后得出整个框架的性能数据，这其实和线上实际运行情况很可能是不符的。\n如果要压测 Server，应该给 Client 尽可能多的资源，把 Server 压到极限，反之亦然。如果 Client 和 Server 都只给了 4 核 CPU 进行压测，会导致开发者无法判断最终得出来的性能数据是哪个视角下的，更无法给线上服务做实际的参考。\n对齐连接模型 常规 RPC 的连接模型主要有三种：\n 短连接：每次请求都创建新连接，得到返回后立即关闭连接 长连接池：单个连接同时只能处理一次完整请求与返回 连接多路复用：单个连接可以同时异步处理多个请求与返回  每类连接模型没有绝对好坏，取决于实际使用场景。连接多路复用虽然一般来说性能相对最好，但应用上必须依赖协议能够支持包序列号，且一些老框架服务可能也并不支持多路复用的方式调用。\nKitex 最早为保证最大程度的兼容性，在 Client 端默认使用了短连接，而其他主流开源框架默认使用连接多路复用，这导致一些用户在使用默认配置压测时，出现了比较大的性能数据偏差。\n后来为了契合开源用户的常规使用场景，Kitex 在 v0.0.2 中也加入了默认使用长连接的设置。\n对齐序列化方式 对于 RPC 框架来说，不考虑服务治理的话，计算开销主要都集中在序列化与反序列化中。\nKitex 对于 Protobuf 的序列化使用的是官方的 Protobuf 库，对于 Thrift 的序列化，则专门进行了性能优化，这方面的内容在官网博客中有介绍。\n当前开源框架大多优先支持 Protobuf，而部分框架内置使用的 Protobuf 其实是做了许多性能优化的 gogo/protobuf 版本，但由于 gogo/protobuf 当前有失去维护的风险，所以出于可维护性角度考虑，我们依然决定只使用官方的 Protobuf 库，当然后续我们也会计划对 Protobuf 进行优化。\n使用独占 CPU 虽然线上应用通常是多个进程共享 CPU，但在压测场景下，Client 与 Server 进程都处于极端繁忙的状况，如果同时还共享 CPU 会导致大量上下文切换，从而使得数据缺乏可参考性，且容易产生前后很大波动。\n所以我们建议是将 Client 与 Server 进程隔离在不同 CPU 或者不同独占机器上进行。如果还想要进一步避免其他进程产生影响，可以再加上 nice -n -20 命令调高压测进程的调度优先级。\n另外如果条件允许，相比云平台虚拟机，使用真实物理机会使得测试结果更加严谨与具备可复现性。\n性能数据参考 在满足上述要求的前提下，我们对多个框架使用 Protobuf 进行了压测对比，压测代码在 kitex-benchmark 仓库。在充分压满 Server 的目标下，Kitex 在连接池模式下的 P99 Latency 在所有框架中最低。而在多路复用模式下，Kitex 在各指标上也都具有更加明显的优势。\n配置：\n Client 16 CPUs，Server 4 CPUs 1KB 请求大小，Echo 场景  参考数据：\n KITEX：连接池模式（默认模式） KITEX-MUX：多路复用模式 其他框架均使用多路复用模式  结语 在当前主流的 Golang 开源 RPC 框架中，每个框架其实在设计目标上都各有侧重：有些框架侧重于通用性，有些侧重于类似 Redis 这种轻业务逻辑的场景，有些侧重于吞吐性能，而有些则更侧重 P99 时延。\n字节跳动的业务在日常迭代中，常常会出现因某个 feature 导致一个指标上升，另一个指标下降的情况，因此 Kitex 在设计之初就更倾向于解决大规模微服务场景下各种问题。\nKitex 发布后，我们接到了大量来自用户的自测数据，感谢社区对我们的关注和支持，也欢迎广大开发者基于本文提供的测试指南，针对自己的实际场景选择合适的工具。更多问题，请在 GitHub 上提 Issue 交流。\n相关链接   CloudWeGo 官网: https://www.cloudwego.io\n  Kitex: https://github.com/cloudwego/kitex\n  Netpoll: https://github.com/cloudwego/netpoll\n  kitex-benchmark: https://github.com/cloudwego/kitex-benchmark\n  netpoll-benchmark: https://github.com/cloudwego/netpoll-benchmark\n  官方 Protobuf 库：https://github.com/golang/protobuf\n  Thriftgo：https://github.com/cloudwego/thriftgo\n  ","categories":"","description":"","excerpt":" 2021 年 9 月 8 日，字节跳动宣布正式开源 CloudWeGo。CloudWeGo 是一套字节跳动内部微服务中间件集合，具备高性 …","ref":"/zh/blog/2021/11/24/rpc-%E6%A1%86%E6%9E%B6-kitex-%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E6%8C%87%E5%8D%97/","tags":"","title":"RPC 框架 Kitex 实践入门：性能测试指南"},{"body":"Improvement:  Use shard rings to reduce lock overhead in connpool. Fill upstream information to rpcinfo from TTheader, for printing useful log when decode error happened. Move unlink uds operation to CreateListener. Replace sync.Mutex by sync.RWMutex of event.go and ring_single.go.  Bugfix:  Fix netpollmux shard index overflow. Remove reflection of WithCircuitBreaker option arguments to prevent data-race. Fix rpc finished error may happen small probability in failure retry scenario \u0026\u0026 add sample check for retry circuit breaking. Fix a test case mistake in endpoint_test.go. Modify longconn variable name to conn.  Tool:  Kitex codegen tool supports passing through thrift-go plugin arguments.  Docs:  Use a link to the the kitex-benchmark repository to replace the performance section in README.  Dependency Change:  github.com/tidwall/gjson: v1.8.0 -\u003e v1.9.3  ","categories":"","description":"","excerpt":"Improvement:  Use shard rings to reduce lock overhead in connpool. …","ref":"/blog/2021/11/05/kitex-release-v0.0.8/","tags":"","title":"Kitex Release v0.0.8"},{"body":"优化  使用分片 ring 减少连接池的锁开销。 装填 TTHeader 中的上游服务信息到 rpcinfo 中，用于在 decode 出错时输出来源信息。 Unlink uds 调整至 CreateListener 中。 event.go 和 ring_single.go 中的 Mutex 改为 RWMutex。  Bug 修复  修复 netpollmux shard index 溢出的问题。 移除 WithCircuitBreaker option 里对参数的反射，避免 data-race。 在重试场景下， 修复 rpc finish 错误导致的小概率失败的问题，并且加上了熔断 sample 的校验。 修复 endpoint_test.go 中的一处单测错误。 修改 conn_wrapper.go 中 longconn 变量命名为 conn.。  生成工具  代码生成工具支持透传thrift-go插件参数。  文档  将 README 中的性能结果改为引用 kitex-benchmark 仓库的数据。  依赖变化  github.com/tidwall/gjson: v1.8.0 -\u003e v1.9.3  ","categories":"","description":"","excerpt":"优化  使用分片 ring 减少连接池的锁开销。 装填 TTHeader 中的上游服务信息到 rpcinfo 中，用于在 decode 出错 …","ref":"/zh/blog/2021/11/05/kitex-v0.0.8-%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/","tags":"","title":"Kitex v0.0.8 版本发布"},{"body":" This article is excerpted from the ByteDance Architecture Practice series.\n“ByteDance Architecture Practice” is a series of articles produced by the technical teams and experts from the ByteDance Architecture Team, to share the team’s practical experience and lessons learnt from the development of infra-architecture, for the purpose of enhancing communication and growth of the developers.\nAs an important part of R\u0026D system, RPC framework carries almost all service traffics. This paper will briefly introduce the design and practice of the ByteDance in-house developed network library – Netpoll, as well as the problems and solutions that arise during our practices. This article can be used as a reference to help the tech-community’s further practices and experiments.\n Preface As an important part of the R\u0026D system, RPC framework carries almost all service traffics. As Golang is used more and more widely in ByteDance, the business has higher requirements on its framework. However, the “Go net” library cannot provide sufficient performance and control to support the business, notably inability to perceive the connection state, low utilization due to the large number of connections, and inability to control the number of goroutines. In order to take full control of the network layer, it’s necessary to make some exploration prospectively, and finally empower the business. The Service Framework Team launched a new self-developed network library – “Netpoll” based on “epoll”, and developed a new-generation Golang framework – “Kitex” based on “Netpoll”.\nSince there are many articles discussing the principles of “epoll”, this article will briefly introduce the design of “Netpoll” only. We’ll then try to review some of our practices regarding “Netpoll”. Finally, we’ll share a problem we encountered during our practices and how we solved it. In the meantime, we welcome more peers who are interested in Golang and Go framework to join us!\nDesign of the New Network Library Reactor - Event Monitoring and the Core of Scheduling The core of “Netpoll” is the event monitoring scheduler – “Reactor”, which uses “epoll” to monitor the “File Descriptor (fd)” of the connection and triggers the read, write and close events on the connection through the callback mechanism.  Server - MainReactor \u0026 SubReactor Implementation Netpoll combines Reactors in a 1: N master-slave pattern:\n “MainReactor” mainly manages the “Listener”, and is responsible for monitoring ports and establishing new connections. The “SubReactor” manages the “Connection”, listens all assigned connections, and submits all triggered events to the goroutine pool for processing. “Netpoll” supports “NoCopy RPC” by introducing active memory management in I/O tasks and providing an “NoCopy” invocation interface to the upper layer. Add a goroutine pool to centrally process I/O tasks, reduce the number of goroutines and scheduling overhead.   Client - Shares the Capability of Reactor SubReactor is shared between the client and server. Netpoll implements “Dialer” and provides the function for establishing connections. On the client, similar to “net.Conn”, Netpoll provides underlying support for “write -\u003e wait read callback”.  Nocopy Buffer Why Nocopy Buffer? As mentioned earlier, the way epoll is triggered affects the design of I/O and buffer, which can be generally divided into two approaches:\n Level Trigger (LT). It is necessary to complete I/O actively after the event is triggered, and provides buffers directly to the upper code. Edge Trigger (ET). You can choose to manage the event notification only (e.g. go net), with the upper layer code for I/O completion and buffers management.  Both ways have their advantages and disadvantages. “Netpoll” adopts the first strategy, which has better timeliness and higher fault tolerance rate. Active I/O can centralize memory usage and management, provide nocopy operation, and reduce GC. In fact, some popular open-source network libraries, such as “easygo”, “evio”, “gnet”, etc. are also designed in this way.\nHowever, using LT also brings another problem, namely the additional concurrency overhead caused by the underlying active I/O and the concurrent buffer operations by the upper code. For example, there are concurrent read and write when I/O read(data)/write(buffer) and the upper code reads the buffer, vice versa. In order to ensure data correctness and avoid lock contention, existing open-source network libraries usually adopt synchronous processing of buffer (“easygo”, “evio”) or provide a copy of buffer to the upper layer code (“gnet”), which are not suitable for business processing or have considerable copy overhead.\nOn the other hand, common buffer libraries such as “bytes”, “bufio”, and “ringbuffer” have problems such as “growth” requiring copy of data from the original array; capacity can only be expanded but can’t be reduced; occupying a large amount of memory etc. Therefore, we hope to introduce a new form of buffer to solve the two problems above.\nThe Design and Advantages of Nocopy Buffer Nocopy Buffer is implemented based on linked-list of array. As shown in the figure below, we abstract []byte array into blocks and combine blocks into Nocopy Buffer in the form of a linked list. Meanwhile, reference counting mechanism, Nocopy API and object pool are introduced.   Nocopy Buffer has the following advantages over some common buffer libraries like “bytes”, “bufio”, and “ringbuffer”:\n Read and write in parallel without lock, and supports stream read/write with nocopy  Read and write operate the head pointer and tail pointer separately without interfering with each other.   Efficient capacity expansion and reduction  For capacity expansion, you can add new blocks directly after the tail pointer without copying the original array. For capacity reduction, the head pointer directly releases the used block node to complete the capacity reduction. Each block has an independent reference count, and when the freed block is no longer referenced, the block node is actively reclaimed.   Flexible slicing and splicing of buffer (the characteristic of linked list)  Support arbitrary read slicing (nocopy), and the upper layer code can process data stream slicing in parallel with nocopy by reference counting GC, regardless of the lifecycle. Support arbitrary splicing (nocopy). Buffer write supports splicing block after the tail pointer, without copy, and ensuring that data is written only once.   Nocopy Buffer is pooled to reduce GC  Treat each []byte array as a block node, and build an object pool to maintain free blocks, thus reuse blocks, reduce memory footprint and GC. Based on the Nocopy Buffer, we implemented Nocopy Thrift, so that the codec process allocates zero memory with zero copy.    Connection Multiplexing RPC calling is usually in the form of short connection or persistent connection pool, and each call is bound to one connection. Therefore, when the scale of upstream and downstream is large, the number of existing connections in the network increases in the speed of MxN, which brings huge scheduling pressure and computing overhead, and makes service governance difficult. Therefore, we want to introduce a mechanism for “parallel processing of calls on a single persistent connection” to reduce the number of connections in the network. This mechanism is called connection multiplexing.\nThere are some existing open-source connection multiplexing solutions. But they are limited by code level constraints. They all require copy buffer for data subcontracting and merging, resulting in poor performance. Nocopy Buffer, with its flexible slicing and splicing, well supports data subcontracting and merging with nocopy, making it possible to achieve high-performance connection multiplexing schemes.\nThe design of Netpoll-based connection multiplexing is shown in the figure below. We abstract the Nocopy Buffer(and its sharding) into virtual connections, so that the upper layer code retains the same calling experience as “net.Conn”. At the same time, the data on the real connection can be flexibly allocated to the virtual connection through protocol subcontracting in the underlying code. Or send virtual connection data through protocol encoding.  \nThe connection multiplexing scheme contains the following core elements:\n The virtual connection  It is essentially a “Nocopy Buffer”, designed to replace real connections and avoid memory copy. The upper-layer service logic/codec is executed on the virtual connection, and the upper-layer logic can be executed in parallel asynchronously and independently.   Shared map  Shared locking is introduced to reduce the lock intensity. The Sequence ID is used to mark the request on the caller side and the shared lock is used to store the callback corresponding to the ID. After receiving the response data, find the corresponding callback based on the sequence ID and execute it.   Data subcontracting and encoding  How to identify the complete request-response data package is the key to make the connection multiplexing scheme feasible, so the protocol needs to be introduced. The “Thrift Header Protocol” is used to check the data package integrity through the message header, and sequence ids are used to mark the corresponding relations between request and response.    ZeroCopy “ZeroCopy” refers to the ZeroCopy function provided by Linux. In the previous chapter, we discussed nocopy of the service layer. But as we know, when we call the “sendmsg” system-call to send a data package, actually there is still a copy of the data, and the overhead of such copies is considerable when the data packages are large. For example, when the data package has the size of 100M, we can see the following result:   The previous example is merely the overhead of tcp package sending. In our scenario, most services are connected to the “Service Mesh”. Therefore, there are three copies in a package sending: Service process to kernel, kernel to sidecar, sidecar to kernel. This makes the CPU usage caused by copying especially heavy for services demanding large package transactions, as shown in the following figure:   To solve this problem, we chose to use the ZeroCopy API provided by Linux (send is supported after 4.14; receive is supported after 5.4). But this introduces an additional engineering problem: the ZeroCopy send API is incompatible with the original call method and does not coexist well. Here’s how ZeroCopy Send works: After the service process calls “sendmsg”, “sendmsg” records the address of the “iovec” and returns it immediately. In this case, the service process cannot release the memory, and needs to wait for the kernel to send a signal indicating that an “iovec” has been successfully sent before it can be released via “epoll”. Since we don’t want to change the way the business side uses it, we need to provide a synchronous sending and receiving interface to the upper layer, so it is difficult to provide both ZeroCopy and non-Zerocopy abstraction based on the existing API. Since ZeroCopy has performance degradation in small package scenarios, this is not the default option.\nThus, the ByteDance Service Framework Team collaborated with the ByteDance Kernel Team. The Kernel Team provided the synchronous interface: when “sendmsg” is called, the kernel listens and intercepts the original kernel callback to the service, and doesn’t let “sendmsg” return values until the callback is complete. This allows us to easily plug in “ZeroCopy send” without changing the original model. Meanwhile, the ByteDance Kernel Team also implements ZeroCopy based on Unix domain socket, which enables zero-copy communication between service processes and Mesh sidecar.\nAfter using “ZeroCopy send”, we can see that the kernel is no longer occupied by copy through perf:   In terms of CPU usage, ZeroCopy can save half the cpu of non-ZeroCopy in large package scenarios.\nDelay Caused By Go Scheduling In our practice, we found that although our newly written “Netpoll” outperformed the “Go net” library in terms of avg delay, it was generally higher than the “Go net” library in terms of p99 and max delay, and the spikes would be more obvious, as shown in the following figure (Go 1.13, Netpoll + multiplexing in blue, Netpoll + persistent connection in green, Go net library + persistent connection in yellow):  \nWe tried many ways to improve it, but the outcomes were unsatisfactory. Finally, we locate that the delay was not caused by the overhead of “Netpoll” itself, but by the scheduling of Go, for example:\n In “Netpoll”, the “SubReactor” itself is also a “goroutine”, which is affected by scheduling and cannot be guaranteed to be executed immediately after the “EpollWait” callback, so there would be a delay here. At the same time, since the “SubReactor” used to handle I/O events and the “MainReactor” used to handle connection listening are “goroutines” themselves, it is actually impossible to ensure that these reactors can be executed in parallel under multi-kernel conditions. Even in the most extreme cases, these reactors may be under the same P, and eventually become sequential execution, which cannot take full advantage of multi-kernel; After “EpollWait callback”, I/O events are processed serially in the “SubReactor”, so the last event may have a long tail problem. In connection multiplexing scenarios, since each connection is bound to a “SubReactor”, the delay is entirely dependent on the scheduling of the “SubReactor”, resulting in more pronounced spikes. Because Go has specific improvements for the net library in runtime, the net library will not have the above situation. At the same time, the net library is also a “goroutine-per-connection” model, so it ensures that requests can be executed in parallel without interfering with each other.  For the above problems, we have two solutions at present:\n Modify the Go runtime source code, register a callback in the Go runtime, call EpollWait each time, and pass the fd to the callback execution; Work with the ByteDance Kernel Team to support simultaneous batch read/write of multiple connections to solve sequential problems. In addition, in our tests, Go 1.14 reduces the latency slightly lower and smoother, but the max QPS that can be achieved is lower. I hope our ideas can provide some references to peers in the industry who also encountered this problem.  Postscript We hope the above sharing can be helpful to the community. At the same time, we are accelerating the development of “Netpoll” and “Kitex” – a new framework based on “Netpoll”. You are welcome to join us and build Golang ecology together!\nReference  http://man7.org/linux/man-pages/man7/epoll.7.html https://golang.org/src/runtime/proc.go https://github.com/panjf2000/gnet https://github.com/tidwall/evio  ","categories":"","description":"","excerpt":" This article is excerpted from the ByteDance Architecture Practice …","ref":"/blog/2021/10/09/bytedance-practices-on-go-network-library/","tags":"","title":"ByteDance Practices on Go Network Library"},{"body":" 本文选自“字节跳动基础架构实践”系列文章。\n“字节跳动基础架构实践”系列文章是由字节跳动基础架构部门各技术团队及专家倾力打造的技术干货内容，和大家分享团队在基础架构发展和演进过程中的实践经验与教训，与各位技术同学一起交流成长。\nRPC 框架作为研发体系中重要的一环，承载了几乎所有的服务流量。本文将简单介绍字节跳动自研网络库 netpoll 的设计及实践；以及我们实际遇到的问题和解决思路，希望能为大家提供一些参考。\n 前言 RPC 框架作为研发体系中重要的一环，承载了几乎所有的服务流量。随着公司内 Go 语言使用越来越广，业务对框架的要求越来越高，而 Go 原生 net 网络库却无法提供足够的性能和控制力，如无法感知连接状态、连接数量多导致利用率低、无法控制协程数量等。为了能够获取对于网络层的完全控制权，同时先于业务做一些探索并最终赋能业务，框架组推出了全新的基于 epoll 的自研网络库 —— netpoll，并基于其之上开发了字节内新一代 Golang 框架 Kitex。\n由于 epoll 原理已有较多文章描述，本文将仅简单介绍 netpoll 的设计；随后，我们会尝试梳理一下我们基于 netpoll 所做的一些实践；最后，我们将分享一个我们遇到的问题，以及我们解决的思路。同时，欢迎对于 Go 语言以及框架感兴趣的同学加入我们！\n新型网络库设计 Reactor - 事件监听和调度核心 netpoll 核心是 Reactor 事件监听调度器，主要功能为使用 epoll 监听连接的文件描述符（fd），通过回调机制触发连接上的 读、写、关闭 三种事件。\n Server - 主从 Reactor 实现 netpoll 将 Reactor 以 1:N 的形式组合成主从模式。\n MainReactor 主要管理 Listener，负责监听端口，建立新连接； SubReactor 负责管理 Connection，监听分配到的所有连接，并将所有触发的事件提交到协程池里进行处理。 netpoll 在 I/O Task 中引入了主动的内存管理，向上层提供 NoCopy 的调用接口，由此支持 NoCopy RPC。 使用协程池集中处理 I/O Task，减少 goroutine 数量和调度开销。\n   Client - 共享 Reactor 能力 client 端和 server 端共享 SubReactor，netpoll 同样实现了 dialer，提供创建连接的能力。client 端使用上和 net.Conn 相似，netpoll 提供了 write -\u003e wait read callback 的底层支持。\n Nocopy Buffer 为什么需要 Nocopy Buffer ? 在上述提及的 Reactor 和 I/O Task 设计中，epoll 的触发方式会影响 I/O 和 buffer 的设计，大体来说分为两种方式：\n 采用水平触发(LT)，则需要同步的在事件触发后主动完成 I/O，并向上层代码直接提供 buffer。 采用边沿触发(ET)，可选择只管理事件通知(如 go net 设计)，由上层代码完成 I/O 并管理 buffer。  两种方式各有优缺，netpoll 采用前者策略，水平触发时效性更好，容错率高，主动 I/O 可以集中内存使用和管理，提供 nocopy 操作并减少 GC。事实上一些热门开源网络库也是采用方式一的设计，如 easygo、evio、gnet 等。\n但使用 LT 也带来另一个问题，即底层主动 I/O 和上层代码并发操作 buffer，引入额外的并发开销。比如：I/O 读数据写 buffer 和上层代码读 buffer 存在并发读写，反之亦然。为了保证数据正确性，同时不引入锁竞争，现有的开源网络库通常采取 同步处理 buffer(easygo, evio) 或者将 buffer 再 copy 一份提供给上层代码(gnet) 等方式，均不适合业务处理或存在 copy 开销。\n另一方面，常见的 bytes、bufio、ringbuffer 等 buffer 库，均存在 growth 需要 copy 原数组数据，以及只能扩容无法缩容，占用大量内存等问题。因此我们希望引入一种新的 Buffer 形式，一举解决上述两方面的问题。\nNocopy Buffer 设计和优势 Nocopy Buffer 基于链表数组实现，如下图所示，我们将 []byte 数组抽象为 block，并以链表拼接的形式将 block 组合为 Nocopy Buffer，同时引入了引用计数、nocopy API 和对象池。\n  Nocopy Buffer 相比常见的 bytes、bufio、ringbuffer 等有以下优势：\n 读写并行无锁，支持 nocopy 地流式读写  读写分别操作头尾指针，相互不干扰。   高效扩缩容  扩容阶段，直接在尾指针后添加新的 block 即可，无需 copy 原数组。 缩容阶段，头指针会直接释放使用完毕的 block 节点，完成缩容。每个 block 都有独立的引用计数，当释放的 block 不再有引用时，主动回收 block 节点。   灵活切片和拼接 buffer (链表特性)  支持任意读取分段(nocopy)，上层代码可以 nocopy 地并行处理数据流分段，无需关心生命周期，通过引用计数 GC。 支持任意拼接(nocopy)，写 buffer 支持通过 block 拼接到尾指针后的形式，无需 copy，保证数据只写一次。   Nocopy Buffer 池化，减少 GC  将每个 []byte 数组视为 block 节点，构建对象池维护空闲 block，由此复用 block，减少内存占用和 GC。基于该 Nocopy Buffer，我们实现了 Nocopy Thrift，使得编解码过程内存零分配零拷贝。    连接多路复用 RPC 调用通常采用短连接或者长连接池的形式，一次调用绑定一个连接，那么当上下游规模很大的情况下，网络中存在的连接数以 MxN 的速度扩张，带来巨大的调度压力和计算开销，给服务治理造成困难。因此，我们希望引入一种 “在单一长连接上并行处理调用” 的形式，来减少网络中的连接数，这种方案即称为 “连接多路复用”。\n当前业界也存在一些开源的连接多路复用方案，掣肘于代码层面的束缚，这些方案均需要 copy buffer 来实现数据分包和合并，导致实际性能并不理想。而上述 Nocopy Buffer 基于其灵活切片和拼接的特性，很好的支持了 nocopy 的数据分包和合并，使得实现高性能连接多路复用方案成为可能。\n基于 netpoll 的连接多路复用设计如下图所示，我们将 Nocopy Buffer(及其分片) 抽象为虚拟连接，使得上层代码保持同 net.Conn 相同的调用体验。与此同时，在底层代码上通过协议分包将真实连接上的数据灵活的分配到虚拟连接上；或通过协议编码合并发送虚拟连接数据。\n  连接多路复用方案包含以下核心要素：\n  虚拟连接\n 实质上是 Nocopy Buffer，目的是替换真正的连接，规避内存 copy。 上层的业务逻辑/编解码 均在虚拟连接上完成，上层逻辑可以异步独立并行执行。    Shared map\n 引入分片锁来减少锁力度。 在调用端使用 sequence id 来标记请求，并使用分片锁存储 id 对应的回调。 在接收响应数据后，根据 sequence id 来找到对应回调并执行。    协议分包和编码\n 如何识别完整的请求响应数据包是连接多路复用方案可行的关键，因此需要引入协议。 这里采用 thrift header protocol 协议，通过消息头判断数据包完整性，通过 sequence id 标记请求和响应的对应关系。    ZeroCopy 这里所说的 ZeroCopy，指的是 Linux 所提供的 ZeroCopy 的能力。上一章中我们说了业务层的零拷贝，而众所周知，当我们调用 sendmsg 系统调用发包的时候，实际上仍然是会产生一次数据的拷贝的，并且在大包场景下这个拷贝的消耗非常明显。以 100M 为例，perf 可以看到如下结果： \n 这还仅仅是普通 tcp 发包的占用，在我们的场景下，大部分服务都会接入 Service Mesh，所以在一次发包中，一共会有 3 次拷贝：业务进程到内核、内核到 sidecar、sidecar 再到内核。这使得有大包需求的业务，拷贝所导致的 cpu 占用会特别明显，如下图：\n  为了解决这个问题，我们选择了使用 Linux 提供的 ZeroCopy API（在 4.14 以后支持 send；5.4 以后支持 receive）。但是这引入了一个额外的工程问题：ZeroCopy send API 和原先调用方式不兼容，无法很好地共存。这里简单介绍一下 ZeroCopy send 的工作方式：业务进程调用 sendmsg 之后，sendmsg 会记录下 iovec 的地址并立即返回，这时候业务进程不能释放这段内存，需要通过 epoll 等待内核回调一个信号表明某段 iovec 已经发送成功之后才能释放。由于我们并不希望更改业务方的使用方法，需要对上层提供同步收发的接口，所以很难基于现有的 API 同时提供 ZeroCopy 和非 ZeroCopy 的抽象；而由于 ZeroCopy 在小包场景下是有性能损耗的，所以也不能将这个作为默认的选项。\n于是，字节跳动框架组和字节跳动内核组合作，由内核组提供了同步的接口：当调用 sendmsg 的时候，内核会监听并拦截内核原先给业务的回调，并且在回调完成后才会让 sendmsg 返回。这使得我们无需更改原有模型，可以很方便地接入 ZeroCopy send。同时，字节跳动内核组还实现了基于 unix domain socket 的 ZeroCopy，可以使得业务进程与 Mesh sidecar 之间的通信也达到零拷贝。\n在使用了 ZeroCopy send 后，perf 可以看到内核不再有 copy 的占用：\n  从 cpu 占用数值上看，大包场景下 ZeroCopy 能够比非 ZeroCopy 节省一半的 cpu。\nGo 调度导致的延迟问题分享 在我们实践过程中，发现我们新写的 netpoll 虽然在 avg 延迟上表现胜于 Go 原生的 net 库，但是在 p99 和 max 延迟上要普遍略高于 Go 原生的 net 库，并且尖刺也会更加明显，如下图（Go 1.13，蓝色为 netpoll + 多路复用，绿色为 netpoll + 长连接，黄色为 net 库 + 长连接）：\n  我们尝试了很多种办法去优化，但是收效甚微。最终，我们定位出这个延迟并非是由于 netpoll 本身的开销导致的，而是由于 go 的调度导致的，比如说：\n 由于在 netpoll 中，SubReactor 本身也是一个 goroutine，受调度影响，不能保证 EpollWait 回调之后马上执行，所以这一块会有延迟； 同时，由于用来处理 I/O 事件的 SubReactor 和用来处理连接监听的 MainReactor 本身也是 goroutine，所以实际上很难保证在多核情况之下，这些 Reactor 能并行执行；甚至在最极端情况之下，可能这些 Reactor 会挂在同一个 P 下，最终变成了串行执行，无法充分利用多核优势； 由于 EpollWait 回调之后，SubReactor 内是串行处理 I/O 事件的，导致排在最后的事件可能会有长尾问题； 在连接多路复用场景下，由于每个连接绑定了一个 SubReactor，故延迟完全取决于这个 SubReactor 的调度，导致尖刺会更加明显。 由于 Go 在 runtime 中对于 net 库有做特殊优化，所以 net 库不会有以上情况；同时 net 库是 goroutine-per-connection 的模型，所以能确保请求能并行执行而不会相互影响。  对于以上这个问题，我们目前解决的思路有两个：\n 修改 Go runtime 源码，在 Go runtime 中注册一个回调，每次调度时调用 EpollWait，把获取到的 fd 传递给回调执行； 与字节跳动内核组合作，支持同时批量读/写多个连接，解决串行问题。另外，经过我们的测试，Go 1.14 能够使得延迟略有降低同时更加平稳，但是所能达到的极限 QPS 更低。希望我们的思路能够给业界同样遇到此问题的同学提供一些参考。  后记 希望以上的分享能够对社区有所帮助。同时，我们也在加速建设 netpoll 以及基于 netpoll 的新框架 Kitex。欢迎各位感兴趣的同学加入我们，共同建设 Go 语言生态！\n参考资料  http://man7.org/linux/man-pages/man7/epoll.7.html https://golang.org/src/runtime/proc.go https://github.com/panjf2000/gnet https://github.com/tidwall/evio  ","categories":"","description":"","excerpt":" 本文选自“字节跳动基础架构实践”系列文章。\n“字节跳动基础架构实践”系列文章是由字节跳动基础架构部门各技术团队及专家倾力打造的技术干货内 …","ref":"/zh/blog/2021/10/09/%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9C%A8-go-%E7%BD%91%E7%BB%9C%E5%BA%93%E4%B8%8A%E7%9A%84%E5%AE%9E%E8%B7%B5/","tags":"","title":"字节跳动在 Go 网络库上的实践"},{"body":"Feature:  Add default ErrorHandler to wrap remote error when no ErrorHandler is specified. Backward metainfo is supported. JSON generic call is supported. Usage guide: link.  Improvement:  Use new netpoll API to improve throughput and reduce latency for mux. Backward and forward metainfo is supported for mux. Client will use RPCTimeout middleware when necessary. Add validity verification of idle connection in ConnectionPool. QPS limiter token will be reset when QPS limit updates. Reduce the deviation of QPS Limiter.  Bugfix:  Fix WithExitWaitTime won’t set exit wait time correctly. Fix goroutine leak when update interval of QPS limiter. Use actual listen address to build registry info.  Tool:  Fix code generating error when no stream method in protobuf file.  Docs:  English is available for README and all other documents. Guide for generic call. English | 中文 Landscape and Roadmap in README.  Dependency Change:  github.com/cloudwego/netpoll: v0.0.3 -\u003e v0.0.4 github.com/bytedance/gopkg: v0.0.0-20210709064845-3c00f9323f09 -\u003e v0.0.0-20210910103821-e4efae9c17c3  ","categories":"","description":"","excerpt":"Feature:  Add default ErrorHandler to wrap remote error when no …","ref":"/blog/2021/09/26/kitex-release-v0.0.5/","tags":"","title":"Kitex Release v0.0.5"},{"body":"功能:  增加默认的 ErrorHandler 封装 Error（用户指定会被覆盖）。 metainfo 支持反向传递。 支持了 JSON 泛化调用，使用指南可参考：Kitex 泛化调用使用指南。  优化:  多路复用场景下使用了新的 netpoll API 来改善吞吐和延迟。 多路复用场景下支持 metainfo 的正向和反向传递。 Client 会在需要的时候默认使用 RPCTimeout 中间件。 连接池配置增加全局空闲连接和单实例空闲连接合法性校验。 当更新 QPS 最大限制时会重置计数器。 减小 QPS 限流的误差。  Bug 修复:  修复 WithExitWaitTime 没有正确设置退出等待时间的问题。 修复更新 QPS 限制器更新间隔时，协程泄漏的问题。 服务注册使用真实监听的地址。  工具:  修复了当 protobuf 文件只有 unary 方法时，生成出错的问题。  文档:  提供了英文版的README和其他文档。 补充了泛化调用手册： English | 中文。 README 中增加了 landsapce 和 roadmap。  依赖变化:  github.com/cloudwego/netpoll: v0.0.3 -\u003e v0.0.4 github.com/bytedance/gopkg: v0.0.0-20210709064845-3c00f9323f09 -\u003e v0.0.0-20210910103821-e4efae9c17c3  ","categories":"","description":"","excerpt":"功能:  增加默认的 ErrorHandler 封装 Error（用户指定会被覆盖）。 metainfo 支持反向传递。 支持了 JSON  …","ref":"/zh/blog/2021/09/26/kitex-v0.0.5-%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/","tags":"","title":"Kitex v0.0.5 版本发布"},{"body":"Preface Kitex is the next generation high-performance and extensible Go RPC framework developed by ByteDance Service Framework Team. Compared with other RPC frameworks, in addition to its rich features for service governance, it has the following characteristics: integrated with the self-developed network library - Netpoll; supports multiple Message Protocols (Thrift, Protobuf) and Interactive Models (Ping-Pong, Oneway, Streaming); provides a more flexible and extensible code generator.\nCurrently, Kitex has been widely used by the major lines of business in ByteDance, and statistics shows that the number of service access is up to 8K. We’ve been continuously improving Kitex’s performance since its launch. This article will share our work on optimizing Netpoll and serialization.\nOptimization of the Network Library - Netpoll Netpoll, the self-developed network library based on Epoll. Compared with the previous version and the go net library, its performance has been significantly improved. Test results indicated that compared with the last version (2020.05), the latest version (2020.12) has ↑30% throughput capacity, ↓25% AVG latency, and ↓67% TP99 . Its performance is far better than the Go Net library. Below, we’ll share two solutions that can significantly improve its performance.\nOptimizing Scheduling Delays When Calling “epoll_wait” When Netpoll was newly released, it encountered the problem of low AVG latency but high TP99. Through our research and analysis on “epoll_wait”, we found that such a problem could be mitigated by integrating “polling” and “event trigger”. With such improvements in scheduling strategy, the latency can be reduced considerably.\nLet’s have a look at the “syscall.EpollWait” method provided by Go first:\nfunc EpollWait(epfd int, events []EpollEvent, msec int) (n int, err error) Three parameters are provided here, they represent “epoll fd”, “callback events”, and “milliseconds to wait” respectively. Only “msec” is dynamic.\nNormally, we would set “msec = -1” when we are actively calling “EpollWait”, as we want to wait for the event infinitely. In fact, many open-source net libraries were also using it in this way. But our research showed that setting “msec =-1” was not the optimal solution.\nThe kernel source (below) of “epoll_wait” shows that setting “msec = -1” arises extra “fetch_events” checks than setting “msec = 0”, and therefore consumes more time.\nstatic int ep_poll(struct eventpoll *ep, struct epoll_event __user *events, int maxevents, long timeout) { ... if (timeout \u003e 0) { ... } else if (timeout == 0) { ... goto send_events; } fetch_events: ... if (eavail) goto send_events; send_events: ... The Benchmark shows that when an event is triggered, setting “msec = 0” is about 18% faster than setting “msec = -1”. Thus, when triggering complex events, setting “msec = 0” is obviously a better choice.\n   Benchmark time/op bytes/op     BenchmarkEpollWait, msec=0 270 ns/op 0 B/op   BenchmarkEpollWait, msec=-1 328 ns/op 0 B/op   EpollWait Delta -17.68% ~    However, setting “msec = 0” would lead to infinite polling when no event is triggered, consumes lots of resources.\nTaking the previously mentioned factors into account, it’s preferred to set “msec = 0” when an event is triggered and “msec = -1” when no event is triggered to reduce polling. The pseudocode is demonstrated as follows:\nvar msec = -1 for { n, err = syscall.EpollWait(epfd, events, msec) if n \u003c= 0 { msec = -1 continue } msec = 0 ... } Neverthless, our experiments have proved that the improvement is insignificant. Setting “msec = 0” merely reduces the delay of a single call by 50ns, which is not a considerable improvement. If we want to further reduce latency, adjustment must be made in Go runtime scheduling. Thus, let’s further explore this issue: In the pseudocode above, setting “msec= -1” with no triggered event, and “continue” directly will immediately execute “EpollWait” again. Since there is no triggered event while “msec = -1”, the current “goroutine” will block and be switched by “P” passively. However, it is less efficient, and we can save time if we actively switch “goroutine” for “P” before “continue”. So we modified the above pseudocode as follows:\nvar msec = -1 for { n, err = syscall.EpollWait(epfd, events, msec) if n \u003c= 0 { msec = -1 runtime.Gosched() continue } msec = 0 ... } The test results of the modified code showed that throughput ↑12% and TP99 ↓64%. The latency was significantly reduced.\nUtilizing “unsafe.Pointer” Through further study of “epoll_wait”, we find that the “syscall.EpollWait” published by Go and the “epollwait” used by “runtime” are two different versions, as they use different “EpollEvent”. They are demonstrated as follows:\n// @syscall type EpollEvent struct { Events uint32 Fd int32 Pad int32 } // @runtime type epollevent struct { events uint32 data [8]byte // unaligned uintptr } As we can see, the “epollevent” used by “runtime” is the original structure defined by “epoll” at system layer. The published version encapsulates it and splits “epoll_data(epollevent.data)” into two fixed fields: “Fd” and “Pad”. For “runtime”, in its source code we found the following logic:\n*(**pollDesc)(unsafe.Pointer(\u0026ev.data)) = pd pd := *(**pollDesc)(unsafe.Pointer(\u0026ev.data)) Obviously, “runtime” uses “epoll_data(\u0026ev.data)” to store the pointer of the corresponding structure (pollDesc) of “fd” directly. Thus, when an event is triggered, the “struct” object can be found directly with the corresponding logic being executed. However, the external version can only obtain the encapsulated “fd” parameters. So it needs to introduce additional “Map” to manipulate the “struct” object, and the performance will be diminished.\nTherefore, we abandoned “syscall.EpollWait” and designed our own “EpollWait” call by referring to “runtime”. We also use “unsafe.Pointer” to access “struct” objects. The test results showed that our “EpollWait” call contributed to ↑10% throughput and ↓10% TP99, which has significantly improved efficiency.\nSerialization/Deserialization Optimization of Thrift Serialization refers to the process of converting a data structure or object into a binary or textual form. Deserialization is the opposite process. A serialization protocol needs to be agreed during RPC communication. The serialization process is executed before the client sends requests. The bytes are transmitted to the server over the network, and the server will logic-process the bytes to complete an RPC request. Thrift supports “Binary”, “Compact”, and “JSON” serialization protocols. Since “Binary” is the most common protocol used in Bytedance, we will only discuss about “Binary” protocol.\n“Binary” protocol is “TLV” (“Type”, “Length”, “Value”) encoded, that is, each field is described using “TLV” structure. It emphasizes that the “Value” can also be a “TLV” structure, where the “Type” and “Length” are fixed in length, and the length of “Value” is determined by the input value of “Length”. The TLV coding structure is simple, clear, and scalable. However, since it requires the input of “Type” and “Length”, there is extra memory overhead incurred. It wastes considerable memory especially when most fields are in base types.\nThe performance of serialization and deserialization can be optimized from two dimensions - “time” \u0026 “space”. To be compatible with the existing “binary” protocols, optimization in “space” seems to be infeasible. Improvement can only be made in “time”, it includes:\n Reduce the frequency of operations on memory, notably memory allocation and copying. Try to pre-allocate memory to reduce unnecessary time consumption. Reduce the frequency of function calls by adjusting code structure or using “inline” etc.  Research on Serialization Strategy Based on “go_serialization_benchmarks”, we investigated a number of serialization schemes that performed well to guide the optimization of our serialization strategy.\nAnalysis of “protobuf”, “gogoprotobuf”, and “Cap ‘n Proto” has provided us the following results:\n Considering I/O, the transmitted data is usually compressed in size during network transmission. “protobuf” uses “Varint” encoding and has good data compression capabilities in most scenarios. “gogoprotobuf” uses precomputation to reduce memory allocations and copies during serialization. Thus, it eliminates the cost of system calls, locks and GC arisen from memory allocations. “Cap ‘n Proto” directly operates buffer, which also reduces memory allocations and copies. In addition, it also designs “struct pointer” in a way that processes fixed-length data and non-fixed-length data separately, which enables fast processing for fixed-length data.  For compatibility reasons, it is impossible to change the existing “TLV” encoding format, so data compression is not feasible. But finding 2 and 3 are inspiring to our optimization work, and in fact we have taken a similar approach.\nApproaches Reducing Operations on Memory Buffer management\nBoth serialization and deserialization involve copying data from one piece of memory to another. It involves memory allocation and memory copying. Avoiding memory operations can reduce unnecessary overhead such as system calls, locks, and GC.\nIn fact, Kitex has provided “LinkBuffer” for buffer management purposes. “LinkBuffer” is designed with a linked structure and consists of multiple blocks, among which blocks are memory chunks with a fixed size. Object pool is constructed to maintain unoccupied block and support block multiplexing, thus, reduce memory usage and GC.\nInitially we simply used “sync.Pool” to multiplex the “LinkBufferNode” of netpoll, but it didn’t significantly contribute to multiplexing in large data package scenarios (large nodes can’t be reclaimed or it would cause memory leaking). At present, we have changed our strategy to maintain a group of “sync.Pool”, and the buffer size in each chunk is different. When new blocks are created, it is obtained from the pool with the closest size to the required size, so that the memory can be multiplexed as much as possible. And the test results also proved that it contributed to significant improvement in terms of memory allocation and GC.\nCopy-free String / Binary\nFor some services, such as video-related services, during its request or return processes, large-size “Binary” data will be arisen, representing the processed video or image data. Meanwhile, some services will return large-size “String” data (such as full-text information, etc.). In this scenario, all the hot spots we see through the flame graph are on the copies of the data. So we thought, can we reduce the frequency of such copies?\nThe answer is positive. Since our underlying buffer is a linked list, it is easy to insert a node in the middle of the list.\nThus, we have taken a similar approach, when a “string” or “binary” data exists during serialization processes. First, split the node’s buffer into two segments and then insert the buffer of the “string” / “binary” objects in the middle correspondingly. This avoids the copy of large “string” / “binary” .\nFurthermore, a copy will occur if we convert a string to “[]byte” using “[]byte(string)”. Because “string” is immutable and “[]byte” is mutable in Golang language. “unsafe” is needed if you don’t want to copy during the conversion:\nfunc StringToSliceByte(s string) []byte { l := len(s) return *(*[]byte)(unsafe.Pointer(\u0026reflect.SliceHeader{ Data: (*(*reflect.StringHeader)(unsafe.Pointer(\u0026s))).Data, Len: l, Cap: l, })) } The meaning of this demonstrated code is to take the address of the string first, and then give it a slice byte header, so that the “string” can be converted into “[]byte” without copying the data. Note that the resulting “[]byte” is not writable, or the behavior is undefined.\nPre-Calculation\nSome services support transmissions of large data package, which incurs considerable serialization / deserialization overhead. Generally, large packages are associated with the large size of the container type. If the buffer can be pre-calculated, some O(n) operations can be reduced to O(1), and further reduce the frequency of function calls. In the case of large data packages, the number of memory allocation can also be greatly reduced, bringing considerable benefits.\n  Base types\n If the container element is defined as base type (bool, byte, i16, i32, i64, double), the total size can be pre-calculated during serialization as its size is fixed, and enough buffer can be allocated at once. The number of “malloc” operations of O(n) can be reduced to O(1), thus greatly reducing the frequency of “malloc” operations. Similarly, the number of “next” operations can be reduced during deserialization.    Rearrangement of “Struct” Fields\n The above optimizations are valid only for container elements that are defined as base types. Can they be optimized for “struct” elements? The answer is yes. If there are fields of base type in “struct”, we can pre-calculate the size of these fields, then allocate buffer for these fields in advance during serialization and write these fields in the first order. We can also reduce the frequency of “malloc”.    Size calculation\n The optimization mentioned above is for base types. If you first iterate over all the fields of the request during serialization, you can calculate the size of the entire request, allocate buffer in advance, and directly manipulate buffer during serialization and deserialization, so that the optimization effect can be achieved for non-base types. Define a new “codec” interface:    type thriftMsgFastCodec interface { BLength() int // count length of whole req/resp  FastWrite(buf []byte) int FastRead(buf []byte) (int, error) }  Change the “Marshal” and “Unmarshal” interfaces accordingly:  func (c thriftCodec) Marshal(ctx context.Context, message remote.Message, out remote.ByteBuffer) error { ... if msg, ok := data.(thriftMsgFastCodec); ok { msgBeginLen := bthrift.Binary.MessageBeginLength(methodName, thrift.TMessageType(msgType), int32(seqID)) msgEndLen := bthrift.Binary.MessageEndLength() buf, err := out.Malloc(msgBeginLen + msg.BLength() + msgEndLen)// malloc once  if err != nil { return perrors.NewProtocolErrorWithMsg(fmt.Sprintf(\"thrift marshal, Malloc failed: %s\", err.Error())) } offset := bthrift.Binary.WriteMessageBegin(buf, methodName, thrift.TMessageType(msgType), int32(seqID)) offset += msg.FastWrite(buf[offset:]) bthrift.Binary.WriteMessageEnd(buf[offset:]) return nil } ... } func (c thriftCodec) Unmarshal(ctx context.Context, message remote.Message, in remote.ByteBuffer) error { ... data := message.Data() if msg, ok := data.(thriftMsgFastCodec); ok \u0026\u0026 message.PayloadLen() != 0 { msgBeginLen := bthrift.Binary.MessageBeginLength(methodName, msgType, seqID) buf, err := tProt.next(message.PayloadLen() - msgBeginLen - bthrift.Binary.MessageEndLength()) // next once  if err != nil { return remote.NewTransError(remote.PROTOCOL_ERROR, err.Error()) } _, err = msg.FastRead(buf) if err != nil { return remote.NewTransError(remote.PROTOCOL_ERROR, err.Error()) } err = tProt.ReadMessageEnd() if err != nil { return remote.NewTransError(remote.PROTOCOL_ERROR, err.Error()) } tProt.Recycle() return err } ... }  Modify the generated code accordingly:  func (p *Demo) BLength() int { l := 0 l += bthrift.Binary.StructBeginLength(\"Demo\") if p != nil { l += p.field1Length() l += p.field2Length() l += p.field3Length() ... } l += bthrift.Binary.FieldStopLength() l += bthrift.Binary.StructEndLength() return l } func (p *Demo) FastWrite(buf []byte) int { offset := 0 offset += bthrift.Binary.WriteStructBegin(buf[offset:], \"Demo\") if p != nil { offset += p.fastWriteField2(buf[offset:]) offset += p.fastWriteField4(buf[offset:]) offset += p.fastWriteField1(buf[offset:]) offset += p.fastWriteField3(buf[offset:]) } offset += bthrift.Binary.WriteFieldStop(buf[offset:]) offset += bthrift.Binary.WriteStructEnd(buf[offset:]) return offset } Optimizing Thrift Encoding with SIMD “list\u003ci64/i32\u003e” is widely used in the company to carry the ID list, and the encoding method of “list\u003ci64/i32\u003e” is highly consistent with the rule of vectorization. Thus, we use SIMD to optimize the encoding process of list\u003ci64/i32\u003e.\nWe implement “avx2” to improve the encoding process, and the improved results are significant. When dealing with large amounts of data, the performance can be improved by 6 times for i64 and 12 times for i32. In the case of small data volume, the improvement is more obvious, which achieves 10 times for i64 and 20 times for I32.\nReducing Function Calls inline\nThe purpose of “inline” is to expand a function call during its compilation and replace it with the implementation of the function. It improves program performance by reducing the overhead of the function call.\n“inline” can’t be implemented on all functions in Go. Run the process with the argument - (gflags=\"-m\") to display the functions that are inlined. The following conditions cannot be inlined:\n A function containing a loop; Functions that include: closure calls, select, for, defer, coroutines created by the go keyword; For Functions over a certain length, by default when parsing the AST, Go applies 80 nodes. Each node consumes one unit of inline budget. For example, a = a + 1 contains five nodes: AS, NAME, ADD, NAME, LITERAL. When the overhead of a function exceeds this budget, it cannot be inlined.  You can specify the intensity (go 1.9+) of the compiler’s inlined code by specifying “-l” at compile time. But it is not recommended, as in our test scenario, it is buggy and does not work:\n// The debug['l'] flag controls the aggressiveness. Note that main() swaps level 0 and 1, making 1 the default and -l disable. Additional levels (beyond -l) may be buggy and are not supported. // 0: disabled // 1: 80-nodes leaf functions, oneliners, panic, lazy typechecking (default) // 2: (unassigned) // 3: (unassigned) // 4: allow non-leaf functions Although using “inline” can reduce the overhead of function calls, it may also lead to lower CPU cache hit rate due to code redundancy. Therefore, excessive usage of “inline” should not be blindly pursued, and specific analysis should be carried out based on “profile” results.\ngo test -gcflags='-m=2' -v -test.run TestNewCodec 2\u003e\u00261 | grep \"function too complex\" | wc -l 48 go test -gcflags='-m=2 -l=4' -v -test.run TestNewCodec 2\u003e\u00261 | grep \"function too complex\" | wc -l 25 As you can see, from the output above, increasing the inline intensity does reduce the “function too complex”. Following are the benchmark results:\n   Benchmark time/op bytes/op allocs/op     BenchmarkOldMarshal-4 309 µs ± 2% 218KB 11   BenchmarkNewMarshal-4 310 µs ± 3% 218KB 11    It reveals that turning on the highest level of inlining intensity does eliminate many functions that cannot be inlined due to “function too complex”, but the test results show that the improvement is insignificant.\nTesting Results We built benchmarks to compare performance before and after optimization, and here are the results. Testing Enviornment: Go 1.13.5 darwin/amd64 on a 2.5 GHz Intel Core i7 16GB\nSmall Data Size\nData size: 20KB\n   Benchmark time/op bytes/op allocs/op     BenchmarkOldMarshal-4 138 µs ± 3% 25.4KB 19   BenchmarkNewMarshal-4 29 µs ± 3% 26.4KB 11   Marshal Delta -78.97% 3.87% -42.11%   BenchmarkOldUnmarshal-4 199 µs ± 3% 4720 1360   BenchmarkNewUnmarshal-4 94µs ± 5% 4700 1280   Unmarshal Delta -52.93% -0.24% -5.38%    Large Data Size\nData size: 6MB\n   Benchmark time/op bytes/op allocs/op     BenchmarkOldMarshal-4 58.7ms ± 5% 6.96MB 3350   BenchmarkNewMarshal-4 13.3ms ± 3% 6.84MB 10   Marshal Delta -77.30% -1.71% -99.64%   BenchmarkOldUnmarshal-4 56.6ms ± 3% 17.4MB 391000   BenchmarkNewUnmarshal-4 26.8ms ± 5% 17.5MB 390000   Unmarshal Delta -52.54% 0.09% -0.37%    Copy-free Serialization In some services with large request and response data, the cost of serialization and deserialization is high. There are two ways for optimization:\n Implement the optimization strategy on serialization and deserialization as described earlier. Scheduling by copy-free serialization.  Research on Copy-free Serilization RPC through copy-free serialization, which originated from the “Cap ‘n Proto” project of Kenton Varda. “Cap ‘n Proto” provides a set of data exchange formats and corresponding codec libraries.\nIn essence, “Cap ‘n Proto” creates a bytes slice as buffer, and all read \u0026 write operations on data structures are directly operated on buffer. After reading \u0026 writing, information contained by the buffer is added to the head and can be sent directly. And the peer end can read it after receiving it. Since there is no Go structure as an intermediate storage, serialization and deserialization are not required.\nTo briefly summarize the characteristics of “Cap ‘n Proto”:\n All data is read and written to a contiguous memory. The serialization operation is preceded. “Get/Set” data and encoding process in parallel. In the data exchange format, pointer (“offset” at the data memory) mechanism is used to store data at any location in the contiguous memory, so that data in the structure can be read and written in any order.  Fixed-size fields of a structure are rearranged so that they are stored in contiguous memory. Fields with indeterminate size of a structure (e.g. list), are represented by a fixed-size pointer that stores information including the location of the data.    First of all, “Cap ‘n Proto” has no Go language structure as an intermediate carrier, which can reduce a copy. Then, “Cap ‘n Proto” operates on a contiguous memory, and the read and write of coded data can be completed at once. Because of these two reasons, Cap ‘n Proto has excellent performance.\nHere are the benchmarks of “Thrift” and “Cap ‘n Proto” for the same data structure. Considering that “Cap ‘n Proto” presets the codec operation, we compare the complete process including data initialization. That is, structure data initialization + (serialization) + write buffer + read from buffer + (deserialization) + read from structure.\nstructMyTest{1:i64Num,2:AnoAno,3:list\u003ci64\u003eNums,// 长度131072 大小1MB }structAno{1:i64Num,}   Benchmark Iter time/op bytes/op alloc/op     BenchmarkThriftReadWrite 172 6855840 ns/op 3154209 B/op 545 allocs/op   BenchmarkCapnpReadWrite 1500 844924 ns/op 2085713 B/op 9 allocs/op   ReadWrite Delta / -87.68% -33.88% -98.35%    (deserialization) + read data, depending on the size of data package,“Cap ‘n Proto” performance is about 8-9 times better than “Thrift”. Write data + (serialization), depending on the size of data package, “Cap ‘n Proto” performance is approximately 2-8 times better than “Thrift”. Overall performance of “Cap ‘n Proto” is approximately 4-8 times better than “Thrift”.\nPreviously, we discussed the advantages of “Cap ‘n Proto”. We will then summarize some problems existing in “Cap ‘n Proto”:\n One problem with the contiguous memory of Cap ‘n Proto is that when the data of variable size is resized, and the required space is larger than the original space, the space of the original data can only be reallocated later. As a result, the original space becomes a hole that cannot be removed. This problem gets worse as the call link is resized, and can only be solved with strict constraints throughout the link: avoid resizing variable size fields, and when resize is necessary, rebuild a structure and make a deep copy of the data. “Cap ‘n Proto” has no Go language structure as an intermediate carrier, so all fields can only be read and written through the interface, resulting in poor user experience.  Thrift Protocol‘s Compatible Copy-free Serialization In order to support copy-free serialization better and more efficiently, “Cap ‘n Proto” uses a self-developed codec format, but it is difficult to be implemented in the current environment where “Thrift” and “ProtoBuf” are dominant. In order to achieve the performance of copy-free serialization with protocol compatibility, we started the exploration of copy-free serialization that is compatible with “Thrift” protocol.\n“Cap ‘n Proto” is a benchmark for copy-free serialization, so let’s see if the optimizations on “Cap ‘n Proto” can be applied to Thrift:\n Nature is the core of copy-free serialization, which does not use Go structure as intermediate carriers to reduce one copy. This optimization is not about a particular protocol and can be applied to any existing protocol (So it’s naturally compatible with the Thrift protocol), but the user experience of “Cap ‘n Proto” reflects that it needs to be carefully polished. “Cap ‘n Proto” is operated on a contiguous memory. The read \u0026 write of the encoded data can be completed at once. “Cap ‘n Proto” can operate on contiguous memory because there is a pointer mechanism that allows data to be stored anywhere, allowing fields to be written in any order without affecting decoding. However, it is very likely to leave a hole in the resize due to misoperation on contiguous memory. Besides, “Thrift” has no pointer alike mechanism, so it has stricter requirements on data layout. Here are two ways to approach such problems:  Insist on operating in contiguous memory, while imposing strict regulations on users’ usage: 1. Resize operation must rebuild the data structure; 2. When a structure is nested, there are strict requirements on the order in which the fields are written (we can think of it as unfolding a nested structure from the outside in, and being written in the same order) . In addition, due to TLV encoding such as Binary, when writing begins for each nesting, it requires declaration (such as “StartWriteFieldX”). Operating not entirely in contiguous memory, alterable fields are allocated a separate piece of memory. Since memory is not completely contiguous, the write operation can’t complete the output at once. In order to get closer to the performance of writing data at once, we adopted a linked buffer scheme. On the one hand, when the variable field resize occurs, only one node of the linked buffer is replaced, and there is no need to reconstruct the structure like “Cap ‘n Proto”. On the other hand, there is no need to clarify the actual structure like “Thrift” when the output is needed, just write the buffer on the link.    To summarize what we have determined previously: 1. Do not use Go structure as the intermediate carrier, directly operate the underlying memory through the interface, and complete the codec at the same time of “Get/Set”. 2. Data is stored through a linked buffer.\nThen let’s take a look at the remaining issues:\n Degradation of the user experience caused by not using Go structures.  Solution: Improve the user experience of “Get/Set” interface and make it as easy to use as the Go structure.   Binary Format of “Cap ‘n Proto” is designed specifically for copy-free serialization scenarios, and although decoding is performed once for every Get, the decoding costs are minimal. The “Thrift” protocol (taking “Binary” as an example) has no mechanism that is similar to pointer. When there are multiple fields of variable size or nesting, they must be resolved sequentially instead of directly calculating the offset to get the field data location. Moreover, the cost of sequential resolution for each Get is too high.  Solution: In addition to recording the structure’s buffer nodes, we also add an index that records the pointer to the buffer node at the beginning of each field with unfixed size. The following is the ultimate performance comparison test between the current copy-free serialization scheme and “FastRead/Write” under the condition of 4 cores:       Package Size Type QPS TP90 TP99 TP999 CPU     1KB Non-serialization 70,700 1 ms 3 ms 6 ms /    FastWrite/FastRead 82,490 1 ms 2 ms 4 ms /   2KB Non-serialization 65,000 1 ms 4 ms 9 ms /    FastWrite/FastRead 72,000 1 ms 2 ms 8 ms /   4KB Non-serialization 56,400 2 ms 5 ms 10 ms 380%    FastWrite/FastRead 52,700 2 ms 4 ms 10 ms 380%   32KB Non-serialization 27,400 / / / /    FastWrite/FastRead 19,500 / / / /   1MB Non-serialization 986 53 ms 56 ms 59 ms 260%    FastWrite/FastRead 942 55 ms 59 ms 62 ms 290%   10MB Non-serialization 82 630 ms 640 ms 645 ms 240%    FastWrite/FastRead 82 630 ms 640 ms 640 ms 270    Summary of the test results:\n In small data package scenario, performance of non-serialization is poorer - about 85% of FastWrite/FastRead’s performance. In large data package scenario, the performance of non-serialization is better. When processing packages larger than 4K, the performance of non-serialization is 7%-40% better compared with “FastWrite/FastRead”.  Postscript Hope the above sharing can be helpful to the community. At the same time, we are trying to share memory-based IPC, io_uring, TCP zero copy, RDMA, etc., to better improve Kitex performance. And we will also focus on improving the communication scenarios of the same device and container. Welcome to join us and contribute to Go ecology together!\nReference  https://github.com/alecthomas/go_serialization_benchmarks https://capnproto.org/ Intel C++ Compiler Classic Developer Guide and Reference  ","categories":"","description":"","excerpt":"Preface Kitex is the next generation high-performance and extensible …","ref":"/blog/2021/09/23/performance-optimization-on-kitex/","tags":"","title":"Performance Optimization on Kitex"},{"body":"前言 Kitex 是字节跳动框架组研发的下一代高性能、强可扩展性的 Go RPC 框架。除具备丰富的服务治理特性外，相比其他框架还有以下特点：集成了自研的网络库 Netpoll；支持多消息协议（Thrift、Protobuf）和多交互方式（Ping-Pong、Oneway、 Streaming）；提供了更加灵活可扩展的代码生成器。\n目前公司内主要业务线都已经大范围使用 Kitex，据统计当前接入服务数量多达 8k。Kitex 推出后，我们一直在不断地优化性能，本文将分享我们在 Netpoll 和 序列化方面的优化工作。\n自研网络库 Netpoll 优化 自研的基于 epoll 的网络库 —— Netpoll，在性能方面有了较为显著的优化。测试数据表明，当前版本(2020.12) 相比于上次分享时(2020.05)，吞吐能力 ↑30%，延迟 AVG ↓25%，TP99 ↓67%，性能已远超官方 net 库。以下，我们将分享两点显著提升性能的方案。\nepoll_wait 调度延迟优化 Netpoll 在刚发布时，遇到了延迟 AVG 较低，但 TP99 较高的问题。经过认真研究 epoll_wait，我们发现结合 polling 和 event trigger 两种模式，并优化调度策略，可以显著降低延迟。\n首先我们来看 Go 官方提供的 syscall.EpollWait 方法：\nfunc EpollWait(epfd int, events []EpollEvent, msec int) (n int, err error) 这里共提供 3 个参数，分别表示 epoll 的 fd、回调事件、等待时间，其中只有 msec 是动态可调的。\n通常情况下，我们主动调用 EpollWait 都会设置 msec=-1，即无限等待事件到来。事实上不少开源网络库也是这么做的。但是我们研究发现，msec=-1 并不是最优解。\nepoll_wait 内核源码(如下) 表明，msec=-1 比 msec=0 增加了 fetch_events 检查，因此耗时更长。\nstatic int ep_poll(struct eventpoll *ep, struct epoll_event __user *events, int maxevents, long timeout) { ... if (timeout \u003e 0) { ... } else if (timeout == 0) { ... goto send_events; } fetch_events: ... if (eavail) goto send_events; send_events: ... Benchmark 表明，在有事件触发的情况下，msec=0 比 msec=-1 调用要快 18% 左右，因此在频繁事件触发场景下，使用 msec=0 调用明显是更优的。\n   Benchmark time/op bytes/op     BenchmarkEpollWait, msec=0 270 ns/op 0 B/op   BenchmarkEpollWait, msec=-1 328 ns/op 0 B/op   EpollWait Delta -17.68% ~    而在无事件触发的场景下，使用 msec=0 显然会造成无限轮询，空耗大量资源。\n综合考虑后，我们更希望在有事件触发时，使用 msec=0 调用，而在无事件时，使用 msec=-1 来减少轮询开销。伪代码如下：\nvar msec = -1 for { n, err = syscall.EpollWait(epfd, events, msec) if n \u003c= 0 { msec = -1 continue } msec = 0 ... } 那么这样就可以了吗？事实证明优化效果并不明显。\n我们再做思考：\nmsec=0 仅单次调用耗时减少 50ns，影响太小，如果想要进一步优化，必须要在调度逻辑上做出调整。\n进一步思考：\n上述伪代码中，当无事件触发，调整 msec=-1 时，直接 continue 会立即再次执行 EpollWait，而由于无事件，msec=-1，当前 goroutine 会 block 并被 P 切换。但是被动切换效率较低，如果我们在 continue 前主动为 P 切换 goroutine，则可以节约时间。因此我们将上述伪代码改为如下：\nvar msec = -1 for { n, err = syscall.EpollWait(epfd, events, msec) if n \u003c= 0 { msec = -1 runtime.Gosched() continue } msec = 0 ... } 测试表明，调整代码后，吞吐量 ↑12%，TP99 ↓64%，获得了显著的延迟收益。\n合理利用 unsafe.Pointer 继续研究 epoll_wait，我们发现 Go 官方对外提供的 syscall.EpollWait 和 runtime 自用的 epollwait 是不同的版本，即两者使用了不同的 EpollEvent。以下我们展示两者的区别：\n// @syscall type EpollEvent struct { Events uint32 Fd int32 Pad int32 } // @runtime type epollevent struct { events uint32 data [8]byte // unaligned uintptr } 我们看到，runtime 使用的 epollevent 是系统层 epoll 定义的原始结构；而对外版本则对其做了封装，将 epoll_data(epollevent.data) 拆分为固定的两字段：Fd 和 Pad。那么 runtime 又是如何使用的呢？在源码里我们看到这样的逻辑：\n*(**pollDesc)(unsafe.Pointer(\u0026ev.data)) = pd pd := *(**pollDesc)(unsafe.Pointer(\u0026ev.data)) 显然，runtime 使用 epoll_data(\u0026ev.data) 直接存储了 fd 对应结构体(pollDesc)的指针，这样在事件触发时，可以直接找到结构体对象，并执行相应逻辑。而对外版本则由于只能获得封装后的 Fd 参数，因此需要引入额外的 Map 来增删改查结构体对象，这样性能肯定相差很多。\n所以我们果断抛弃了 syscall.EpollWait，转而仿照 runtime 自行设计了 EpollWait 调用，同样采用 unsafe.Pointer 存取结构体对象。测试表明，该方案下 吞吐量 ↑10%，TP99 ↓10%，获得了较为明显的收益。\nThrift 序列化/反序列化优化 序列化是指把数据结构或对象转换成字节序列的过程，反序列化则是相反的过程。RPC 在通信时需要约定好序列化协议，client 在发送请求前进行序列化，字节序列通过网络传输到 server，server 再反序列进行逻辑处理，完成一次 RPC 请求。Thrift 支持 Binary、Compact 和 JSON 序列化协议。目前公司内部使用的基本都是 Binary，这里只介绍 Binary 协议。\nBinary 采用 TLV 编码实现，即每个字段都由 TLV 结构来描述，TLV 意为：Type 类型， Lenght 长度，Value 值，Value 也可以是个 TLV 结构，其中 Type 和 Length 的长度固定，Value 的长度则由 Length 的值决定。TLV 编码结构简单清晰，并且扩展性较好，但是由于增加了 Type 和 Length，有额外的内存开销，特别是在大部分字段都是基本类型的情况下有不小的空间浪费。\n序列化和反序列的性能优化从大的方面来看可以从空间和时间两个维度进行优化。从兼容已有的 Binary 协议来看，空间上的优化似乎不太可行，只能从时间维度进行优化，包括：\n  减少内存操作次数，包括内存分配和拷贝，尽量预分配内存，减少不必要的开销；\n  减少函数调用次数，比如可调整代码结构和 inline 等手段进行优化；\n  调研 根据 go_serialization_benchmarks 的压测数据，我们找到了一些性能卓越的序列化方案进行调研，希望能够对我们的优化工作有所启发。\n通过对 protobuf、gogoprotobuf 和 Cap’n Proto 的分析，我们得出以下结论：\n  网络传输中出于 IO 的考虑，都会尽量压缩传输数据，protobuf 采用了 Varint 编码在大部分场景中都有着不错的压缩效果；\n  gogoprotobuf 采用预计算方式，在序列化时能够减少内存分配次数，进而减少了内存分配带来的系统调用、锁和 GC 等代价；\n  Cap’n Proto 直接操作 buffer，也是减少了内存分配和内存拷贝（少了中间的数据结构），并且在 struct pointer 的设计中把固定长度类型数据和非固定长度类型数据分开处理，针对固定长度类型可以快速处理；\n  从兼容性考虑，不可能改变现有的 TLV 编码格式，因此数据压缩不太现实，但是 2 和 3 对我们的优化工作是有启发的，事实上我们也是采取了类似的思路。\n思路 减少内存操作 buffer 管理\n无论是序列化还是反序列化，都是从一块内存拷贝数据到另一块内存，这就涉及到内存分配和内存拷贝操作，尽量避免内存操作可以减少不必要的系统调用、锁和 GC 等开销。\n事实上 Kitex 已经提供了 LinkBuffer 用于 buffer 的管理，LinkBuffer 设计上采用链式结构，由多个 block 组成，其中 block 是大小固定的内存块，构建对象池维护空闲 block，由此复用 block，减少内存占用和 GC。\n刚开始我们简单地采用 sync.Pool 来复用 netpoll 的 LinkBufferNode，但是这样仍然无法解决对于大包场景下的内存复用（大的 Node 不能回收，否则会导致内存泄漏）。目前我们改成了维护一组 sync.Pool，每组中的 buffer size 都不同，新建 block 时根据最接近所需 size 的 pool 中去获取，这样可以尽可能复用内存，从测试来看内存分配和 GC 优化效果明显。\nstring / binary 零拷贝\n对于有一些业务，比如视频相关的业务，会在请求或者返回中有一个很大的 Binary 二进制数据代表了处理后的视频或者图片数据，同时会有一些业务会返回很大的 String（如全文信息等）。这种场景下，我们通过火焰图看到的热点都在数据的 copy 上，那我们就想了，我们是否可以减少这种拷贝呢？\n答案是肯定的。既然我们底层使用的 Buffer 是个链表，那么就可以很容易地在链表中间插入一个节点。\n我们就采用了类似的思想，当序列化的过程中遇到了 string 或者 binary 的时候， 将这个节点的 buffer 分成两段，在中间原地插入用户的 string / binary 对应的 buffer，这样可以避免大的 string / binary 的拷贝了。\n这里再介绍一下，如果我们直接用 []byte(string) 去转换一个 string 到 []byte 的话实际上是会发生一次拷贝的，原因是 Go 的设计中 string 是 immutable 的但是 []byte 是 mutable 的，所以这么转换的时候会拷贝一次；如果要不拷贝转换的话，就需要用到 unsafe 了：\nfunc StringToSliceByte(s string) []byte { l := len(s) return *(*[]byte)(unsafe.Pointer(\u0026reflect.SliceHeader{ Data: (*(*reflect.StringHeader)(unsafe.Pointer(\u0026s))).Data, Len: l, Cap: l, })) } 这段代码的意思是，先把 string 的地址拿到，再拼装上一个 slice byte 的 header，这样就可以不拷贝数据而将 string 转换成 []byte 了，不过要注意这样生成的 []byte 不可写，否则行为未定义。\n预计算\n线上存在某些服务有大包传输的场景，这种场景下会引入不小的序列化 / 反序列化开销。一般大包都是容器类型的大小非常大导致的，如果能够提前计算出 buffer，一些 O(n) 的操作就能降到 O(1)，减少了函数调用次数，在大包场景下也大量减少了内存分配的次数，带来的收益是可观的。\n  基本类型\n 如果容器元素为基本类型（bool, byte, i16, i32, i64, double）的话，由于基本类型大小固定，在序列化时是可以提前计算出总的大小，并且一次性分配足够的 buffer，O(n) 的 malloc 操作次数可以降到 O(1)，从而大量减少了 malloc 的次数，同理在反序列化时可以减少 next 的操作次数。    struct 字段重排\n  上面的优化只能针对容器元素类型为基本类型的有效，那么对于元素类型为 struct 的是否也能优化呢？答案是肯定的。\n  沿用上面的思路，假如 struct 中如果存在基本类型的 field，也可以预先计算出这些 field 的大小，在序列化时为这些 field 提前分配 buffer，写的时候也把这些 field 顺序统一放到前面写，这样也能在一定程度上减少 malloc 的次数。\n    一次性计算\n 上面提到的是基本类型的优化，如果在序列化时，先遍历一遍 request 所有 field，便可以计算得到整个 request 的大小，提前分配好 buffer，在序列化和反序列时直接操作 buffer，这样对于非基本类型也能有优化效果。    定义新的 codec 接口：\n  type thriftMsgFastCodec interface { BLength() int // count length of whole req/resp  FastWrite(buf []byte) int FastRead(buf []byte) (int, error) }  在 Marshal 和 Unmarshal 接口中做相应改造：  func (c thriftCodec) Marshal(ctx context.Context, message remote.Message, out remote.ByteBuffer) error { ... if msg, ok := data.(thriftMsgFastCodec); ok { msgBeginLen := bthrift.Binary.MessageBeginLength(methodName, thrift.TMessageType(msgType), int32(seqID)) msgEndLen := bthrift.Binary.MessageEndLength() buf, err := out.Malloc(msgBeginLen + msg.BLength() + msgEndLen)// malloc once  if err != nil { return perrors.NewProtocolErrorWithMsg(fmt.Sprintf(\"thrift marshal, Malloc failed: %s\", err.Error())) } offset := bthrift.Binary.WriteMessageBegin(buf, methodName, thrift.TMessageType(msgType), int32(seqID)) offset += msg.FastWrite(buf[offset:]) bthrift.Binary.WriteMessageEnd(buf[offset:]) return nil } ... } func (c thriftCodec) Unmarshal(ctx context.Context, message remote.Message, in remote.ByteBuffer) error { ... data := message.Data() if msg, ok := data.(thriftMsgFastCodec); ok \u0026\u0026 message.PayloadLen() != 0 { msgBeginLen := bthrift.Binary.MessageBeginLength(methodName, msgType, seqID) buf, err := tProt.next(message.PayloadLen() - msgBeginLen - bthrift.Binary.MessageEndLength()) // next once  if err != nil { return remote.NewTransError(remote.PROTOCOL_ERROR, err.Error()) } _, err = msg.FastRead(buf) if err != nil { return remote.NewTransError(remote.PROTOCOL_ERROR, err.Error()) } err = tProt.ReadMessageEnd() if err != nil { return remote.NewTransError(remote.PROTOCOL_ERROR, err.Error()) } tProt.Recycle() return err } ... }  生成代码中也做相应改造：  func (p *Demo) BLength() int { l := 0 l += bthrift.Binary.StructBeginLength(\"Demo\") if p != nil { l += p.field1Length() l += p.field2Length() l += p.field3Length() ... } l += bthrift.Binary.FieldStopLength() l += bthrift.Binary.StructEndLength() return l } func (p *Demo) FastWrite(buf []byte) int { offset := 0 offset += bthrift.Binary.WriteStructBegin(buf[offset:], \"Demo\") if p != nil { offset += p.fastWriteField2(buf[offset:]) offset += p.fastWriteField4(buf[offset:]) offset += p.fastWriteField1(buf[offset:]) offset += p.fastWriteField3(buf[offset:]) } offset += bthrift.Binary.WriteFieldStop(buf[offset:]) offset += bthrift.Binary.WriteStructEnd(buf[offset:]) return offset } 使用 SIMD 优化 Thrift 编码 公司内广泛使用 list\u003ci64/i32\u003e 类型来承载 ID 列表，并且 list\u003ci64/i32\u003e 的编码方式十分符合向量化的规律，于是我们用了 SIMD 来优化 list\u003ci64/i32\u003e 的编码过程。\n我们使用了 avx2，优化后的结果比较显著，在大数据量下针对 i64 可以提升 6 倍性能，针对 i32 可以提升 12 倍性能；在小数据量下提升更明显，针对 i64 可以提升 10 倍，针对 i32 可以提升 20 倍。\n减少函数调用 inline\ninline 是在编译期间将一个函数调用原地展开，替换成这个函数的实现，它可以减少函数调用的开销以提高程序的性能。\n在 Go 中并不是所有函数都能 inline，使用参数-gflags=\"-m\"运行进程，可显示被 inline 的函数。以下几种情况无法内联：\n  包含循环的函数；\n  包含以下内容的函数：闭包调用，select，for，defer，go 关键字创建的协程；\n  超过一定长度的函数，默认情况下当解析 AST 时，Go 申请了 80 个节点作为内联的预算。每个节点都会消耗一个预算。比如，a = a + 1 这行代码包含了 5 个节点：AS, NAME, ADD, NAME, LITERAL。当一个函数的开销超过了这个预算，就无法内联。\n  编译时通过指定参数-l可以指定编译器对代码内联的强度（go 1.9+），不过这里不推荐大家使用，在我们的测试场景下是 buggy 的，无法正常运行：\n// The debug['l'] flag controls the aggressiveness. Note that main() swaps level 0 and 1, making 1 the default and -l disable. Additional levels (beyond -l) may be buggy and are not supported. // 0: disabled // 1: 80-nodes leaf functions, oneliners, panic, lazy typechecking (default) // 2: (unassigned) // 3: (unassigned) // 4: allow non-leaf functions 内联虽然可以减少函数调用的开销，但是也可能因为存在重复代码，从而导致 CPU 缓存命中率降低，所以并不能盲目追求过度的内联，需要结合 profile 结果来具体分析。\ngo test -gcflags='-m=2' -v -test.run TestNewCodec 2\u003e\u00261 | grep \"function too complex\" | wc -l 48 go test -gcflags='-m=2 -l=4' -v -test.run TestNewCodec 2\u003e\u00261 | grep \"function too complex\" | wc -l 25 从上面的输出结果可以看出，加强内联程度确实减少了一些\"function too complex\"，看下 benchmark 结果：\n   Benchmark time/op bytes/op allocs/op     BenchmarkOldMarshal-4 309 µs ± 2% 218KB 11   BenchmarkNewMarshal-4 310 µs ± 3% 218KB 11    上面开启最高程度的内联强度，确实消除了不少因为“function too complex”带来无法内联的函数，但是压测结果显示收益不太明显。\n测试结果 我们构建了基准测试来对比优化前后的性能，下面是测试结果。\n环境：Go 1.13.5 darwin/amd64 on a 2.5 GHz Intel Core i7 16GB\n小包\ndata size: 20KB\n   Benchmark time/op bytes/op allocs/op     BenchmarkOldMarshal-4 138 µs ± 3% 25.4KB 19   BenchmarkNewMarshal-4 29 µs ± 3% 26.4KB 11   Marshal Delta -78.97% 3.87% -42.11%   BenchmarkOldUnmarshal-4 199 µs ± 3% 4720 1360   BenchmarkNewUnmarshal-4 94µs ± 5% 4700 1280   Unmarshal Delta -52.93% -0.24% -5.38%    大包\ndata size: 6MB\n   Benchmark time/op bytes/op allocs/op     BenchmarkOldMarshal-4 58.7ms ± 5% 6.96MB 3350   BenchmarkNewMarshal-4 13.3ms ± 3% 6.84MB 10   Marshal Delta -77.30% -1.71% -99.64%   BenchmarkOldUnmarshal-4 56.6ms ± 3% 17.4MB 391000   BenchmarkNewUnmarshal-4 26.8ms ± 5% 17.5MB 390000   Unmarshal Delta -52.54% 0.09% -0.37%    无拷贝序列化 在一些 request 和 response 数据较大的服务中，序列化和反序列化的代价较高，有两种优化思路：\n  如前文所述进行序列化和反序列化的优化\n  以无拷贝序列化的方式进行调用\n  调研 通过无拷贝序列化进行 RPC 调用，最早出自 Kenton Varda 的 Cap’n Proto 项目，Cap’n Proto 提供了一套数据交换格式和对应的编解码库。\nCap’n Proto 本质上是开辟一个 bytes slice 作为 buffer ，所有对数据结构的读写操作都是直接读写 buffer，读写完成后，在头部添加一些 buffer 的信息就可以直接发送，对端收到后即可读取，因为没有 Go 语言结构体作为中间存储，所有无需序列化这个步骤，反序列化亦然。\n简单总结下 Cap’n Proto 的特点：\n  所有数据的读写都是在一段连续内存中\n  将序列化操作前置，在数据 Get/Set 的同时进行编解码\n  在数据交换格式中，通过 pointer（数据存储位置的 offset）机制，使得数据可以存储在连续内存的任意位置，进而使得结构体中的数据可以以任意顺序读写\n 对于结构体的固定大小字段，通过重新排列，使得这些字段存储在一块连续内存中 对于结构体的不定大小字段（如 list），则通过一个固定大小的 pointer 来表示，pointer 中存储了包括数据位置在内的一些信息    首先 Cap’n Proto 没有 Go 语言结构体作为中间载体，得以减少一次拷贝，然后 Cap’n Proto 是在一段连续内存上进行操作，编码数据的读写可以一次完成，因为这两个原因，使得 Cap' Proto 的性能表现优秀。\n下面是相同数据结构下 Thrift 和 Cap’n Proto 的 Benchmark，考虑到 Cap’n Proto 是将编解码操作前置了，所以对比的是包括数据初始化在内的完整过程，即结构体数据初始化+（序列化）+写入 buffer +从 buffer 读出+（反序列化）+从结构体读出数据。\nstructMyTest{1:i64Num,2:AnoAno,3:list\u003ci64\u003eNums,// 长度131072 大小1MB }structAno{1:i64Num,}   Benchmark Iter time/op bytes/op alloc/op     BenchmarkThriftReadWrite 172 6855840 ns/op 3154209 B/op 545 allocs/op   BenchmarkCapnpReadWrite 1500 844924 ns/op 2085713 B/op 9 allocs/op   ReadWrite Delta / -87.68% -33.88% -98.35%    （反序列化）+读出数据，视包大小，Cap’n Proto 性能大约是 Thrift 的 8-9 倍。写入数据+（序列化），视包大小，Cap’n Proto 性能大约是 Thrift 的 2-8 倍。整体性能 Cap' Proto 性能大约是 Thrift 的 4-8 倍。\n前面说了 Cap’n Proto 的优势，下面总结一下 Cap’n Proto 存在的一些问题：\n  Cap’n Proto 的连续内存存储这一特性带来的一个问题：当对不定大小数据进行 resize ，且需要的空间大于原有空间时，只能在后面重新分配一块空间，导致原来数据的空间成为了一个无法去掉的 hole 。这个问题随着调用链路的不断 resize 会越来越严重，要解决只能在整个链路上严格约束：尽量避免对不定大小字段的 resize ，当不得不 resize 的时候，重新构建一个结构体并对数据进行深拷贝。\n  Cap’n Proto 因为没有 Go 语言结构体作为中间载体，使得所有的字段都只能通过接口进行读写，用户体验较差。\n  Thrift 协议兼容的无拷贝序列化 Cap’n Proto 为了更好更高效地支持无拷贝序列化，使用了一套自研的编解码格式，但在现在 Thrift 和 ProtoBuf 占主流的环境中难以铺开。为了能在协议兼容的同时获得无拷贝序列化的性能，我们开始了 Thrift 协议兼容的无拷贝序列化的探索。\nCap’n Proto 作为无拷贝序列化的标杆，那么我们就看看 Cap’n Proto 上的优化能否应用到 Thrift 上：\n  自然是无拷贝序列化的核心，不使用 Go 语言结构体作为中间载体，减少一次拷贝。此优化点是协议无关的，能够适用于任何已有的协议，自然也能和 Thrift 协议兼容，但是从 Cap’n Proto 的使用上来看，用户体验还需要仔细打磨一下。\n  Cap’n Proto 是在一段连续内存上进行操作，编码数据的读写可以一次完成。Cap’n Proto 得以在连续内存上操作的原因：有 pointer 机制，数据可以存储在任意位置，允许字段可以以任意顺序写入而不影响解码。但是一方面，在连续内存上容易因为误操作，导致在 resize 的时候留下 hole，另一方面，Thrift 没有类似于 pointer 的机制，故而对数据布局有着更严格的要求。这里有两个思路：\n 坚持在连续内存上进行操作，并对用户使用提出严格要求：1. resize 操作必须重新构建数据结构 2. 当存在结构体嵌套时，对字段写入顺序有着严格要求（可以想象为把一个存在嵌套的结构体从外往里展开，写入时需要按展开顺序写入），且因为 Binary 等 TLV 编码的关系，在每个嵌套开始写入时，需要用户主动声明（如 StartWriteFieldX）。 不完全在连续内存上操作，局部内存连续，可变字段则单独分配一块内存，既然内存不是完全连续的，自然也无法做到一次写操作便完成输出。为了尽可能接近一次写完数据的性能，我们采取了一种链式 buffer 的方案，一方面当可变字段 resize 时只需替换链式 buffer 的一个节点，无需像 Cap’n Proto 一样重新构建结构体，另一方面在需要输出时无需像 Thrift 一样需要感知实际的结构，只要把整个链路上的 buffer 写入即可。    先总结下目前确定的两个点：1. 不使用 Go 语言结构体作为中间载体，通过接口直接操作底层内存，在 Get/Set 时完成编解码 2. 通过链式 buffer 存储数据\n然后让我们看下目前还有待解决的问题：\n  不使用 Go 语言结构体后带来的用户体验劣化\n 解决方案：改善 Get/Set 接口的使用体验，尽可能做到和 Go 语言结构体同等的易用    Cap’n Proto 的 Binary Format 是针对无拷贝序列化场景专门设计的，虽然每次 Get 时都会进行一次解码，但是解码代价非常小。而 Thrift 的协议（以 Binary 为例），没有类似于 pointer 的机制，当存在多个不定大小字段或者存在嵌套时，必须顺序解析而无法直接通过计算偏移拿到字段数据所在的位置，而每次 Get 都进行顺序解析的代价过于高昂。\n 解决方案：我们在表示结构体的时候，除了记录结构体的 buffer 节点，还加了一个索引，里面记录了每个不定大小字段开始的 buffer 节点的指针。下面是目前的无拷贝序列化方案与 FastRead/Write，在 4 核下的极限性能对比测试：       包大小 类型 QPS TP90 TP99 TP999 CPU     1KB 无序列化 70,700 1 ms 3 ms 6 ms /    FastWrite/FastRead 82,490 1 ms 2 ms 4 ms /   2KB 无序列化 65,000 1 ms 4 ms 9 ms /    FastWrite/FastRead 72,000 1 ms 2 ms 8 ms /   4KB 无序列化 56,400 2 ms 5 ms 10 ms 380%    FastWrite/FastRead 52,700 2 ms 4 ms 10 ms 380%   32KB 无序列化 27,400 / / / /    FastWrite/FastRead 19,500 / / / /   1MB 无序列化 986 53 ms 56 ms 59 ms 260%    FastWrite/FastRead 942 55 ms 59 ms 62 ms 290%   10MB 无序列化 82 630 ms 640 ms 645 ms 240%    FastWrite/FastRead 82 630 ms 640 ms 640 ms 270%    测试结果概述：\n  小包场景，无序列化性能表现较差，约为 FastWrite/FastRead 的 85%。\n  大包场景，无序列化性能表现较好，4K 以上的包较 FastWrite/FastRead 提升 7%-40%。\n  后记 希望以上的分享能够对社区有所帮助。同时，我们也在尝试 share memory-based IPC、io_uring、tcp zero copy 、RDMA 等，更好地提升 Kitex 性能；重点优化同机、同容器的通讯场景。欢迎各位感兴趣的同学加入我们，共同建设 Go 语言生态！\n参考资料   https://github.com/alecthomas/go_serialization_benchmarks\n  https://capnproto.org/\n  Intel® C++ Compiler Classic Developer Guide and Reference\n  ","categories":"","description":"","excerpt":"前言 Kitex 是字节跳动框架组研发的下一代高性能、强可扩展性的 Go RPC 框架。除具备丰富的服务治理特性外，相比其他框架还有以下特 …","ref":"/zh/blog/2021/09/23/%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8-go-rpc-%E6%A1%86%E6%9E%B6-kitex-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5/","tags":"","title":"字节跳动 Go RPC 框架 Kitex 性能优化实践"},{"body":"Improvement:  Support TCP_NODELAY by default Read \u0026\u0026 write in a single loop Return real error for nocopy rw Change default number of loops policy Redefine EventLoop.Serve arg: Listener -\u003e net.Listener Add API to DisableGopool Remove reading lock Blocking conn flush API  Bugfix:  Set leftover wait read size  ","categories":"","description":"","excerpt":"Improvement:  Support TCP_NODELAY by default Read \u0026\u0026 write in a single …","ref":"/blog/2021/09/16/netpoll-release-v0.0.4/","tags":"","title":"Netpoll Release v0.0.4"},{"body":"优化:  默认支持 TCP_NODELAY 支持在一个循环中读写 返回 nocopy rw 的真实错误 更改了循环策略的默认数量 重新定义了 EventLoop.Serve arg: Listener -\u003e net.Listener 在 DisableGopool 中增加了API 删除了读锁 连接 Flush API 调整为阻塞的  Bug 修复:  设置剩余待读取大小。  ","categories":"","description":"","excerpt":"优化:  默认支持 TCP_NODELAY 支持在一个循环中读写 返回 nocopy rw …","ref":"/zh/blog/2021/09/16/netpoll-v0.0.4-%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/","tags":"","title":"Netpoll v0.0.4 版本发布"},{"body":"Background ByteDance is proud to announce the launch of open source software CloudWeGo. Focusing on microservice communication and governance, it offers high performance, strong extensibility, and high reliability which enables quick construction of an enterprise-level cloud native architecture.\nByteDance uses Golang as its main development language, and supports the reliable communication of tens of thousands of Golang microservices. We are experienced in microservices after practicing under massive traffic, and so we decided to offer open source software in order to enrich the community’s ecology.\nWe have built the CloudWeGo project to gradually open source the internal microservices system and try to make the projects friendly to external users, and our internal projects will also use this open source project as a library for iterative development. CloudWeGo will follow a key principle of maintaining one set of code internally and externally, iterating them as a whole. As we needed to migrate our internal users to open source libraries transparently, we did not initially pursue any publicity. However, it has been gratifying to see Kitex gain 1.2k stars and Netpoll gain 700+ stars within one month organically.\nCloudWeGo is not only an external open source project, but also a real ultra-large-scale enterprise-level practice project.\nWe look forward to enriching the Golang product system of the cloud native community through CloudWeGo and helping other companies to build cloud-native architectures in a rapid and convenient way. We also hope to attract developers in the open source community, to maintain and improve this project together, provide support for multiple scenarios, and enrich product capabilities.\nBecause the projects under CloudWeGo depend on many internal basic tool libraries, we also open source the basic Golang tool libraries used internally, and maintain them in bytedance/gopkg.\nCloudWeGo To begin with, the two main projects included within CloudWeGo are the Kitex RPC framework and the Netpoll network library. We chose not to publicise these projects prematurely, to ensure our open source technologies were ready and had sufficient verification upon launch.\nKitex Kitex [kaɪt’eks] is a high-performance and strong-extensibility Golang RPC framework used in Bytedance. Before Kitex, the internal Golang framework was Kite, which was strongly coupled with Thrift - the code generation part of which covered intricate logic in the code.\nDue to these factors, it was difficult to optimize the framework from the network model or codec level.\nAdding new features will inevitably lead to more bloated code and would have hindered the iteration process. Instead we designed a new framework, Kitex, to address these concerns. Although Kitex is a new framework, it has been applied online internally for more than a year. At present, more than 50% of Golang microservices in Bytedance use Kitex.\nFeatures of Kitex include:\n High Performance  Kitex integrates Netpoll, a high-performance network library which offers significant performance advantage over go net. Kitex also makes some optimizations on the codec of Thrift, details of which can be found here. Users can also refer to this website for performance results.\n Extensibility  Kitex employs a modular design and provides many interfaces with default implementation for users to customize. Users can then extend or inject them into Kitex to fulfill their needs. Please refer to the official doc for the extensibility of Kitex. For their network library, developers can freely choose other network libraries aside from netpoll.\n Multi-message Protocol  Regarding the RPC message protocol, Kitex supports Thrift, Kitex Protobuf and gRPC by default. For Thrift, it supports two binary protocols, Buffered and Framed. Kitex Protobuf is a Kitex custom Protobuf messaging protocol with a protocol format similar to Thrift. The gRPC message protocol enables Kitex to interact with gRPC. Additionally, Kitex allows developers to extend their own messaging protocols.\n Multi-transport Protocol  The transport protocol encapsulates the message protocol for RPC communication and is able to transparently transmit meta-information used for service governance. Kitex supports two transport protocols, TTHeader and HTTP2. TTHeader can be used in conjunction with Thrift and Kitex Protobuf. At present, HTTP2 is mainly used in combination with the gRPC protocol, and will support Thrift in the future.\n Multi-message Type  Kitex supports PingPong, One-way, and Bidirectional Streaming. Among them, One-way currently only supports Thrift protocol, two-way Streaming only supports gRPC, and Kitex will support Thrift’s two-way Streaming in the future.\n Service Governance  Kitex integrates service governance modules such as service registry, service discovery, load balancing, circuit breaker, rate limiting, retry, monitoring, tracing, logging, diagnosis, etc. Most of these modules have been provided with default extensions, and users can make their choice of modules to integrate.\n Code Generation  Kitex has built-in code generation tools that support generating Thrift, Protobuf, and scaffold code. The original Thrift code is generated by Thriftgo, which is now open sourced. Kitex’s optimization of Thrift is supported by Kitex Tool as a plugin. Protobuf code is generated by Kitex as an official protoc plugin. Currently, Protobuf IDL parsing and code generation are not separately supported.\nNetpoll Netpoll is a high-performance, non-blocking I/O networking framework which focuses on RPC scenarios, developed by ByteDance.\nRPC is usually heavy on processing logic, including business logic and codec, and therefore cannot handle I/O serially like Redis. However, Go’s standard library net is designed for blocking I/O APIs, so that the RPC framework can only follow the One Conn One Goroutine design. It increases cost for context switching due to a large number of goroutines under high concurrency. Moreover, net.Conn has no API to check Alive, so it is difficult to make an efficient connection pool for the RPC framework, because there may be a large number of failed connections in the pool.\nOn the other hand, the open source community currently lacks Go network libraries that focus on RPC scenarios. Similar repositories such as evio and gnet are focused on scenarios like Redis and Haproxy.\nNetpoll has been designed to solve these problems. It draws inspiration from the design of evio and netty, achieves excellent performance and is more suitable for microservice architecture. Netpoll also provides a number of Features. Developers are recommended to use Netpoll as the network library of the RPC framework.\nThriftgo Thriftgo is an implementation of thrift compiler in go language that supports complete syntax and semantic checking of Thrift IDL.\nCompared with the official Golang code generation by Apache Thrift, Thriftgo made some bug fixes and supports a plugin mechanism. Users can customize the generated code according to their needs.\nThriftgo is the code generation tool of Kitex. CloudWeGo will soon opensource thrift-gen-validator, a plugin of Thriftgo that supports IDL Validator and is used for field verification, which is not provide by Thrift. With the IDL Validator, developers do not need to implement code verification logic by themselves.\nAlthough Thriftgo currently only supports the generation of Golang Thrift code, it is positioned to support Thrift code generation in various languages. If there is a need in future, we will also consider supporting code generation for other programming languages. At the same time, we will try to contribute Thriftgo to the Apache Thrift community.\nMaintenance A complete microservice system builds upon a basic cloud ecosystem. No matter how the microservices are developed; based on the public cloud, a private cloud or your own infrastructure, additional services (including service governance platform, monitoring, tracing, service registry and discovery, configuration and service mesh etc) and some customized standards are needed to provide better service governance. At Bytedance we have complete internal services to support the microservice system, but these services cannot be open source in the short term. So, how will CloudWeGo maintain a set of code internally and externally, and iterate them as a whole?\nProjects in CloudWeGo are not coupled with the internal ecology. For example, Netpoll is directly migrated to open source libraries, and our internal dependencies are adjusted to open source libraries.\nKitex’s code is split into two parts, including the core of Kitex which has been migrated to the open source library, and the encapsulated internal library which will provide transparent upgrades for internal users.\nFor open source users who use Kitex, they can also extend Kitex and integrate Kitex into their own microservice system. We hope, and expect, that more developers will contribute their own extensions to kitex-contrib, providing help and convenience for more users.\nFuture directions  Open source other internal projects  We will continue to open source other internal projects, such as HTTP framework Hertz, shared memory-based IPC communication library ShmIPC and others, to provide more support for microservice scenarios.\n Open source verified and stable features  The main projects of CloudWeGo provide support for internal microservices of Bytedance. New features are usually verified internally, and we will gradually open source them when they are relatively mature, such as the integration of ShmIPC, no serialization, and no code generation.\n Combine internal and external needs and iterate  After launching open source software, in addition to supporting internal users we also hope that CloudWeGo can provide good support for external users and help everyone quickly build their own microservice system. As such, we will iterate based on the needs of both internal and external users.\nFollowing initial feedback, users have shown a stronger demand for Protobuf. Although Kitex supports multiple protocols, the internal RPC communication protocol of Bytedance is Thrift. Protobuf, Kitex Protobuf or compatibility with gRPC is supported only to fulfill the needs of a small number of internal users, so performance [for Protobuf] has not been optimized yet. In terms of code generation, we have not made any optimizations, and currently utilize Protobuf’s official binary directly.\nGogo/protobuf is an excellent open source library that optimizes Protobuf serialization performance based on generated code, but unfortunately the library is currently out of maintenance, which is why Kitex did not choose gogo. In order to meet the growing needs of developers, we are planning to carry out Kitex’s performance optimization for Protobuf support.\nYou are welcome to submit issues and PRs to build CloudWeGo together. We are excited for more developers to join, and also look forward to CloudWeGo helping more and more companies quickly build cloud-native architectures. If any corporate customers want to employ CloudWeGo in your internal projects, we can provide technical support. Feel free to raise an issue in Github if you have any questions.\n","categories":"","description":"ByteDance now offers open source through CloudWeGo！","excerpt":"ByteDance now offers open source through CloudWeGo！","ref":"/blog/2021/09/13/cloudwego-open-source-announcement/","tags":"","title":"CloudWeGo Open Source Announcement"},{"body":"开源背景 CloudWeGo 是一套由字节跳动开源的、以 Go 语言为核心的、可快速构建企业级云原生架构的中间件集合，专注于微服务通信与治理，具备高性能、可扩展、高可靠的特点。\n字节跳动内部使用 Golang 作为主要的业务开发语言，我们支持着数万个 Golang 微服务的可靠通信，经过数量众多的微服务和海量流量的验证，我们已经有了较为成熟的微服务最佳实践，于是考虑将内部的实践开源出去丰富社区生态。但微服务相关的项目较多，每个项目单独开源对外部用户并不友好，为了更好地让大家聚焦于微服务，我们以 CloudWeGo 作为项目名，逐步将内部微服务体系的项目开源，内外统一使用开源库，各项目以开源库为主进行迭代。\n内外维护一套代码，统一迭代演进，是我们开源前明确的原则，但毕竟涉及到代码库的调整，我们要保证内部用户尽可能无感知的迁移到开源库，本着对内部和开源用户负责的态度，我们要先确认内部可以平滑过渡，所以开源时并未对外宣传。让我们欣慰的是，在未宣传的情况下，一个月内 Kitex 收获了 1.2k stars，Netpoll 收获了700+ stars。\nCloudWeGo 不仅仅是一个对外的开源项目，也是一个真实的超大规模企业级实践项目。\n我们希望通过 CloudWeGo 丰富云原生社区的 Golang 产品体系，助力其他企业快速构建云原生架构，也希望吸引外部开发者共建，促进面向多元场景支持的演进，丰富产品能力。\n因为 CloudWeGo 下的项目会依赖很多内部的基础工具库，我们也推动将内部常用的 Golang 基础工具库开源出去，统一在 bytedance/gopkg 维护。\nCloudWeGo 开源项目 CloudWeGo 第一批以 Kitex RPC 框架和 Netpoll 网络库为主开源四个项目。Kitex 和 Netpoll 开源前我们发布过两篇文章 字节跳动 Go RPC 框架 Kitex 性能优化实践 和 字节跳动在 Go 网络库上的实践 分享我们的实践，文章发布后大家都在关注我们什么时候开源，因为我们希望将成熟的实践开源出去，所以没有过早的推动开源。\nKitex Kitex 是字节跳动内部的 Golang 微服务 RPC 框架，具有高性能、强可扩展的主要特点。在 Kitex 之前内部的 Golang 框架是 Kite，但 Kite 与 Thrift 深度耦合、生成代码逻辑重，很难从网络模型或编解码层面改造优化，继续支持新特性势必会造成代码越发臃肿迭代受阻问题，于是我们针对曾经的痛点设计了新的框架 Kitex。虽然 Kitex 是新框架，但已经在线上应用一年多，目前字节内部超过 50% 的 Golang 微服务使用 Kitex。\n以下简述 Kitex 的一些特性：\n  高性能：网络传输模块 Kitex 默认集成了自研的网络库 Netpoll，性能相较使用 go net 有显著优势；除了网络库带来的性能收益，Kitex 对 Thrift 编解码也做了优化，详见 优化实践。关于性能数据可参考 kitex-benchmark。\n  扩展性：Kitex 设计上做了模块划分，提供了较多的扩展接口以及默认的扩展实现，使用者也可以根据需要自行定制扩展，更多扩展能力参见 文档。Kitex 也并未耦合 Netpoll，开发者也可以选择其它网络库扩展使用。\n  消息协议：RPC 消息协议默认支持 Thrift、Kitex Protobuf、gRPC。Thrift 支持 Buffered 和 Framed 二进制协议；Kitex Protobuf 是 Kitex 自定义的 Protobuf 消息协议，协议格式类似 Thrift；gRPC 是对 gRPC 消息协议的支持，可以与 gRPC 互通。除此之外，使用者也可以扩展自己的消息协议。\n  传输协议：传输协议封装消息协议进行 RPC 互通，传输协议可以额外透传元信息，用于服务治理，Kitex 支持的传输协议有 TTHeader、HTTP2。TTHeader 可以和 Thrift、Kitex Protobuf 结合使用；HTTP2 目前主要是结合 gRPC 协议使用，后续也会支持 Thrift。\n  多消息类型：支持 PingPong、Oneway、双向 Streaming。其中 Oneway 目前只对 Thrift 协议支持，双向 Streaming 只对 gRPC 支持，后续会考虑支持 Thrift 的双向 Streaming。\n  服务治理：支持服务注册/发现、负载均衡、熔断、限流、重试、监控、链路跟踪、日志、诊断等服务治理模块，大部分均已提供默认扩展，使用者可选择集成。\n  Kitex 内置代码生成工具，可支持生成 Thrift、Protobuf 以及脚手架代码。原生的 Thrift 代码由本次一起开源的 Thriftgo 生成，Kitex 对 Thrift 的优化由 Kitex Tool 作为插件支持。Protobuf 代码由 Kitex 作为官方 protoc 插件生成 ，目前暂未单独支持 Protobuf IDL 的解析和代码生成。\n  Netpoll Netpoll 是字节跳动内部的 Golang 高性能、I/O 非阻塞的网络库，专注于 RPC 场景。\nRPC 通常有较重的处理逻辑（业务逻辑、编解码），耗时长，不能像 Redis 一样采用串行处理(必须异步)。而 Go 的标准库 net 设计了 BIO(Blocking I/O) 模式的 API，为了保证异步处理，RPC 框架设计上需要为每个连接都分配一个 goroutine，这在空闲连接较多时，产生大量的空闲 goroutine，增加调度开销。此外，net.Conn 没有提供检查连接活性的 API，很难设计出高效的连接池，池中的失效连接无法及时清理，复用低效。\n开源社区目前缺少专注于 RPC 方案的 Go 网络库。类似的项目如：evio , gnet 等，均面向 Redis, Haproxy 这样的场景。\n因此 Netpoll 应运而生，它借鉴了 evio 和 Netty 的优秀设计，具有出色的 性能，更适用于微服务架构。 同时，Netpoll 还提供了一些 特性，推荐在 RPC 框架中作为底层网络库。\nThriftgo Thriftgo 是 Go 语言实现的 Thrift IDL 解析和代码生成器，支持完善的 Thrift IDL 语法和语义检查，相较 Apache Thrift 官方的 Golang 生成代码，Thriftgo 做了一些问题修复且支持插件机制，用户可根据需求自定义生成代码。\nKitex 的代码生成工具就是 Thriftgo 的插件，CloudWeGo 近期也会开源另一个 Thriftgo 的插件 thrift-gen-validator，支持 IDL Validator，用于字段值校验，解决开发者需要自行实现代码校验逻辑的负担，弥补 Thrift 缺失的能力。\nThriftgo 目前虽然仅支持生成 Golang Thrift 代码，但其定位是可支持各语言的 Thrift 代码生成，未来如果有需求，我们也会考虑生成其他语言的代码，同时我们也将尝试将其回馈至 Apache Thrift 社区。\nNetpoll-http2 Netpoll-http2 是基于 Golang 标准库 golang.org/x/net/http2 的源码替换 go net 为 Netpoll，目前用于 Kitex 对 gRPC 协议的支持，对 HTTP2 有需求的外部开发者也可以使用此库。\n内外版本维护 完整的微服务体系离不开基础的云生态，无论在公有云、私有云还是基于自己的基础设施开发微服务，都需要搭建额外的服务以很好的支持微服务的治理，比如治理平台、监控、链路跟踪、注册/发现、配置中心、服务网格等，而且还存在一些定制的规范。字节跳动自然也有完善的内部服务支持微服务体系，但这些服务短期还无法开源，那 CloudWeGo 如何内外维护一套代码，统一迭代呢？\nCloudWeGo 下与内部生态没有耦合的项目，如 Netpoll，直接迁移到开源库，内部依赖调整为开源库。\n而需要集成治理能力融入微服务体系的 Kitex 则基于其扩展性，将内外部的代码做了拆分，Kitex 的核心代码迁移到开源库，内部库封装一层壳保证内部用户无感知升级。集成内部治理特性的模块则作为 Kitex 的扩展保留在内部库，同时对于一些新的特性也会优先在内部库支持，稳定后迁移到开源库。\n对于使用 Kitex 的开源用户，同样可以对 Kitex 进行扩展，将 Kitex 融入自己的微服务体系中，也希望开发者能贡献自己的扩展到 kitex-contrib，为更多用户提供便利。\n未来展望 继续开源其他内部项目\n 我们会继续开源其他内部项目，如 HTTP 框架 Hertz、基于共享内存的 IPC 通信库 ShmIPC 等，提供更多场景的微服务需求支持。  逐步开源经验证的、稳定的特性\n CloudWeGo 的主要项目均为字节内部微服务提供支持，新的特性通常会在内部验证，相对成熟后我们会逐步开源出去，比如对 ShmIPC 的集成、无序列化、无生成代码的支持等等。  结合内外部用户需求，持续迭代\n  CloudWeGo 开源后除向内部提供支持外，我们也希望 CloudWeGo 能为外部用户提供良好的支持，帮助大家快速搭建自己的微服务体系，所以我们会面向内外部用户迭代。\n  就开源一个月的反馈看，大家对 Protobuf 的诉求较为强烈。坦诚来说 Kitex 虽然支持多协议，但字节内部 RPC 通信协议是 Thrift，对 Protobuf 无论是 Kitex Protobuf 还是兼容 gRPC 更多的是支持少部分内部用户的需求，所以暂时未开展性能优化，生成代码也是直接使用 Protobuf 官方的二进制（gogo/protobuf 是基于生成代码优化 Protobuf 序列化性能的优秀开源库，但很遗憾该库目前是停止维护状态，所以 Kitex 并未选择 gogo），但鉴于大家强烈的诉求，我们会计划开展 Kitex 对 Protobuf 支持的性能优化。\n  欢迎大家向 CloudWeGo 提交 issue 和 PR 共建 CloudWeGo，我们诚心期待更多的开发者加入，也期待 CloudWeGo 助力越来越多的企业快速构建云原生架构。如果企业客户想内部试用，我们可以排期提供专项技术支持和交流，欢迎入群咨询。\n  ","categories":"","description":"","excerpt":"开源背景 CloudWeGo 是一套由字节跳动开源的、以 Go 语言为核心的、可快速构建企业级云原生架构的中间件集合，专注于微服务通信与治 …","ref":"/zh/blog/2021/09/07/%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%BC%80%E6%BA%90%E5%86%85%E9%83%A8%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%B8%AD%E9%97%B4%E4%BB%B6-cloudwego/","tags":"","title":"字节跳动开源内部微服务中间件 CloudWeGo"},{"body":"Improvement:  Make transMetaHandler executed before customized boundHandlers to ensure the customized boundHandlers could get metainfo. TransError uses internal error typeID if exist.  Bugfix:  Not reset stats level when clear RPCInfo in netpollmux to fix metric missing bug when use netpollmux. Remove stale addresses in long pool. Add an EOF condition to eliminate a redundant warning. Modify error types check of service circuit breaker to fix the bug that fuse cannot be triggered.  Tool:  Adjust protobuf generated code of unary to support both Kitex Protobuf and gRPC. Upgrade version of thriftgo to fix golint style. Fix typo in thrift generated code. Fix a bug that streaming generated code missing transport option.  Docs:  Add Golang setup section and Golang version requirement Some docs are updated. Add some English documents.  Dependency Change:  Thriftgo: v0.0.2-0.20210726073420-0145861fcd04 -\u003e v0.1.2 Netpoll: v0.0.2 -\u003e v0.0.3  ","categories":"","description":"","excerpt":"Improvement:  Make transMetaHandler executed before customized …","ref":"/blog/2021/08/26/kitex-release-v0.0.4/","tags":"","title":"Kitex Release v0.0.4"},{"body":"优化:  transMetaHandler 在自定义 boundHandlers 之前执行，保证自定义 boundHandlers 可以拿到 RPCInfo 信息。 TransError 暴露封装 error 的 typeID 用于支持自定义 Error 回传错误码。  Bug 修复:  复用 RPCInfo 不对 stats level 重置， 以修复在使用 netpollmux 时 metric 丢失问题。 清理不存在节点的连接池。 Streaming 中增加 Netpoll EOF 错误判断来清除冗余的 warning 日志。 修改熔断错误统计类型，非 Ignorable 错误类型均做熔断统计，以修复开源版本熔断无法正确生效和内部版本在开启mesh后重试熔断无法生效问题。  工具:  调整了 Protobuf unary 方法的生成代码，来同时支持 Kitex Protobuf 和 gRPC。 升级了 thriftgo 版本来修复 golint。 修复了生成代码中的错误。 修复了流生成的代码缺少传输选项的错误。  文档:  添加了 Golong 配置部分的文档以及 Golang 版本要求。 更新了一些现有文档。 添加了一些英文文档。  依赖变化:  Thriftgo: v0.0.2-0.20210726073420-0145861fcd04 -\u003e v0.1.2 Netpoll: v0.0.2 -\u003e v0.0.3  ","categories":"","description":"","excerpt":"优化:  transMetaHandler 在自定义 boundHandlers 之前执行，保证自定义 boundHandlers …","ref":"/zh/blog/2021/08/26/kitex-v0.0.4-%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/","tags":"","title":"Kitex v0.0.4 版本发布"},{"body":"Bugfix:  Prevent connection pool from being overridden.  ","categories":"","description":"","excerpt":"Bugfix:  Prevent connection pool from being overridden.  ","ref":"/blog/2021/08/01/kitex-release-v0.0.3/","tags":"","title":"Kitex Release v0.0.3"},{"body":"Bug 修复:  防止连接池被覆盖。  ","categories":"","description":"","excerpt":"Bug 修复:  防止连接池被覆盖。  ","ref":"/zh/blog/2021/08/01/kitex-v0.0.3-%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/","tags":"","title":"Kitex v0.0.3 版本发布"},{"body":"Improvement:  Kitex now disables all stats to improve performance when no tracer is provided. The Kitex client now will reuse connections by default.  Bugfix:  A nil-pointer bug in lbcache has been fixed. A data-race issue in the retry(backup request) is fixed.  Tool:  The kitex tool no longer generates a default config file. The kitex tool now uses the latest API of thriftgo which fixes several bad corner cases in code generation. The kitex tool now checks the existence of the go command instead of assuming it. Thanks to @anqiansong  Docs:   We have updated some documentations in this version.\n  Several lint issues and typos are fixed thanks to @rleungx @Huangxuny1 @JeffreyBool.\n  ","categories":"","description":"","excerpt":"Improvement:  Kitex now disables all stats to improve performance when …","ref":"/blog/2021/07/30/kitex-release-v0.0.2/","tags":"","title":"Kitex Release v0.0.2"},{"body":"优化：  Kitex 在没有 tracer 时关闭 stats 分阶段耗时采集，避免无 Trace 时额外的性能消耗。 Kitex client 默认使用连接池。  Bug 修复:  修复了一个 lbcache 中 nil-pointer 的错误。 修复了一个 retry 重试（Backup Request）中的 data race 问题。  工具:  Kitex 工具去掉默认生成的配置文件。 Kitex 工具现在使用最新的 thriftgo API 以避免老版 API 在生成代码时的几个边角案例。 Kitex 工具现在会检查代码中是否包含 go 命令，不再假设它的存在。感谢 @anqiansong 的贡献。  文档:  我们在这个版本中更新了一些文档。 我们修改了一些拼写错误和错别字。感谢 @rleungx @Huangxuny1 @JeffreyBool 的贡献。  ","categories":"","description":"","excerpt":"优化：  Kitex 在没有 tracer 时关闭 stats 分阶段耗时采集，避免无 Trace 时额外的性能消耗。 Kitex …","ref":"/zh/blog/2021/07/30/kitex-v0.0.2-%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/","tags":"","title":"Kitex v0.0.2 版本发布"},{"body":"Kitex project initialization.\n","categories":"","description":"","excerpt":"Kitex project initialization.\n","ref":"/blog/2021/07/12/kitex-release-v0.0.1/","tags":"","title":"Kitex Release v0.0.1"},{"body":"Kitex 项目初始化。\n","categories":"","description":"","excerpt":"Kitex 项目初始化。\n","ref":"/zh/blog/2021/07/12/kitex-v0.0.1-%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/","tags":"","title":"Kitex v0.0.1 版本发布"},{"body":"  #td-cover-block-0 { background-image: url(/about/featured-background_hu32328cd19520b83601287ce1c2b24452_94020_960x540_fill_catmullrom_bottom_2.png); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/about/featured-background_hu32328cd19520b83601287ce1c2b24452_94020_1920x1080_fill_catmullrom_bottom_2.png); } }  About CloudWeGo A leading practice for building enterprise cloud native middleware.       CloudWeGo is an open-source middleware set launched by ByteDance that can be used to quickly build enterprise-class cloud native architectures. It contains many components, including the RPC framework Kitex, the HTTP framework Hertz, the basic network library Netpoll, thrfitgo, etc. By combining the community's excellent open source products, developers can quickly build a complete set of microservices systems. For more ecosystem，please refer kitex-contrib and hertz-contrib.     Kitex Next generation high performance, highly scalable Golang RPC framework.\n     Hertz High-performance, high-usability, extensible HTTP framework for Go. It's designed to simplify building microservices for developers.\n     Netpoll High-performance NIO (Non-blocking I/O) network library, focusing on RPC scenarios.\n     Thriftgo A thrift compiler implemented by Golang supports plug-in mechanism.\n    ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/about/","tags":"","title":"About CloudWeGo"},{"body":"This is the blog section. It has two categories: News and Releases.\nFiles in these directories will be listed in reverse chronological order.\n","categories":"","description":"","excerpt":"This is the blog section. It has two categories: News and Releases. …","ref":"/blog/","tags":"","title":"Docsy Blog"},{"body":"  #td-cover-block-0 { background-image: url(/featured-background_hu32328cd19520b83601287ce1c2b24452_94020_960x540_fill_catmullrom_top_2.png); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/featured-background_hu32328cd19520b83601287ce1c2b24452_94020_1920x1080_fill_catmullrom_top_2.png); } }  Welcome to CloudWeGo A leading practice for building enterprise cloud native middleware!\n Get Started    Github           Why CloudWeGo? CloudWeGo is an open-source middleware set launched by ByteDance that can be used to quickly build enterprise-class cloud native architectures. The common characteristics of CloudWeGo projects are high performance, high scalability, high reliability and focus on microservices communication and governance.       Open  Fully open source, community neutral, compatible with the community open source ecology, pluggable components, CloudWeGo components and other open source components can be integrated or replaced with each other     Industry-standard  Contains the components needed to build an enterprise-class cloud-native architecture, allowing users to focus more on business development, meet the current and future needs of user scenarios, and experience the refinement of large-scale scenarios     Cloud Native  Quickly build a cloud-native microservice system and develop more reliable, scalable and easy-to-maintain cloud-native applications.       Projects Kitex Next generation high performance, highly scalable Golang RPC framework.\n  Hertz Next generation high performance, highly scalable Golang HTTP framework.\n  Netpoll High-performance NIO (Non-blocking I/O) network library, focusing on RPC scenarios.\n  Thriftgo A thrift compiler implemented by Golang supports plug-in mechanism.\n  Volo A high-performance and strong-extensibility Rust RPC framework.\n  Monoio A thread-per-core Rust runtime with io_uring/epoll/kqueue.\n  Motore Async middleware abstraction powered by GAT and TAIT.\n  Pilota A thrift and protobuf implementation in pure rust with high performance and extensibility.\n  Sonic A blazingly fast JSON serializing \u0026 deserializing library, accelerated by JIT and SIMD.\n  Frugal A very fast dynamic Thrift serializer \u0026 deserializer based on just-in-time compilation.\n  Fastpb A faster Protobuf serializer \u0026 deserializer.\n       \"Over the past three years, Bytedance has witnessed rapid growth in the number and scale of its microservices. In 2018, we had about 7,000-8,000 online microservices, and by May of 2021, the number had exceeded 50,000. Now, we have decided to open source these technologies to help more developers. \"  — Service Framework Team, ByteDance       Created by   Used by                  CloudWeGo enriches the CNCF CLOUD NATIVE Landscape.       Docs  Kitex Hertz Volo Netpoll Motore Pilota   Projects  Kitex Hertz Volo Netpoll Motore Pilota   Contact Us  Email: conduct@cloudwego.io Slack        ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/","tags":"","title":"CloudWeGo"},{"body":"  #td-cover-block-0 { background-image: url(/zh/featured-background_hu32328cd19520b83601287ce1c2b24452_94020_960x540_fill_catmullrom_top_2.png); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/zh/featured-background_hu32328cd19520b83601287ce1c2b24452_94020_1920x1080_fill_catmullrom_top_2.png); } }  欢迎使用 CloudWeGo 构建企业级云原生中间件的领先实践！\n 开始    Github           为什么选 CloudWeGo? CloudWeGo 是一套由字节跳动开源的、可快速构建企业级云原生微服务架构的中间件集合。CloudWeGo 项目共同的特点是高性能、高扩展性、高可靠，专注于微服务通信与治理。       开放  技术栈全面开源共建、保持社区中立、兼容社区开源生态，组件可插拔，CloudWeGo 组件与其它开源组件可互相集成或替换。    企业级  包含构建企业级云原生架构所需的各个组件，让用户更加专注于业务开发，满足用户场景的现状和未来需求，经历过大规模场景的锤炼。    云原生  基于 CloudWeGo 可快速搭建云原生微服务体系，快速开发更具可靠性、扩展性、易维护的云原生应用。      项目 Kitex 下一代高性能、强可扩展的 Golang RPC 框架。\n  Hertz 下一代高性能、强可扩展的 Golang HTTP 框架。\n  Netpoll 高性能、I/O非阻塞的网络框架，专注于 RPC 场景。\n  Thriftgo Go 语言实现的 Thrift 编译器，支持插件机制，支持完整的 Thrift IDL 语法和完善的语义检查。\n  Volo 高性能、强可扩展的 Rust RPC 框架。\n  Monoio 基于 io_uring/epoll/kqueue 和 thread-per-core 模型的 Rust Runtime。\n  Motore 基于 GAT 和 TAIT 的异步中间件抽象。\n  Pilota 一个在纯 Rust 中具有高性能和可扩展性的 Thrift 和 Protobuf 实现。\n  Sonic 非常快的由 JIT 和 SIMD 加速的 JSON 序列化和反序列化库。\n  Frugal 基于 JIT 编译的高性能动态 Thrift 编解码器。\n  Fastpb 非常快的 Protobuf 序列化和反序列化库。\n       \"近三年来，字节跳动的微服务数量和规模迎来快速发展。2018 年，我们的在线微服务数大约是 7000-8000，到 2021 年五月份，这一数字已经突破 5 万。现在我们决定把这些技术开源出来，帮助更多开发者。\"  — 字节跳动服务框架团队       出品方   谁在用                  CloudWeGo 丰富了 CNCF 云原生生态.       文档  Kitex Hertz Volo Netpoll Motore Pilota   项目  Kitex Hertz Volo Netpoll Motore Pilota   联系我们  邮箱：conduct@cloudwego.io 加入 Slack        ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/zh/","tags":"","title":"CloudWeGo"},{"body":"  Welcome to CloudWeGo Community CloudWeGo is an open source project that anyone in the community can use, improve, and enjoy. We'd love you to join us! Here's a few ways to find out what's happening and get involved.\n    Develop and Contribute If you want to get more involved by contributing to CloudWeGo, join us here:\n  GitHub:  Development takes place here!   Slack:  Join our CloudWeGo Slack Channel    You can find out how to contribute to these docs in our Contribution Guidelines.  ","categories":"","description":"","excerpt":"  Welcome to CloudWeGo Community CloudWeGo is an open source project …","ref":"/community/","tags":"","title":"Community"},{"body":"  #td-cover-block-0 { background-image: url(/cooperation/featured-background_hu12e647f8638b137fcd0c7539335f6f57_163512_960x540_fill_catmullrom_bottom_2.png); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/cooperation/featured-background_hu12e647f8638b137fcd0c7539335f6f57_163512_1920x1080_fill_catmullrom_bottom_2.png); } }  谁在使用CloudWeGo CloudWeGo 用户实践案例分享       CloudWeGo 是一套由字节跳动开源的、可快速构建企业级云原生架构的中间件集合，专注于微服务通信与治理，具备高性能、高扩展性、高可靠的特点，满足不同业务在不同场景的诉求。 此外，CloudWeGo 也重视与云原生生态的集成，支持对接主流注册中心、Prometheus 监控以及 OpenTelemetry \u0026 OpenTracing 链路追踪等。 目前 CloudWeGo 已经在诸多企业和相关业务落地，涉及到电商、证券、游戏、企业软件、基础架构等诸多领域，详见下面案例介绍。 华兴证券在混合云原生架构下的 Kitex 实践  华兴证券是 CloudWeGo 企业用户，使用 Kitex 框架完成混合云部署下的跨机房调用。完成搭建针对 Kitex 的可观测性系统，以及在 K8s 同集群和跨集群下使用 Kitex 的落地实践。 了解更多   Kitex 在森马电商场景的落地实践  近些年电商行业高速发展，森马电商线上业务激增，面临着高并发、高性能的业务场景需求。森马正式成为 CloudWeGo 的企业用户，通过使用 Kitex 接入 Istio，极大地提高了对高并发需求的处理能力。 了解更多   飞书管理后台平台化改造的演进史  飞书管理后台是飞书套件专为企业管理员提供的信息管理平台，通过引入 Kitex 泛化调用对飞书管理后台进行平台化改造，提供一套统一的标准和通用服务，实现了飞书管理后台作为企业统一数字化管理平台的愿景。 了解更多       ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/cooperation/","tags":"","title":"cooperation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/search/","tags":"","title":"Search Results"},{"body":"This is the security section. It has two categories: vulnerability-reporting and safety-bulletin.\nFiles in these directories will be listed in reverse chronological order.\n","categories":"","description":"","excerpt":"This is the security section. It has two categories: …","ref":"/security/","tags":"","title":"Security"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/community/meeting_notes/","tags":"","title":"会议记录"},{"body":"  #td-cover-block-0 { background-image: url(/zh/about/featured-background_hu32328cd19520b83601287ce1c2b24452_94020_960x540_fill_catmullrom_bottom_2.png); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/zh/about/featured-background_hu32328cd19520b83601287ce1c2b24452_94020_1920x1080_fill_catmullrom_bottom_2.png); } }  关于 一套可快速构建企业级云原生架构的最佳实践！\n      CloudWeGo 是一套可快速构建企业级云原生微服务架构的中间件集合。 它包含许多组件：RPC 框架 Kitex，HTTP 框架 Hertz，网络框架 Netpoll，Go 语言 thrift 编译器 Thriftgo 等等。 通过结合社区优秀的开源产品和生态，可以快速搭建一套完善的云原生微服务体系。 更多生态能力对接，请参考 kitex-contrib 和 hertz-contrib。     Kitex 下一代高性能、强可扩展的 Golang RPC 框架。\n     Hertz 高易用性、高性能、高扩展性的 Golang 微服务 HTTP 框架，旨在为开发人员简化构建微服务。      Netpoll 高性能、I/O非阻塞的网络框架，专注于 RPC 场景。      Thriftgo Go 语言实现的 thrift 编译器，支持插件机制，支持完整的 thrift IDL 语法和完善的语义检查。\n    ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/zh/about/","tags":"","title":"关于"},{"body":"This is the blog section. It has two categories: News and Releases.\nFiles in these directories will be listed in reverse chronological order.\n","categories":"","description":"","excerpt":"This is the blog section. It has two categories: News and Releases. …","ref":"/zh/blog/","tags":"","title":"博客"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/community/weekly_report/","tags":"","title":"周报"},{"body":"This is the security section. It has two categories: vulnerability-reporting and safety-bulletin.\nFiles in these directories will be listed in reverse chronological order.\n","categories":"","description":"","excerpt":"This is the security section. It has two categories: …","ref":"/zh/security/","tags":"","title":"安全"},{"body":" 🏅 2021-2022 CloudWeGo Awesome Contributor 评选 CloudWeGo：从开源、开放到企业落地  ","categories":"","description":"","excerpt":" 🏅 2021-2022 CloudWeGo Awesome Contributor 评选 CloudWeGo：从开源、开放到企业落地  ","ref":"/zh/community/past_activities/","tags":"","title":"往期活动"},{"body":"   一个由开发者来定义的开源社区 这里有一群致力于打造「 高性能、可拓展、高可靠 」框架项目的开发者们，我们有着不同的身份、来自不同的企业、承担着不同的角色， 却在社区内，通过社区活动和开源协作，推进项目功能演进，拓展着 CloudWeGo 的开源生态。 开源开放，多元包容！共同定义着什么才是「CloudWeGo 社区」。 在这里，我们有关注开发者从0到1成长的 CSG 小组（CloudWeGo Study Group），CloudWeGo 社区文化乐于分享和帮助开发者共同成长。在这里，我们有社区例会和社区 Meetup 活动， 让成员更加方便的参与社区日常运营和开发任务，共同推进项目演进。 在这里，我们有真正志同道合的开源伙伴：有为了帮助项目技术分享和获得更多开发者反馈的布道师、有热衷技术并不断挑战优化的开发者、 有持续关注技术进展和开源发展的高校群体、有帮助项目上下游生态落地的合作伙伴、有持续不断为项目提供真实需求反馈的企业用户。 我们欢迎更多希望打造一个 真正优质、体检更佳、业界顶尖 的开源项目 的同学加入我们！共同建设，享受开源的魅力与乐趣！  贡献者名单  社区周报  讨论  会议记录  发行版本    贡献者名单 如何成为社区 Committer ? 其中展示了从成立至今经过 CloudWeGo 社区认证的 Committer 们 (排名不分先后) CoderPoet baiyutang horizonzy li-jin-gou liu-song  查看全部    社区周报 CloudWeGo 社区例会和双周进展动态，将会更新在社区周报中\n继续阅读    讨论 社区开发者们都在关注那些技术热点？日常会如何进行技术讨论和答疑解惑？ 欢迎加入社区讨论组，和社区开发者们共同交流。我们有以下方式可参与到社区讨论中   微信群  ▶ 更便捷的交流方式：日常开发者交流、活动分享、技术答疑\n  飞书群  ▶ 更快速的企业支持：企业使用场景、技术问题可被快速跟进\n  GitHub ▶ 关于项目的 issue 和pr 请在这里提交\n  Slack  ▶ 更适合海外同学参与社区\n   会议记录 这里记录了 CloudWeGo 双周例会会议纪要和例会分享材料\n继续阅读     未来活动    演讲主题  高性能 RPC 框架 Kitex 内外统一的开源实践\n杨芮 CloudWeGo - Kitex 项目负责人、字节跳动基础架构服务框架资深研发工程师\n大规模企业级 HTTP 框架设计和实践\n高文举 CloudWeGo - Hertz 项目负责人、字节跳动基础架构服务框架资深研发工程师\n新一代基于 Rust 语言的高性能 RPC 框架\n吴迪 CloudWeGo - Volo 项目负责人、字节跳动基础架构服务框架资深研发工程师\n开源社区的长期主义与新变化 - CloudWeGo 开源社区实践\n邓逸云 CloudWeGo 开源（社区）运营负责人\n查看往期活动    ","categories":"","description":"","excerpt":"   一个由开发者来定义的开源社区 这里有一群致力于打造「 高性能、可拓展、高可靠 」框架项目的开发者们，我们有着不同的身份、来自不同的企 …","ref":"/zh/community/","tags":"","title":"概述"},{"body":"  #td-cover-block-0 { background-image: url(/zh/cooperation/featured-background_hu12e647f8638b137fcd0c7539335f6f57_163512_960x540_fill_catmullrom_bottom_2.png); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/zh/cooperation/featured-background_hu12e647f8638b137fcd0c7539335f6f57_163512_1920x1080_fill_catmullrom_bottom_2.png); } }  谁在使用CloudWeGo CloudWeGo 用户实践案例分享       CloudWeGo 是一套由字节跳动开源的、可快速构建企业级云原生架构的中间件集合，专注于微服务通信与治理，具备高性能、高扩展性、高可靠的特点，满足不同业务在不同场景的诉求。 此外，CloudWeGo 也重视与云原生生态的集成，支持对接主流注册中心、Prometheus 监控以及 OpenTelemetry \u0026 OpenTracing 链路追踪等。 目前 CloudWeGo 已经在诸多企业和相关业务落地，涉及到电商、证券、游戏、企业软件、基础架构等诸多领域，详见下面案例介绍。 华兴证券在混合云原生架构下的 Kitex 实践  华兴证券是 CloudWeGo 企业用户，使用 Kitex 框架完成混合云部署下的跨机房调用。完成搭建针对 Kitex 的可观测性系统，以及在 K8s 同集群和跨集群下使用 Kitex 的落地实践。 了解更多   Kitex 在森马电商场景的落地实践  近些年电商行业高速发展，森马电商线上业务激增，面临着高并发、高性能的业务场景需求。森马正式成为 CloudWeGo 的企业用户，通过使用 Kitex 接入 Istio，极大地提高了对高并发需求的处理能力。 了解更多   飞书管理后台平台化改造的演进史  飞书管理后台是飞书套件专为企业管理员提供的信息管理平台，通过引入 Kitex 泛化调用对飞书管理后台进行平台化改造，提供一套统一的标准和通用服务，实现了飞书管理后台作为企业统一数字化管理平台的愿景。 了解更多       ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/zh/cooperation/","tags":"","title":"案例"}]